{
  "api/app/runner.html": {
    "href": "api/app/runner.html",
    "title": "Class app::Runner | qiotoolkit",
    "keywords": "Class app::Runner Inheritance app::Runner Constructors Runner() Declaration app::Runner::Runner() Runner() Declaration app::Runner::Runner(const Runner&)=delete Methods operator=() Declaration Runner&app::Runner::operator=(const Runner&)=delete ~Runner() Declaration virtual app::Runner::~Runner() set_output_benchmark() Declaration void app::Runner::set_output_benchmark(bool value) set_input_file() Declaration void app::Runner::set_input_file(std::string file_path) set_parameter_file() Declaration void app::Runner::set_parameter_file(std::string file_path) set_solver() Declaration virtual void app::Runner::set_solver(std::string target) get_target() Declaration std::string app::Runner::get_target() configure() Declaration void app::Runner::configure() configure() Configure the solver and model to run. This parses the string being passed as json and selects the solver and model according to the target and input_data.type attributes defined therein. In particular, it calls Solver::configure() and Model::configure() with a Json config, meaning that any work performed as part of reading the input arguments will happen here (this includes setting up the graph for GraphModels). Note Response.benchmark.end2end_runtime_ms in the benchmark output will reflect the time / spent in this function PLUS the time spent in Runner::run(). Declaration void app::Runner::configure(const std::string&json) run() Run the simulation. This calls Solver::init() followed by Solver::run(). Note Response.benchmark.execution_time_ms will reflect the time spent in this method (excluding Structure::to_string() converting the response to string, because that happens after the time measurement is written into the response) Declaration std::string app::Runner::run() get_run_output() Declaration utils::Structure app::Runner::get_run_output() get_solver() Declaration ::solver::Solver* app::Runner::get_solver() const copy_if_present() Declaration void app::Runner::copy_if_present(const std::string&key, const utils::Structure&src, utils::Structure&target) add_common_metrics() Declaration void app::Runner::add_common_metrics(utils::Structure&benchmarks, utils::Structure&metrics) configure() Configure using streaming. Declaration void app::Runner::configure(const std::string&input, const utils::Json&parameters, const std::string&solver_name) configure() Handle dimacs input for max-sat. Handling of parsed dimacs. Declaration void app::Runner::configure(const utils::Dimacs&dimacs, const utils::Json&parameters, const std::string&solver_name) select_max_sat_implementation() Select a suitable implementation (i.e., Counter_T size) for the max sat model. Declaration void app::Runner::select_max_sat_implementation(::model::MaxSat32 *maxsat, const utils::Json&params, const std::string&target) check_solver() Check that a solver has been created successfully. Otherwise, throw an exception indicating whether model selection or solver selection failed. Declaration void app::Runner::check_solver(const std::string&model_type, const std::string&selected_model, const std::string&target) metric_to_console() Declaration void app::Runner::metric_to_console(utils::Structure&metrics) memory_saving_enabled() Declaration bool app::Runner::memory_saving_enabled() const memory_saving_retry() Declaration bool app::Runner::memory_saving_retry() const reset_for_memory_saving() Declaration void app::Runner::reset_for_memory_saving() target_support_memory_saving() Declaration bool app::Runner::target_support_memory_saving() const model_support_memory_saving() Declaration bool app::Runner::model_support_memory_saving() const"
  },
  "api/argp.html": {
    "href": "api/argp.html",
    "title": "Struct argp | qiotoolkit",
    "keywords": "Struct argp"
  },
  "api/argp/option.html": {
    "href": "api/argp/option.html",
    "title": "Struct argp_option | qiotoolkit",
    "keywords": "Struct argp_option"
  },
  "api/argp/state.html": {
    "href": "api/argp/state.html",
    "title": "Struct argp_state | qiotoolkit",
    "keywords": "Struct argp_state"
  },
  "api/boost/uniform/real.html": {
    "href": "api/boost/uniform/real.html",
    "title": "Class boost::uniform_real | qiotoolkit",
    "keywords": "Class boost::uniform_real Inheritance boost::uniform_real"
  },
  "api/config.html": {
    "href": "api/config.html",
    "title": "Struct Config | qiotoolkit",
    "keywords": "Struct Config"
  },
  "api/examples/descent.html": {
    "href": "api/examples/descent.html",
    "title": "Class examples::Descent | qiotoolkit",
    "keywords": "Class examples::Descent Inheritance solver::SteppingSolver examples::Descent Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver get_solutions copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Methods get_identifier() Get the identifier of this solver. This identifier is denoted as the target in the request. Declaration std::string examples::Descent<Model_T>::get_identifier() const override configure() Check the identifier and version against the configuraiton. Declaration void examples::Descent<Model_T>::configure(const utils::Json&json) override init_memory_check_error_message() Declaration std::string examples::Descent<Model_T>::init_memory_check_error_message() const override target_number_of_states() Declaration size_t examples::Descent<Model_T>::target_number_of_states() const override init() Initialize the solver. Declaration void examples::Descent<Model_T>::init() override make_step() Declaration void examples::Descent<Model_T>::make_step(uint64_t) override finalize() Declaration void examples::Descent<Model_T>::finalize() override"
  },
  "api/examples/soft-spin-state.html": {
    "href": "api/examples/soft-spin-state.html",
    "title": "Class examples::SoftSpinState | qiotoolkit",
    "keywords": "Class examples::SoftSpinState Inheritance markov::State examples::SoftSpinState Inherited Members configure ~Component Component param Methods render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure examples::SoftSpinState::render() const override get_status() get_status shows a simplified state representation Like render, this produces a structured representation of the object's state. However, it is intended to be simpler in nature with the purpose of rendering the object during stream-output and logging. By default, it will fall back to the full render, but overloading this allows distinguising how the object looks during LOG vs full output. Declaration utils::Structure examples::SoftSpinState::get_status() const override get_class_name() get_class_name shows an identifier of the (derived) class name Its primary use is for type identification during stream output and logging, where a component is rendered as <ClassName: status>. Example: {c++} MyClass : public Component {} // This will render as '<MyClass>' MyClass my_object; std::cout << my_object << std::endl; Note You do not need to overload this method unless you want to change the output of the default implementation. Declaration std::string examples::SoftSpinState::get_class_name() const override memory_estimate() Declaration static size_t examples::SoftSpinState::memory_estimate(size_t N) state_only_memory_estimate() Declaration static size_t examples::SoftSpinState::state_only_memory_estimate(size_t N)"
  },
  "api/examples/soft-spin-transition.html": {
    "href": "api/examples/soft-spin-transition.html",
    "title": "Class examples::SoftSpinTransition | qiotoolkit",
    "keywords": "Class examples::SoftSpinTransition Inheritance markov::Transition examples::SoftSpinTransition Inherited Members configure render ~Component Component get_status param get_class_name Constructors SoftSpinTransition() Declaration examples::SoftSpinTransition::SoftSpinTransition() Methods <() Declaration bool examples::SoftSpinTransition::operator<(const SoftSpinTransition&trans) const <() Declaration bool examples::SoftSpinTransition::operator<(const SoftSpinTransition&trans) const"
  },
  "api/examples/soft-spin.html": {
    "href": "api/examples/soft-spin.html",
    "title": "Class examples::SoftSpin | qiotoolkit",
    "keywords": "Class examples::SoftSpin Inheritance model::GraphModel examples::SoftSpin Inherited Members Model state_only_memory_estimate init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff set_step_limit state_memory_estimate render calculate_cost_difference get_random_transition configure match_version ~BaseModel configure BaseModel edge get_const_cost node rescale node_count estimate_max_cost_diff get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled edges get_initial_configuration nodes get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string examples::SoftSpin::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string examples::SoftSpin::get_version() const override configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void examples::SoftSpin::configure(const utils::Json&json) override configure() Declaration void examples::SoftSpin::configure(Configuration_T&configuration) calculate_cost() Declaration double examples::SoftSpin::calculate_cost(const State_T&state) const override calculate_cost_difference() Declaration double examples::SoftSpin::calculate_cost_difference(const State_T&state, const Transition_T&transition) const override get_random_state() Return a valid random state for the model. The various algorithms don't know how to initialize a valid state; this allows them to start with a random one. NOTE: The rng being passed should be used for randomness (as opposed to creating one for the model), as multiple threads can potentially use the same underlying model (albeit with separate rngs). Declaration State_T examples::SoftSpin::get_random_state(utils::RandomGenerator&rng) const override get_random_transition() Declaration Transition_T examples::SoftSpin::get_random_transition(const State_T&, utils::RandomGenerator&rng) const override apply_transition() Declaration void examples::SoftSpin::apply_transition(const Transition_T&transition, State_T&state) const override state_memory_estimate() Declaration size_t examples::SoftSpin::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t examples::SoftSpin::state_only_memory_estimate() const override"
  },
  "api/graph/clustering.html": {
    "href": "api/graph/clustering.html",
    "title": "Class graph::Clustering | qiotoolkit",
    "keywords": "Class graph::Clustering Clustering coefficient (global and average of local). Note There are open questions on how to generalize the clustering coefficient for weighter graphs and hyper edges. Inheritance graph::Property graph::Clustering Inherited Members ~Property Methods compute() Declaration utils::Structure graph::Clustering<Graph>::compute(const Graph&g) const override"
  },
  "api/graph/compact-graph-visitor.html": {
    "href": "api/graph/compact-graph-visitor.html",
    "title": "Class graph::CompactGraphVisitor | qiotoolkit",
    "keywords": "Class graph::CompactGraphVisitor Inheritance graph::CompactGraphVisitor Constructors CompactGraphVisitor() Declaration graph::CompactGraphVisitor::CompactGraphVisitor() Methods reset() Declaration void graph::CompactGraphVisitor::reset()"
  },
  "api/graph/compact-graph.html": {
    "href": "api/graph/compact-graph.html",
    "title": "Class graph::CompactGraph | qiotoolkit",
    "keywords": "Class graph::CompactGraph Save graph mode of ising and pubo mode in compact format (BitStream) Inheritance utils::Component graph::CompactGraph Inherited Members render ~Component Component get_status param get_class_name Constructors CompactGraph() Declaration graph::CompactGraph<ELEMTYPE>::CompactGraph() Methods set_allow_dup_merge() Declaration void graph::CompactGraph<ELEMTYPE>::set_allow_dup_merge(bool value) get_const_cost() Declaration double graph::CompactGraph<ELEMTYPE>::get_const_cost() const is_empty() Declaration bool graph::CompactGraph<ELEMTYPE>::is_empty() const configure() Declaration void graph::CompactGraph<ELEMTYPE>::configure(Configuration_T&config) configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void graph::CompactGraph<ELEMTYPE>::configure(const utils::Json&) override init() Declaration void graph::CompactGraph<ELEMTYPE>::init() nodes_size() Declaration size_t graph::CompactGraph<ELEMTYPE>::nodes_size() const edges_size() Declaration size_t graph::CompactGraph<ELEMTYPE>::edges_size() const is_rescaled() Declaration bool graph::CompactGraph<ELEMTYPE>::is_rescaled() const rescale() Declaration void graph::CompactGraph<ELEMTYPE>::rescale() get_scale_factor() Declaration double graph::CompactGraph<ELEMTYPE>::get_scale_factor() const map_output() Declaration int graph::CompactGraph<ELEMTYPE>::map_output(int internal_id) const output_map() Declaration const std::map<int, int>&graph::CompactGraph<ELEMTYPE>::output_map() const edge() Declaration const Edge_T&graph::CompactGraph<ELEMTYPE>::edge(size_t edge_id) const read_next_neig() Declaration uint64_t graph::CompactGraph<ELEMTYPE>::read_next_neig(CompactGraphVisitor&reader) const read_next_coeff() Declaration double graph::CompactGraph<ELEMTYPE>::read_next_coeff(CompactGraphVisitor&reader) const is_edge_end() Declaration bool graph::CompactGraph<ELEMTYPE>::is_edge_end(uint32_t value) const is_node_end() Declaration bool graph::CompactGraph<ELEMTYPE>::is_node_end(uint32_t value) const get_locality() Declaration uint32_t graph::CompactGraph<ELEMTYPE>::get_locality() const get_min_locality() Declaration uint32_t graph::CompactGraph<ELEMTYPE>::get_min_locality() const get_avg_locality() Declaration double graph::CompactGraph<ELEMTYPE>::get_avg_locality() const get_accumulated_dependent_vars() Declaration uint64_t graph::CompactGraph<ELEMTYPE>::get_accumulated_dependent_vars() const get_max_coupling_magnitude() Declaration double graph::CompactGraph<ELEMTYPE>::get_max_coupling_magnitude() const get_min_coupling_magnitude() Declaration double graph::CompactGraph<ELEMTYPE>::get_min_coupling_magnitude() const get_sum_coefficient_degrees_total() Declaration uint64_t graph::CompactGraph<ELEMTYPE>::get_sum_coefficient_degrees_total() const estimate_max_cost_diff() Declaration double graph::CompactGraph<ELEMTYPE>::estimate_max_cost_diff() const get_node_name_to_id_map() Declaration const std::map<int, int>&graph::CompactGraph<ELEMTYPE>::get_node_name_to_id_map() const"
  },
  "api/graph/cost-edge.html": {
    "href": "api/graph/cost-edge.html",
    "title": "Class graph::CostEdge | qiotoolkit",
    "keywords": "Class graph::CostEdge Edge subclass with an associated cost. This attaches a Cost (default type: double) to each edge in a graph. This is used, for instance, to represent the coefficients in the graph representation of an Ising cost function: Inheritance graph::Edge graph::CostEdge Inherited Members nodes_count clear_node_ids Edge Edge remove_node_id num_nodes add_node_id sort_node_ids node_ids ~Component Component param Constructors CostEdge() Declaration graph::CostEdge<Cost>::CostEdge() CostEdge() Declaration graph::CostEdge<Cost>::CostEdge(Cost c) CostEdge() Declaration graph::CostEdge<Cost>::CostEdge(Cost c, const std::vector<int>&node_ids) Methods cost() Accessor for the cost. Declaration Cost graph::CostEdge<Cost>::cost() const set_cost() Setter for the cost. Declaration void graph::CostEdge<Cost>::set_cost(Cost cost) configure() Serialize as a regular edge with an entry \"c\" for the cost. Declaration void graph::CostEdge<Cost>::configure(const utils::Json&json) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure graph::CostEdge<Cost>::render() const override get_status() get_status shows a simplified state representation Like render, this produces a structured representation of the object's state. However, it is intended to be simpler in nature with the purpose of rendering the object during stream-output and logging. By default, it will fall back to the full render, but overloading this allows distinguising how the object looks during LOG vs full output. Declaration utils::Structure graph::CostEdge<Cost>::get_status() const override get_class_name() get_class_name shows an identifier of the (derived) class name Its primary use is for type identification during stream output and logging, where a component is rendered as <ClassName: status>. Example: {c++} MyClass : public Component {} // This will render as '<MyClass>' MyClass my_object; std::cout << my_object << std::endl; Note You do not need to overload this method unless you want to change the output of the default implementation. Declaration std::string graph::CostEdge<Cost>::get_class_name() const override rescale() Declaration void graph::CostEdge<Cost>::rescale(double scale_factor) override memory_estimate() Declaration static size_t graph::CostEdge<Cost>::memory_estimate(size_t num_nodes)"
  },
  "api/graph/cost-edge/get/cost.html": {
    "href": "api/graph/cost-edge/get/cost.html",
    "title": "Struct graph::CostEdge::Get_Cost | qiotoolkit",
    "keywords": "Struct graph::CostEdge::Get_Cost Methods get() Declaration static Cost&graph::CostEdge<Cost>::Get_Cost::get(CostEdge&e) get_key() Declaration static std::string graph::CostEdge<Cost>::Get_Cost::get_key()"
  },
  "api/graph/couplings.html": {
    "href": "api/graph/couplings.html",
    "title": "Class graph::Couplings | qiotoolkit",
    "keywords": "Class graph::Couplings Coupling strength statistics. Produces an average and distribution of the coupling strength. Inheritance graph::Property graph::Couplings Inherited Members ~Property Methods compute() Declaration utils::Structure graph::Couplings<Graph>::compute(const Graph&g) const override"
  },
  "api/graph/degree.html": {
    "href": "api/graph/degree.html",
    "title": "Class graph::Degree | qiotoolkit",
    "keywords": "Class graph::Degree Degree. Histogram and average degree of the nodes in the graph. Inheritance graph::Property graph::Degree Inherited Members ~Property Methods compute() Declaration utils::Structure graph::Degree<Graph>::compute(const Graph&g) const override"
  },
  "api/graph/edge-with-face.html": {
    "href": "api/graph/edge-with-face.html",
    "title": "Class graph::EdgeWithFace | qiotoolkit",
    "keywords": "Class graph::EdgeWithFace This edge class extends the CostEdge class. The key difference is that EdgeWithFace objects maintain a face ID to which the edge belongs. No validation on an edge's nodes belonging to the face is handled here. Inheritance graph::CostEdge graph::EdgeWithFace Inherited Members CostEdge set_cost CostEdge configure CostEdge rescale get_status get_class_name render cost nodes_count clear_node_ids Edge Edge remove_node_id num_nodes render add_node_id sort_node_ids get_status node_ids ~Component Component param Constructors EdgeWithFace() Constructor with node IDs list and face ID initialization. Declaration graph::EdgeWithFace<Cost>::EdgeWithFace() EdgeWithFace() Declaration graph::EdgeWithFace<Cost>::EdgeWithFace(Cost c) EdgeWithFace() Declaration graph::EdgeWithFace<Cost>::EdgeWithFace(Cost c, const std::vector<int>&node_ids) Methods face_id() Accessor and setter for the face id. Each edge has just one face. Declaration int graph::EdgeWithFace<Cost>::face_id() const set_face_id() Declaration void graph::EdgeWithFace<Cost>::set_face_id(int face_id) get_class_name() get_class_name shows an identifier of the (derived) class name Its primary use is for type identification during stream output and logging, where a component is rendered as <ClassName: status>. Example: {c++} MyClass : public Component {} // This will render as '<MyClass>' MyClass my_object; std::cout << my_object << std::endl; Note You do not need to overload this method unless you want to change the output of the default implementation. Declaration std::string graph::EdgeWithFace<Cost>::get_class_name() const override memory_estimate() Declaration static size_t graph::EdgeWithFace<Cost>::memory_estimate(size_t num_nodes)"
  },
  "api/graph/edge.html": {
    "href": "api/graph/edge.html",
    "title": "Class graph::Edge | qiotoolkit",
    "keywords": "Class graph::Edge Base representation of an edge. An edge consists of a list of ids of nodes that are part of it. For a regular graph, this list of nodes would always have lenght 2; in the case of a hypergraph it will typically be 2+. There is currently no support for directed graphs. Note A node does not know its own id as this information can change when the graph is mutated. Inheritance utils::Component graph::Edge graph::CostEdge Inherited Members ~Component Component param get_class_name Constructors Edge() Default constructor for an empty edge. Declaration graph::Edge::Edge() Edge() Constructor with node list initialization. Declaration graph::Edge::Edge(const std::vector<int>&node_ids) Methods node_ids() Accessor for the list of node ids. Declaration const std::vector<int>&graph::Edge::node_ids() const clear_node_ids() Remove all node_ids from this edge. Declaration void graph::Edge::clear_node_ids() sort_node_ids() Order the node_ids ascendingly. Declaration void graph::Edge::sort_node_ids() add_node_id() Append a node at the end of the node_id list. Declaration void graph::Edge::add_node_id(int node_id) remove_node_id() Find and remove a node from the list. This will remove exactly one instance if one is found, zero otherwise. Declaration void graph::Edge::remove_node_id(int node_id) configure() Serialize the edge's node_ids under [\"ids\"]. Declaration void graph::Edge::configure(const utils::Json&json) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure graph::Edge::render() const override get_status() get_status shows a simplified state representation Like render, this produces a structured representation of the object's state. However, it is intended to be simpler in nature with the purpose of rendering the object during stream-output and logging. By default, it will fall back to the full render, but overloading this allows distinguising how the object looks during LOG vs full output. Declaration utils::Structure graph::Edge::get_status() const override rescale() Declaration virtual void graph::Edge::rescale(double scale_factor) nodes_count() Declaration size_t graph::Edge::nodes_count() const num_nodes() Declaration size_t graph::Edge::num_nodes(const utils::Json&json) memory_estimate() Declaration static size_t graph::Edge::memory_estimate(size_t num_nodes)"
  },
  "api/graph/edge/get/node/ids.html": {
    "href": "api/graph/edge/get/node/ids.html",
    "title": "Struct graph::Edge::Get_Node_Ids | qiotoolkit",
    "keywords": "Struct graph::Edge::Get_Node_Ids Methods get() Declaration static std::vector<int>&graph::Edge::Get_Node_Ids::get(Edge&e) get_key() Declaration static std::string graph::Edge::Get_Node_Ids::get_key()"
  },
  "api/graph/face.html": {
    "href": "api/graph/face.html",
    "title": "Class graph::Face | qiotoolkit",
    "keywords": "Class graph::Face Base representation of a face. A face consists of a (face) type, a weight cost, and a list of component edge IDs Inheritance utils::Component graph::Face Inherited Members ~Component Component param get_class_name Constructors Face() Default constructor for an empty face. Declaration graph::Face<Cost>::Face() Face() Constructor with type. Declaration graph::Face<Cost>::Face(const FaceType type) Face() Constructor with cost. Declaration graph::Face<Cost>::Face(const Cost c) Face() Constructor with type and cost. Declaration graph::Face<Cost>::Face(const FaceType type, const Cost c) Face() Constructor with edge ID list initialization. Declaration graph::Face<Cost>::Face(const std::vector<int>&edge_ids) Face() Constructor with type, cost, and edge ID list initialization. Declaration graph::Face<Cost>::Face(const FaceType type, const Cost c, const std::vector<int>&edge_ids) Methods edge_ids() Accessor for the list of edge IDs. Declaration const std::vector<int>&graph::Face<Cost>::edge_ids() const type() Accessor for face type. Declaration const FaceType&graph::Face<Cost>::type() const cost() Accessor for the cost. Declaration Cost graph::Face<Cost>::cost() const set_cost() Setter for the cost. Declaration void graph::Face<Cost>::set_cost(Cost cost) sort_edge_ids() Order the list of edge IDs ascendingly. Declaration void graph::Face<Cost>::sort_edge_ids() add_edge_id() Append an edge ID to the end of the edge ID list. Declaration void graph::Face<Cost>::add_edge_id(int edge_id) remove_edge_id() Find and remove an edge ID from the edge ID list. Declaration void graph::Face<Cost>::remove_edge_id(int edge_id) configure() Serialize the face. Declaration void graph::Face<Cost>::configure(const utils::Json&json) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure graph::Face<Cost>::render() const override get_status() get_status shows a simplified state representation Like render, this produces a structured representation of the object's state. However, it is intended to be simpler in nature with the purpose of rendering the object during stream-output and logging. By default, it will fall back to the full render, but overloading this allows distinguising how the object looks during LOG vs full output. Declaration utils::Structure graph::Face<Cost>::get_status() const override rescale() Declaration void graph::Face<Cost>::rescale(double scale_factor) edges_count() Declaration size_t graph::Face<Cost>::edges_count() const memory_estimate() Declaration static size_t graph::Face<Cost>::memory_estimate(size_t num_edges)"
  },
  "api/graph/face/get/cost.html": {
    "href": "api/graph/face/get/cost.html",
    "title": "Struct graph::Face::Get_Cost | qiotoolkit",
    "keywords": "Struct graph::Face::Get_Cost Methods get() Declaration static Cost&graph::Face<Cost>::Get_Cost::get(Face&f) get_key() Declaration static std::string graph::Face<Cost>::Get_Cost::get_key()"
  },
  "api/graph/face/get/edge/ids.html": {
    "href": "api/graph/face/get/edge/ids.html",
    "title": "Struct graph::Face::Get_Edge_Ids | qiotoolkit",
    "keywords": "Struct graph::Face::Get_Edge_Ids Methods get() Declaration static std::vector<int>&graph::Face<Cost>::Get_Edge_Ids::get(Face&f) get_key() Declaration static std::string graph::Face<Cost>::Get_Edge_Ids::get_key()"
  },
  "api/graph/faced-graph-configuration.html": {
    "href": "api/graph/faced-graph-configuration.html",
    "title": "Class graph::FacedGraphConfiguration | qiotoolkit",
    "keywords": "Class graph::FacedGraphConfiguration Inheritance graph::GraphConfiguration graph::FacedGraphConfiguration model::FacedGraphModelConfiguration"
  },
  "api/graph/faced-graph-configuration/get/slc/terms.html": {
    "href": "api/graph/faced-graph-configuration/get/slc/terms.html",
    "title": "Struct graph::FacedGraphConfiguration::Get_SLC_Terms | qiotoolkit",
    "keywords": "Struct graph::FacedGraphConfiguration::Get_SLC_Terms Methods get() Declaration static std::vector<SCL_Term>&graph::FacedGraphConfiguration::Get_SLC_Terms::get(FacedGraphConfiguration&graph_config) get_key() Declaration static std::string graph::FacedGraphConfiguration::Get_SLC_Terms::get_key()"
  },
  "api/graph/faced-graph.html": {
    "href": "api/graph/faced-graph.html",
    "title": "Class graph::FacedGraph | qiotoolkit",
    "keywords": "Class graph::FacedGraph Graph representation as a collection of nodes, edges, and faces. It assumes, beyond the assumptions for Graph, that: faces are identified with a face ID 0...(F-1), where F is the number of faces. Inheritance graph::Graph graph::FacedGraph Inherited Members validate get_sum_coefficient_degrees_total get_const_cost populate_node_edge_ids render nodes_size get_min_locality get_min_coupling_magnitude get_scale_factor output_map edges configure edge node set_allow_dup_merge init get_locality get_avg_locality edges_size get_max_coupling_magnitude map_output init_memory_check is_empty get_node_name_to_id_map Graph is_rescaled get_properties nodes rescale estimate_max_cost_diff get_accumulated_dependent_vars render ~Component Component get_status param get_class_name Constructors FacedGraph() Declaration graph::FacedGraph::FacedGraph() Methods face() Get a face by ID. Declaration const Face&graph::FacedGraph::face(int face_id) const faces() Access the vector of all faces. Declaration const std::vector<Face>&graph::FacedGraph::faces() const sort() Sort the ID lists of each node, edge, and face. Graph::sort Declaration void graph::FacedGraph::sort() override validate() Validate the FacedGraph. Ensure that node, edge, and face references are consistent. validate_face Graph::validate Declaration bool graph::FacedGraph::validate() const override validate_node() Validate a node, ensuring that: the node participants in at least one edge and face, all edges listed have a valid edge ID, and the node is referenced in those edges. Declaration bool graph::FacedGraph::validate_node(int node_indx) const override validate_edge() Validate an edge, ensuring that: edges on certain face types contain a node, all nodes listed have a valid edge ID, and the node is referenced in those edges. Declaration bool graph::FacedGraph::validate_edge(size_t edge_id) const override validate_face() Validate a face, ensuring that: SquaredLinearCombination faces contain an edge, only linear edges, and have like-terms combined; all edges listed have a valid face ID; and the face is referenced in those edges. Declaration bool graph::FacedGraph::validate_face(int face_id) const validate_counts() Declaration bool graph::FacedGraph::validate_counts(int node_id, size_t edge_id) const override configure_memory_check() Declaration void graph::FacedGraph::configure_memory_check(const utils::Json&json) configure() Configure graph from the input by configuring and organizing its component nodes, edges, and faces. Input is structured as a dictionary including a value for \"terms\" which is a list of dictionaries, where each such dictionary is either an edge (given with \"ids\" and \"c\" keys) or a face (given with \"type\", \"c\", and \"terms\" keys where \"terms\" has value of a list of edge dictionaries). Declaration void graph::FacedGraph::configure(const utils::Json&json) override configure() Declaration void graph::FacedGraph::configure(Configuration_T&config) populate() Declaration void graph::FacedGraph::populate(std::vector<Face>&temp_faces, std::vector<std::vector<Edge>>&temp_edges) populate_face() Declaration void graph::FacedGraph::populate_face(Face&new_face, std::vector<Edge>&face_edges)"
  },
  "api/graph/graph-attributes.html": {
    "href": "api/graph/graph-attributes.html",
    "title": "Class graph::GraphAttributes | qiotoolkit",
    "keywords": "Class graph::GraphAttributes Inheritance graph::GraphAttributes"
  },
  "api/graph/graph-configuration.html": {
    "href": "api/graph/graph-configuration.html",
    "title": "Class graph::GraphConfiguration | qiotoolkit",
    "keywords": "Class graph::GraphConfiguration Inheritance graph::GraphConfiguration"
  },
  "api/graph/graph-configuration/get/edges.html": {
    "href": "api/graph/graph-configuration/get/edges.html",
    "title": "Struct graph::GraphConfiguration::Get_Edges | qiotoolkit",
    "keywords": "Struct graph::GraphConfiguration::Get_Edges Methods get() Declaration static std::vector<EdgeType>&graph::GraphConfiguration<EdgeType, NodeType>::Get_Edges::get(GraphConfiguration&graph_config) get_key() Declaration static std::string graph::GraphConfiguration<EdgeType, NodeType>::Get_Edges::get_key()"
  },
  "api/graph/graph-configuration/get/nodes.html": {
    "href": "api/graph/graph-configuration/get/nodes.html",
    "title": "Struct graph::GraphConfiguration::Get_Nodes | qiotoolkit",
    "keywords": "Struct graph::GraphConfiguration::Get_Nodes Methods get() Declaration static std::vector<NodeType>&graph::GraphConfiguration<EdgeType, NodeType>::Get_Nodes::get(GraphConfiguration&graph_config) get_key() Declaration static std::string graph::GraphConfiguration<EdgeType, NodeType>::Get_Nodes::get_key()"
  },
  "api/graph/graph-properties.html": {
    "href": "api/graph/graph-properties.html",
    "title": "Struct graph::GraphProperties | qiotoolkit",
    "keywords": "Struct graph::GraphProperties Constructors GraphProperties() Declaration graph::GraphProperties::GraphProperties() Methods rescale() Declaration void graph::GraphProperties::rescale()"
  },
  "api/graph/graph.html": {
    "href": "api/graph/graph.html",
    "title": "Class graph::Graph | qiotoolkit",
    "keywords": "Class graph::Graph Graph representation as a collection of nodes and edges. It assumes that: nodes are identified with a NodeId 0..(N-1) and edges are identified with an EdgeId 0..(M-1), where N (M) is the number of nodes (edges). This is a concurrent adjacency list AND edge list representation of the graph (slightly slower than using a single representation, but more versatile). Note We use a templated approach such that the accessors for nodes and edges can return the actual type rather than a limited base class view of it. Repetition of a node_id in an edge is explicitly allowed and implies a duplication of the corresponding edge_id in the node. This can be used for, e.g., squared terms in a cost function represented as a graph. Hoewever, this approach is not efficient for very high order terms. Inheritance utils::Component graph::Graph Inherited Members ~Component Component get_status param get_class_name Constructors Graph() Declaration graph::Graph<Edge, Node>::Graph() Methods set_allow_dup_merge() Declaration void graph::Graph<Edge, Node>::set_allow_dup_merge(bool value) get_const_cost() Declaration double graph::Graph<Edge, Node>::get_const_cost() const is_empty() Declaration bool graph::Graph<Edge, Node>::is_empty() const node() Get a node by NodeId. Declaration const Node&graph::Graph<Edge, Node>::node(size_t node_indx) const nodes() Access the vector of all edges. Declaration const std::vector<Node>&graph::Graph<Edge, Node>::nodes() const edge() Get an edge by EdgeId. Declaration const Edge&graph::Graph<Edge, Node>::edge(size_t edge_id) const edges() Access the vector of all edges. Declaration const std::vector<Edge>&graph::Graph<Edge, Node>::edges() const sort() Sort each node and edge. This invokes sort_edge_ids() and sort_node_ids() on all nodes and edges, respectively. It does NOT modify the order of the nodes and edges in the graph (i.e., the node_ids and edge_ids are stable under this operation). The order in which nodes are listed in each edge (and vice-versa) should not matter for the typical application (and can be unsorted either from intial input or from calls to {add,remove}_{node,edge}). This functionality is provided in case an implementation relies on either ascending order or being able to identify repetitions easily. Node::sort_edge_ids Edge::sort_node_ids Declaration virtual void graph::Graph<Edge, Node>::sort() validate() Validate the Graph. Ensure that Nodes, Edges and their references are consistent. validate_edge validate_node validate_counts Declaration virtual bool graph::Graph<Edge, Node>::validate() const validate_node() Validate a Node Ensure that the Node participates in at least one Edge, all edges listed have a valid edge_ids, and the Node is referenced in those Edges. Declaration virtual bool graph::Graph<Edge, Node>::validate_node(int node_indx) const validate_edge() Validate an Edge Ensure that the Edge includes at least one Node, all nodes listed have a valid edge_ids, and the node is referenced in those Edges. Declaration virtual bool graph::Graph<Edge, Node>::validate_edge(size_t edge_id) const validate_counts() Declaration virtual bool graph::Graph<Edge, Node>::validate_counts(int node_id, size_t edge_id) const configure() Configure the graph by configuring its components. { \"terms\": [...], \"variables\": [...], } Note The group name terms is a side effect of the cost_function input format. Declaration void graph::Graph<Edge, Node>::configure(const utils::Json&json) override configure() Declaration void graph::Graph<Edge, Node>::configure(Configuration_T&config) render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure graph::Graph<Edge, Node>::render() const override map_output() Declaration int graph::Graph<Edge, Node>::map_output(int internal_id) const output_map() Declaration const std::map<int, int>&graph::Graph<Edge, Node>::output_map() const nodes_size() Declaration size_t graph::Graph<Edge, Node>::nodes_size() const edges_size() Declaration size_t graph::Graph<Edge, Node>::edges_size() const get_locality() Declaration uint32_t graph::Graph<Edge, Node>::get_locality() const get_min_locality() Declaration uint32_t graph::Graph<Edge, Node>::get_min_locality() const get_avg_locality() Declaration double graph::Graph<Edge, Node>::get_avg_locality() const get_accumulated_dependent_vars() Declaration uint64_t graph::Graph<Edge, Node>::get_accumulated_dependent_vars() const get_max_coupling_magnitude() Declaration double graph::Graph<Edge, Node>::get_max_coupling_magnitude() const get_min_coupling_magnitude() Declaration double graph::Graph<Edge, Node>::get_min_coupling_magnitude() const get_sum_coefficient_degrees_total() Declaration uint64_t graph::Graph<Edge, Node>::get_sum_coefficient_degrees_total() const get_scale_factor() Declaration double graph::Graph<Edge, Node>::get_scale_factor() const is_rescaled() Declaration bool graph::Graph<Edge, Node>::is_rescaled() const get_properties() Declaration const GraphProperties&graph::Graph<Edge, Node>::get_properties() const rescale() Declaration void graph::Graph<Edge, Node>::rescale() estimate_max_cost_diff() Declaration double graph::Graph<Edge, Node>::estimate_max_cost_diff() const get_node_name_to_id_map() Declaration const std::map<int, int>&graph::Graph<Edge, Node>::get_node_name_to_id_map() const populate_node_edge_ids() Build the adjacency list from the edge list. This is used when the graph input is represented solely as an edge list to infer the corresponding adjacencly list representation. Declaration void graph::Graph<Edge, Node>::populate_node_edge_ids() configure_memory_check() Declaration void graph::Graph<Edge, Node>::configure_memory_check(const utils::Json&json) init_memory_check() Declaration void graph::Graph<Edge, Node>::init_memory_check() init() Declaration void graph::Graph<Edge, Node>::init()"
  },
  "api/graph/graph/get/edges.html": {
    "href": "api/graph/graph/get/edges.html",
    "title": "Struct graph::Graph::Get_Edges | qiotoolkit",
    "keywords": "Struct graph::Graph::Get_Edges Methods get() Declaration static std::vector<Edge>&graph::Graph<Edge, Node>::Get_Edges::get(Graph&g) get_key() Declaration static std::string graph::Graph<Edge, Node>::Get_Edges::get_key()"
  },
  "api/graph/graph/get/nodes.html": {
    "href": "api/graph/graph/get/nodes.html",
    "title": "Struct graph::Graph::Get_Nodes | qiotoolkit",
    "keywords": "Struct graph::Graph::Get_Nodes Methods get() Declaration static std::vector<Node>&graph::Graph<Edge, Node>::Get_Nodes::get(Graph&g) get_key() Declaration static std::string graph::Graph<Edge, Node>::Get_Nodes::get_key()"
  },
  "api/graph/index.html": {
    "href": "api/graph/index.html",
    "title": "Graph | qiotoolkit",
    "keywords": "Graph"
  },
  "api/graph/locality.html": {
    "href": "api/graph/locality.html",
    "title": "Class graph::Locality | qiotoolkit",
    "keywords": "Class graph::Locality Locality. Histogram and average number of nodes participating in an edge of the graph. (This is relevant for graph with hyper-edges, i.e., when more than 2 nodes can participate in an edge). Inheritance graph::Property graph::Locality Inherited Members ~Property Methods compute() Declaration utils::Structure graph::Locality<Graph>::compute(const Graph&g) const override"
  },
  "api/graph/node-with-faces.html": {
    "href": "api/graph/node-with-faces.html",
    "title": "Class graph::NodeWithFaces | qiotoolkit",
    "keywords": "Class graph::NodeWithFaces This node class extends the regular Node class. The key differences are that NodeWithFaces objects maintain a list of face IDs to which the node belongs and sorts its edge IDS according to the faces to which the corresponding edges belong. No validation on such edges belonging to the faces is handled here. Though NodeWithFaces inherits edge_ids_ from Node, it goes unused. Inheritance graph::Node graph::NodeWithFaces Inherited Members get_status render configure remove_edge_id Node clear_edge_ids Node ~Component Component param get_class_name Constructors NodeWithFaces() Create a node participating in a given set of edges. Declaration graph::NodeWithFaces::NodeWithFaces(const std::vector<size_t>&edge_ids={}) Methods edge_ids() Return the list of edge IDs this node participates in, in the first face. The default edge list corresponds to the 0th face, which is typically the catch-all Combination face for non-SLC terms. Declaration const std::vector<size_t>&graph::NodeWithFaces::edge_ids() const override edge_ids() Return the list of edge IDs this node participates in for a particular face. Declaration const std::vector<size_t>&graph::NodeWithFaces::edge_ids(int face_id) const edge_ids_by_face() Return the list of edge ID lists (by face) this node participates in. Declaration const std::vector<std::vector<size_t>>&graph::NodeWithFaces::edge_ids_by_face() const face_ids() Return the list of face IDs this node participates in. Declaration const std::vector<int>&graph::NodeWithFaces::face_ids() const sort_edge_ids() Ensure edge IDs are in ascending order. Declaration void graph::NodeWithFaces::sort_edge_ids() override add_edge_id() Add an edge to this node at a particular face. Declaration void graph::NodeWithFaces::add_edge_id(size_t edge_id) override add_edge_id() Declaration void graph::NodeWithFaces::add_edge_id(size_t edge_id, int face_id) sort_face_ids() Ensure face IDs are in ascending order. Declaration void graph::NodeWithFaces::sort_face_ids() add_face_id() Add a face to this node. Declaration void graph::NodeWithFaces::add_face_id(int face_id) contains_face_id() Search vector of face IDs for a particular ID. Declaration bool graph::NodeWithFaces::contains_face_id(int face_id) edges_count() Declaration size_t graph::NodeWithFaces::edges_count() const faces_count() Declaration size_t graph::NodeWithFaces::faces_count() const memory_estimate() Declaration static size_t graph::NodeWithFaces::memory_estimate(size_t num_edges)"
  },
  "api/graph/node.html": {
    "href": "api/graph/node.html",
    "title": "Class graph::Node | qiotoolkit",
    "keywords": "Class graph::Node Base class for graph nodes. A node's core property is the list of edges it participates in. This is a list of numeric edge_ids which only makes sense in the context of the Graph the note belongs to. The node does not know its own NodeId as this information might change when the graph is manipulated (nodes ids are modified to ensure they form a contiguous set 0..N). The node does not perform any validation on the list of edge_id's it is given; Ensuring validity of these is within the responsibility of the Graph. If you need additional properties on the node, you can inherit from this base class (make sure to also overload configure accordingly). Inheritance utils::Component graph::Node graph::NodeWithFaces Inherited Members ~Component Component param get_class_name Constructors Node() Create an empty node. (Not participating in any edges). Declaration graph::Node::Node() Node() Create a node participating in a given set of edges. Declaration graph::Node::Node(const std::vector<size_t>&edge_ids) Methods edge_ids() Return the list of edges this node participates in. Declaration const std::vector<size_t>&graph::Node::edge_ids() const clear_edge_ids() Remove all edge_ids. This should typically be invoked from the Graph [not directly], as remove edge_ids implies the node's NodeId should be removed from the corresponding edges. Graph::remove_node Declaration void graph::Node::clear_edge_ids() sort_edge_ids() Ensure edge_ids are in ascending order. Declaration void graph::Node::sort_edge_ids() add_edge_id() Add an edge to this node. Declaration void graph::Node::add_edge_id(size_t edge_id) remove_edge_id() Remove an edge from this node. Declaration void graph::Node::remove_edge_id(size_t edge_id) configure() Read/Write this node from/to a serializer. Declaration void graph::Node::configure(const utils::Json&json) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure graph::Node::render() const override get_status() get_status shows a simplified state representation Like render, this produces a structured representation of the object's state. However, it is intended to be simpler in nature with the purpose of rendering the object during stream-output and logging. By default, it will fall back to the full render, but overloading this allows distinguising how the object looks during LOG vs full output. Declaration utils::Structure graph::Node::get_status() const override memory_estimate() Declaration static size_t graph::Node::memory_estimate(size_t num_edges)"
  },
  "api/graph/node/get/edge/ids.html": {
    "href": "api/graph/node/get/edge/ids.html",
    "title": "Struct graph::Node::Get_Edge_Ids | qiotoolkit",
    "keywords": "Struct graph::Node::Get_Edge_Ids Methods get() Declaration static std::vector<size_t>&graph::Node::Get_Edge_Ids::get(Node&node) get_key() Declaration static std::string graph::Node::Get_Edge_Ids::get_key()"
  },
  "api/graph/property.html": {
    "href": "api/graph/property.html",
    "title": "Class graph::Property | qiotoolkit",
    "keywords": "Class graph::Property graph::Property interface An API to calculate properties of a graph g. Each implementation is expected to implement a compute method that returns the evaluated property as a structure. Inheritance graph::Property graph::Clustering graph::Couplings graph::Degree graph::Locality graph::Size Methods ~Property() Declaration virtual graph::Property<Graph>::~Property() compute() Declaration virtual utils::Structure graph::Property<Graph>::compute(const Graph&g) const =0"
  },
  "api/graph/scl/term.html": {
    "href": "api/graph/scl/term.html",
    "title": "Class graph::SCL_Term | qiotoolkit",
    "keywords": "Class graph::SCL_Term Inheritance graph::SCL_Term Constructors SCL_Term() Declaration graph::SCL_Term::SCL_Term() Methods get_edges() Declaration std::vector<Edge>&graph::SCL_Term::get_edges() get_cost() Declaration Cost&graph::SCL_Term::get_cost()"
  },
  "api/graph/scl/term/get/cost.html": {
    "href": "api/graph/scl/term/get/cost.html",
    "title": "Struct graph::SCL_Term::Get_Cost | qiotoolkit",
    "keywords": "Struct graph::SCL_Term::Get_Cost Methods get() Declaration static Cost&graph::SCL_Term::Get_Cost::get(SCL_Term&val) get_key() Declaration static std::string graph::SCL_Term::Get_Cost::get_key()"
  },
  "api/graph/scl/term/get/edges.html": {
    "href": "api/graph/scl/term/get/edges.html",
    "title": "Struct graph::SCL_Term::Get_Edges | qiotoolkit",
    "keywords": "Struct graph::SCL_Term::Get_Edges Methods get() Declaration static std::vector<Edge>&graph::SCL_Term::Get_Edges::get(SCL_Term&config) get_key() Declaration static std::string graph::SCL_Term::Get_Edges::get_key()"
  },
  "api/graph/size.html": {
    "href": "api/graph/size.html",
    "title": "Class graph::Size | qiotoolkit",
    "keywords": "Class graph::Size graph::Size Denote the number of nodes and edges in the graph. Inheritance graph::Property graph::Size Inherited Members ~Property Methods compute() Declaration utils::Structure graph::Size<Graph>::compute(const Graph&g) const override"
  },
  "api/index.html": {
    "href": "api/index.html",
    "title": "API documentation | qiotoolkit",
    "keywords": "API documentation This section of the qiotoolkit documentation contains auto-generated C++ interface-descriptions for the components in the repository. For the input and output specifications of the compiled binary, check out the Specifications Section For a step-by-step guide on getting started with qiotoolkit, please refer to the Tutorial Section Models All qiotoolkit models are expected to inherit from the markov::Model base class, which defines the required methods to interface with solvers. Currently, the code base contains implementations for: Ising Model PUBO Blume-Capel (Spin-1, i.e., 3 states) Potts (p-state, no notion of \"neighboring values\") Clock (p-state, neighboring values + periodic boundaries) TSP (single-tour on fully connected graph) Poly (Experimental: Arbitrary polynomial cost function with parameters) Solvers All qiotoolkit solvers are expected to inherit from the solver::AbstractSolver base class, which defines the interface to the simulation runner. Current built-in solvers include: Simulated Annealing Parallel Tempering Population Annealing Substochastic Monte-Carlo MUlti-objective Replica Exchange (Experimental) Utilities Utils - general purpose utilties for I/O and debugging Matcher - Specification of pre-conditions for valid input Graph - Graph representation Schedule - Generators for annealing sequences and temperature sets Observe - Built-in measurement functionality"
  },
  "api/markov/cluster-walker.html": {
    "href": "api/markov/cluster-walker.html",
    "title": "Class markov::ClusterWalker | qiotoolkit",
    "keywords": "Class markov::ClusterWalker ClusterWalker is an abstract base class for quantum like space explorers A ClusterWalker stores multiple state and costs and has pointers to a model and random number generator to make steps. It is specifically designed to allow copy assignment (assuming the Model::State_T does), such that it can be duplicated in population markovs. The Walker provides a base implementation of the step function, which proposes a random new states, computes the cost differences, uses virtual method to decide whether to create cluster binded variables from multiple states invokes a pure virtual method to decide whether to accept, and modifies the internal states using cluster if true is returned. From this we can implement a QuantumWalker by returning acceptance rate based on quantum transverse field. Inheritance utils::Component markov::ClusterWalker markov::QuantumWalker Inherited Members configure render ~Component Component get_status param get_class_name Constructors ClusterWalker() The rng and model must be set before the object can be used. cost is only intialized after a call to init() Declaration markov::ClusterWalker<Model>::ClusterWalker() Methods init() Must be called after the rng_ and model_ have been set. It initializes the walker to a random state and precomputes its current cost. Note This is typically the only direct call to calculate_cost; from here the current cost_ is updated from the computed difference at each step. Declaration virtual void markov::ClusterWalker<Model>::init(size_t n_states=1) init_state() Declaration void markov::ClusterWalker<Model>::init_state(size_t state_index, ::utils::RandomGenerator&rng) accept() Implementations of the walker interface must provide a method to decide which steps to accept/reject. Currently this is only based on cost_difference, but the state or transition could be added. Declaration virtual bool markov::ClusterWalker<Model>::accept(const typename Model::Cost_T&cost_difference, ::utils::RandomGenerator&rng)=0 accept_cluster() Implementations of the walker interface must provide a method to decide which cluster to accept/reject. Declaration virtual bool markov::ClusterWalker<Model>::accept_cluster(::utils::RandomGenerator&rng)=0 get_transition() For models with a 'size_t' transition type, do an ordered sweep. Declaration std::enable_if<std::is_same<TT, size_t>::value, void>::type markov::ClusterWalker<Model>::get_transition(size_t variable_index, typename Model::Transition_T&transition, utils::RandomGenerator&) get_transition() For any other transition type, do a random sweep. Declaration std::enable_if<!std::is_same<TT, size_t>::value, void>::type markov::ClusterWalker<Model>::get_transition(size_t, typename Model::Transition_T&transition, utils::RandomGenerator&rng) set_model() Provide a pointer to the (non-owned) model to use for cost calculations and proposing new steps. Declaration void markov::ClusterWalker<Model>::set_model(const Model *model) model() Return a constant reference to the model being used. Declaration const Model&markov::ClusterWalker<Model>::model() get_evaluation_counter() Declaration solver::EvaluationCounter markov::ClusterWalker<Model>::get_evaluation_counter() add_difference_evaluations() Declaration void markov::ClusterWalker<Model>::add_difference_evaluations(size_t number_evals) add_function_evaluations() Declaration void markov::ClusterWalker<Model>::add_function_evaluations(size_t number_evals) reset_evaluation_counter() Declaration void markov::ClusterWalker<Model>::reset_evaluation_counter() get_lowest_cost() Declaration Model::Cost_T markov::ClusterWalker<Model>::get_lowest_cost() const get_lowest_state() Declaration const Model::State_T&markov::ClusterWalker<Model>::get_lowest_state() const get_states() Declaration const std::vector<typename Model::State_T>&markov::ClusterWalker<Model>::get_states() get_costs() Declaration const std::vector<typename Model::Cost_T>&markov::ClusterWalker<Model>::get_costs() size() Declaration size_t markov::ClusterWalker<Model>::size() try_form_clusters() Declaration void markov::ClusterWalker<Model>::try_form_clusters(size_t state_index, const typename Model::Transition_T&transition, utils::RandomGenerator&rng) try_flip_clusters() Declaration void markov::ClusterWalker<Model>::try_flip_clusters(size_t state_index, utils::RandomGenerator&rng) calculate_cost_diff() For models derived from model::FacedGraphModel, an extra parameter for a cost object is necessary to communicate cached calculations. Declaration std::enable_if<!std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, void>::type markov::ClusterWalker<Model>::calculate_cost_diff(size_t state_index, const typename Model::Transition_T&transition) calculate_cost_diff() For models derived from model::FacedGraphModel, an extra parameter for a cost object is necessary to communicate cached calculations. Declaration std::enable_if<std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, void>::type markov::ClusterWalker<Model>::calculate_cost_diff(size_t state_index, const typename Model::Transition_T&transition) apply_transition() For models derived from model::FacedGraphModel, an extra parameter for a cost object is necessary to communicate cached calculations. Declaration std::enable_if<!std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, void>::type markov::ClusterWalker<Model>::apply_transition(size_t state_index, const typename Model::Transition_T&transition) apply_transition() For models derived from model::FacedGraphModel, an extra parameter for a cost object is necessary to communicate cached calculations. Declaration std::enable_if<std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, void>::type markov::ClusterWalker<Model>::apply_transition(size_t state_index, const typename Model::Transition_T&transition) save_lowest() Store the current lowest state as the lowest. Declaration void markov::ClusterWalker<Model>::save_lowest() memory_estimate() Estimate memory consumtion using model parameters. Declaration static size_t markov::ClusterWalker<Model>::memory_estimate(const Model&model, size_t n_states) compare() Declaration static bool markov::ClusterWalker<Model>::compare(const ClusterWalker<Model>&w1, const ClusterWalker<Model>&w2)"
  },
  "api/markov/hill-climbing-walker.html": {
    "href": "api/markov/hill-climbing-walker.html",
    "title": "Class markov::HillClimbingWalker | qiotoolkit",
    "keywords": "Class markov::HillClimbingWalker A Hill climbing walker accepts every move reducing the cost and keeps moving if cost is same. NOTE: computing cost_difference is typically not superfluous as it will be needed within Walker::make_step() to update the cached cost_. Inheritance markov::Walker markov::HillClimbingWalker Inherited Members get_lowest_state check_lowest state reset_evaluation_counter make_sweep swap_state make_sweep set_model model Walker make_sweeps make_sweep attempt_transition get_evaluation_counter set_rng save_lowest rng memory_estimate apply_transition init get_lowest_cost make_step cost init apply_scale_factor compare attempt_transition apply_transition verify_cost_difference configure render ~Component Component get_status param get_class_name Methods accept() Implementations of the walker interface must provide a method to decide which steps to accept/reject. Currently this is only based on cost_difference, but the state or transition could be added. Declaration bool markov::HillClimbingWalker<Model>::accept(const typename Model::Cost_T&cost_diff) override"
  },
  "api/markov/index.html": {
    "href": "api/markov/index.html",
    "title": "Markov Chain | qiotoolkit",
    "keywords": "Markov Chain"
  },
  "api/markov/linear-sweep-model.html": {
    "href": "api/markov/linear-sweep-model.html",
    "title": "Class markov::LinearSweepModel | qiotoolkit",
    "keywords": "Class markov::LinearSweepModel Base class for implementations which wish to have full control over how an entire sweep is performed. Implementations should. Defer to accept(delta_cost) -> bool when deciding whether to apply a proposed change (this is typically the metropolis acceptance rate) 2) Call check_lowest() whenever a potentially new lowest cost is found (check_lowest, as opposed to save_lowest, does store the state as new best only if its cost is lower than the current best) 3) Keep \"cost\" updated to the one corresponding to the current state (at least before every call to check_lowest() and after the sweep is completed). Inheritance markov::Model markov::LinearSweepModel Inherited Members get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model configure state_only_memory_estimate get_term_count get_sweep_size init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate rescale render calculate_cost_difference get_random_transition get_const_cost configure get_random_state match_version get_version ~BaseModel configure BaseModel get_identifier render ~Component Component get_status param get_class_name Methods make_linear_sweep() Declaration virtual void markov::LinearSweepModel<State>::make_linear_sweep(double&cost, State&state, std::function<bool(double)>accept, std::function<void(void)>check_lowest) const =0"
  },
  "api/markov/metropolis.html": {
    "href": "api/markov/metropolis.html",
    "title": "Class markov::Metropolis | qiotoolkit",
    "keywords": "Class markov::Metropolis Metropolis Algorithm. Random sampling approximating the Boltzmann distribution using a Markov chain Monte Carlo approach. Inheritance markov::Walker markov::Metropolis Inherited Members get_lowest_state check_lowest state reset_evaluation_counter make_sweep swap_state make_sweep set_model model Walker make_sweeps make_sweep attempt_transition get_evaluation_counter set_rng save_lowest rng apply_transition init get_lowest_cost make_step cost init apply_scale_factor compare attempt_transition apply_transition verify_cost_difference configure render ~Component Component get_status param get_class_name Constructors Metropolis() Create a Metropolis instance with uninitialized model and state. Declaration markov::Metropolis<Model>::Metropolis() Methods temperature() Get the current temperature for sampling. Declaration double markov::Metropolis<Model>::temperature() const beta() Get the current inverse sampling temperature. Declaration double markov::Metropolis<Model>::beta() const set_temperature() Set the sampling temperature (also updates beta_). Declaration void markov::Metropolis<Model>::set_temperature(double temperature) set_beta() Declaration void markov::Metropolis<Model>::set_beta(double beta) accept() Decide whether to accept a given cost increase. Declaration bool markov::Metropolis<Model>::accept(const typename Model::Cost_T&cost_diff) override memory_estimate() Estimate memory consumption using model parameters. Declaration static size_t markov::Metropolis<Model>::memory_estimate(const Model&model)"
  },
  "api/markov/model.html": {
    "href": "api/markov/model.html",
    "title": "Class markov::Model | qiotoolkit",
    "keywords": "Class markov::Model Interface for Markov models. Implementations of the Markov model interface must provide A calculate_cost (to calculate the cost of an entire state as well as the calculate_cost_difference incurred by a specific transition). The means to generate the Monte Carlo chain (random_state, random_transition and apply_transition). The interface is templated for both the underlying Markov state and transition, such that users can decide how to represent each. The base classes markov::State and markov::Transition are provided as guidelines for designing these model-specifc types, but they are not enforced to be used. (That is, you may use Model<std::string, int> if a string is sufficient to represent your state and an int defines a transition. State Transition For initial testing, you may also opt to use the simplified classes markov::SimpleTransition and markov::SimpleModel. SimpleModel SimpleTransition Inheritance model::BaseModel markov::Model Inherited Members match_version get_version ~BaseModel configure BaseModel get_identifier ~Component Component get_status param get_class_name Constructors Model() Create an uninitialized model. Declaration markov::Model<State, Transition, Cost>::Model() Methods set_step_limit() Declaration void markov::Model<State, Transition, Cost>::set_step_limit(uint64_t limit) configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void markov::Model<State, Transition, Cost>::configure(const utils::Json&json) override configure() Declaration void markov::Model<State, Transition, Cost>::configure(model::BaseModelConfiguration&configuration) init() Initializes internal data structures (guaranteed to be called after configure()). Declaration void markov::Model<State, Transition, Cost>::init() override calculate_cost() Definition of the cost function. Evaluate the entire cost function for the state being passed. For instance, in the case of a model from statistical mechanics, this would be the Hamiltonian. Declaration virtual Cost_T markov::Model<State, Transition, Cost>::calculate_cost(const State_T&state) const =0 calculate_cost_difference() Partial evaluation of the cost function. This method should calculate the difference in cost if we move from state (=before) to the one resulting from applying transition to state (=after): \\Delta_{C} = C_{\\mathrm{after}} - C_{\\mathrm{before}} In code: State state = get_random_state(rng); Transition transition = get_random_transition(rng); double cost_before = calculate_cost(state); double cost_diff = calculate_cost_difference(state, transition); apply_transition(transition, state); // modify state double cost_after = calculate_cost(state); // before + diff should correspond to after (up to double precision) assert(cost_before + cost_diff == cost_after); Declaration virtual Cost_T markov::Model<State, Transition, Cost>::calculate_cost_difference(const State_T&state, const Transition_T&transition) const =0 get_random_state() Return a valid random state for the model. The various algorithms don't know how to initialize a valid state; this allows them to start with a random one. NOTE: The rng being passed should be used for randomness (as opposed to creating one for the model), as multiple threads can potentially use the same underlying model (albeit with separate rngs). Declaration virtual State_T markov::Model<State, Transition, Cost>::get_random_state(utils::RandomGenerator&rng) const =0 has_initial_configuration() Check if model has initial configuration. Declaration virtual bool markov::Model<State, Transition, Cost>::has_initial_configuration() const get_initial_configuration_state() Return a state from initial configuration. Declaration virtual State_T markov::Model<State, Transition, Cost>::get_initial_configuration_state() const get_random_transition() Return a random transition starting at state. The various algorithms don't know how to move throuhg markov space; this allows them to pick a random direction given a starting point. By definition, the next markov state should only depend on the current one, so only the last one is passed to this function. \"random\" does not necessitate equi-distributed here (you may choose a different distribution if your model dictates it). However, the the typical choise is to pick randomly from all possible moves at state with equal probability. Declaration virtual Transition_T markov::Model<State, Transition, Cost>::get_random_transition(const State_T&state, utils::RandomGenerator&rng) const =0 apply_transition() Apply a transition to a state. This changes the configuration represented by *state. Depending on the optimization algorithm, transition can either be applied conditionally or alaways (e.g., population dynamics). Separating the functionality into the three interfaces random_transition, calculate_cost_difference, apply_transition leaves control over the strategy with the optimization method. Declaration virtual void markov::Model<State, Transition, Cost>::apply_transition(const Transition&transition, State&state) const =0 render_state() Render a state of this model. Default implementation of how a state of this model is rendered (by' invoking the state's proper rendering mechanic). Overloading this method allows a model implementation to customize how a state is printed. This is relevant if the rendering needs to depend on model parameters in addition to the state. Declaration virtual utils::Structure markov::Model<State, Transition, Cost>::render_state(const State_T&state) const get_sweep_size() Return the number of (attempted) transition to consider one sweep. This number is expected to scale roughly with the number of variables in the model such that, on average, each variable is selected once per sweep (if selected randomly). Declaration virtual size_t markov::Model<State, Transition, Cost>::get_sweep_size() const get_term_count() Declaration virtual size_t markov::Model<State, Transition, Cost>::get_term_count() const get_benchmark_properties() Collect statistics about the model being simulated. The output of this method will appear in the Response.benchmark.input_data field. By default, we do not collect any model statistics implementations of this interface must override this method to fill the output. Declaration virtual utils::Structure markov::Model<State, Transition, Cost>::get_benchmark_properties() const render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure markov::Model<State, Transition, Cost>::render() const override is_rescaled() Declaration virtual bool markov::Model<State, Transition, Cost>::is_rescaled() const rescale() Declaration virtual void markov::Model<State, Transition, Cost>::rescale() get_scale_factor() Declaration virtual Cost_T markov::Model<State, Transition, Cost>::get_scale_factor() const get_const_cost() Declaration virtual Cost_T markov::Model<State, Transition, Cost>::get_const_cost() const is_empty() Declaration virtual bool markov::Model<State, Transition, Cost>::is_empty() const =0 state_memory_estimate() Estimate memory consumption of model state and lowest state in bytes, using paramters known by model. Numerous states will be created by solvers. Estimation of memory consumption is needed to avoid memory overflow. States may have complex nature, so knowledge of model parameters should be used for accurate estimation. For prototype models return 0 could be used. Declaration virtual size_t markov::Model<State, Transition, Cost>::state_memory_estimate() const =0 state_only_memory_estimate() Declaration virtual size_t markov::Model<State, Transition, Cost>::state_only_memory_estimate() const =0 estimate_max_cost_diff() Declaration virtual double markov::Model<State, Transition, Cost>::estimate_max_cost_diff() const estimate_min_cost_diff() Declaration virtual double markov::Model<State, Transition, Cost>::estimate_min_cost_diff() const"
  },
  "api/markov/quantum-walker.html": {
    "href": "api/markov/quantum-walker.html",
    "title": "Class markov::QuantumWalker | qiotoolkit",
    "keywords": "Class markov::QuantumWalker Quantum Algorithm. Inheritance markov::ClusterWalker markov::QuantumWalker Inherited Members init_state get_lowest_state try_form_clusters add_difference_evaluations size init try_flip_clusters calculate_cost_diff ClusterWalker save_lowest calculate_cost_diff apply_transition reset_evaluation_counter get_transition model get_costs set_model get_states get_evaluation_counter memory_estimate apply_transition get_transition add_function_evaluations compare get_lowest_cost configure render ~Component Component get_status param get_class_name Constructors QuantumWalker() Create a Metropolis instance with uninitialized model and state. Declaration markov::QuantumWalker<Model>::QuantumWalker() Methods beta() Declaration double markov::QuantumWalker<Model>::beta() const set_beta() Declaration void markov::QuantumWalker<Model>::set_beta(double beta) bond_probability() Declaration double markov::QuantumWalker<Model>::bond_probability() const set_bond_probability() Declaration void markov::QuantumWalker<Model>::set_bond_probability(double bond_prob) accept() Decide whether to accept a given cost increase. Declaration bool markov::QuantumWalker<Model>::accept(const typename Model::Cost_T&cost_diff, ::utils::RandomGenerator&rng) override accept_cluster() Decide whether to accept addition to cluster. Declaration bool markov::QuantumWalker<Model>::accept_cluster(::utils::RandomGenerator&rng) override memory_estimate() Estimate memory consumtion using model parameters. Declaration static size_t markov::QuantumWalker<Model>::memory_estimate(const Model&model)"
  },
  "api/markov/random-walker.html": {
    "href": "api/markov/random-walker.html",
    "title": "Class markov::RandomWalker | qiotoolkit",
    "keywords": "Class markov::RandomWalker A RandomWalker accepts every move. Note computing cost_difference is typically not superfluous as it will be needed within Walker::make_step() to update the cached cost_. Inheritance markov::Walker markov::RandomWalker Inherited Members get_lowest_state check_lowest state reset_evaluation_counter make_sweep swap_state make_sweep set_model model Walker make_sweeps make_sweep attempt_transition get_evaluation_counter set_rng save_lowest rng memory_estimate apply_transition init get_lowest_cost make_step cost init apply_scale_factor compare attempt_transition apply_transition verify_cost_difference configure render ~Component Component get_status param get_class_name Methods accept() Implementations of the walker interface must provide a method to decide which steps to accept/reject. Currently this is only based on cost_difference, but the state or transition could be added. Declaration bool markov::RandomWalker<Model>::accept(const typename Model::Cost_T&) override"
  },
  "api/markov/simple-model.html": {
    "href": "api/markov/simple-model.html",
    "title": "Class markov::SimpleModel | qiotoolkit",
    "keywords": "Class markov::SimpleModel Simplified Model representation. This base class has a predifined transition type (SimpleTransition) and implementations for calculate_cost_difference and apply_transition. These implementations make no assumptions about the model and they are, as a result, typically inefficient (i.e., calculating the whole calculate_cost in lieu of only the changed parts). The base class is provided as a means for rapid prototyping because it requires less interfaces to be implemented. Inheritance markov::Model markov::SimpleModel Inherited Members get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model configure state_only_memory_estimate get_term_count get_sweep_size init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate rescale render get_random_transition get_const_cost configure get_random_state match_version ~BaseModel configure BaseModel render ~Component Component get_status param get_class_name Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string markov::SimpleModel<State>::get_identifier() const override=0 get_version() Returns the version of the input format this implementation expects. Declaration std::string markov::SimpleModel<State>::get_version() const override=0 calculate_cost_difference() Default calculate_cost_difference implementation. This computes the expected difference of a transition by invoking calculate_cost twice. This is almost always inefficient (at the very least, one could cache the value for the starting point). SimpleModel therefore only serves the purpose of rapidly writing a base implementation to check against. Note This relies on using SimpleTransition with the model - such that both the start and end state of the transition can easily be accessed. Declaration Cost_T markov::SimpleModel<State>::calculate_cost_difference(const State_T&state, const Transition_T&transition) const override apply_transition() Default apply_transition implementation. Since SimpleTransition has Transition as a base, we can just use its apply method (which, in turn, will set state to to()). Declaration virtual void markov::SimpleModel<State>::apply_transition(const Transition_T&transition, State_T&state) const override"
  },
  "api/markov/simple-transition.html": {
    "href": "api/markov/simple-transition.html",
    "title": "Class markov::SimpleTransition | qiotoolkit",
    "keywords": "Class markov::SimpleTransition Predefined simple markov transition. This is a transition class which works for any State it simply keeps track of the state before and after the transition. This is convenient for initial testing but not efficient because: New states are repeatedly created for each (potential transition) calculate_cost_difference() cannot exploit knowledge about what changed to partially recalculate the calculate_cost (apart from calculating the diff from state and target). Inheritance markov::Transition markov::SimpleTransition Constructors SimpleTransition() Create a simple transition to an explicit target state. Declaration markov::SimpleTransition<State>::SimpleTransition(State target) Methods target() Accessor for the target state. Declaration const State&markov::SimpleTransition<State>::target() const"
  },
  "api/markov/state.html": {
    "href": "api/markov/state.html",
    "title": "Class markov::State | qiotoolkit",
    "keywords": "Class markov::State Representation of a State in the Markov chain. A markov state is one possible configuration of the underlying system. To form a markov chain it is modified according to allowed Transitions into \"neighboring\" states which typically differ from it by a simple operation (such as the change of one variable of the configuration). There are no API requirements on the state itself other than that it must be serializable (i.e., implement the Serializable interface) to allow checkpointing. utils::Renderable markov::Transition Inheritance utils::Component markov::State examples::SoftSpinState model::Binary model::BinaryWithCounter model::BlumeCapelState model::ClockState model::IsingState model::Partition model::Permutation model::PolyState model::PottsState model::PuboBinaryAdaptive Inherited Members configure render ~Component Component get_status param get_class_name"
  },
  "api/markov/tabu-walker.html": {
    "href": "api/markov/tabu-walker.html",
    "title": "Class markov::TabuWalker | qiotoolkit",
    "keywords": "Class markov::TabuWalker Tabu walker. Inheritance markov::Walker markov::TabuWalker Inherited Members get_lowest_state check_lowest state reset_evaluation_counter swap_state set_model model Walker make_sweeps attempt_transition get_evaluation_counter set_rng save_lowest rng apply_transition get_lowest_cost make_step cost init apply_scale_factor compare attempt_transition apply_transition verify_cost_difference configure render ~Component Component get_status param get_class_name Constructors TabuWalker() Create a TabuWalker instance with uninitialized model and state. Declaration markov::TabuWalker<Model>::TabuWalker() Methods init() Must be called after the rng_ and model_ have been set. It initializes the walker to a random state and precomputes its current cost. Note This is typically the only direct call to calculate_cost; from here the current cost_ is updated from the computed difference at each step. Declaration void markov::TabuWalker<Model>::init() override accept() Decide whether to accept a given cost increase. Declaration bool markov::TabuWalker<Model>::accept(const typename Model::Cost_T&cost_diff) override make_sweep() Declaration void markov::TabuWalker<Model>::make_sweep() set_tenure() Set tabu tenure parameter. Declaration void markov::TabuWalker<Model>::set_tenure(unsigned int tabu_tenure) memory_estimate() Estimate memory consumption using model parameters. Declaration static size_t markov::TabuWalker<Model>::memory_estimate(const Model&model) clear_tabu_list() Declaration std::enable_if<std::is_same<TT, size_t>::value, void>::type markov::TabuWalker<Model>::clear_tabu_list(void) clear_tabu_list() Declaration std::enable_if<!std::is_same<TT, size_t>::value, void>::type markov::TabuWalker<Model>::clear_tabu_list(void) get_transition() Declaration std::enable_if<std::is_same<TT, size_t>::value, TT>::type markov::TabuWalker<Model>::get_transition(size_t i) get_transition() Declaration std::enable_if<!std::is_same<TT, size_t>::value, TT>::type markov::TabuWalker<Model>::get_transition(size_t) local_search_init() Declaration void markov::TabuWalker<Model>::local_search_init() local_search_sweep() Declaration void markov::TabuWalker<Model>::local_search_sweep()"
  },
  "api/markov/transition.html": {
    "href": "api/markov/transition.html",
    "title": "Class markov::Transition | qiotoolkit",
    "keywords": "Class markov::Transition Base class for a markov transition. A transition describes a proposed move from the current to the next state. Depending on the algorithm, this move might be conditionally accepted, in which case the algorithm invokes apply_transition(transition, &state) on the model. Note When deciding how to represent your transition, try to envision the minimum amount of information required for apply_transition to do its work. This is typically also what calculate_cost_difference needs to efficiently calculate the resulting change in cost for a transition. markov::State markov::Model::apply_transition markov::Model::calculate_cost_difference Inheritance utils::Component markov::Transition examples::SoftSpinTransition markov::SimpleTransition model::BlumeCapelTransition model::PolyTransition model::PottsTransition model::TspTransition Inherited Members configure render ~Component Component get_status param get_class_name"
  },
  "api/markov/walker.html": {
    "href": "api/markov/walker.html",
    "title": "Class markov::Walker | qiotoolkit",
    "keywords": "Class markov::Walker Walker is an abstract base class for configuration space explorers A walker container stores its current state and cost and has pointers to a model and random number generator to make steps. It is specifically designed to allow copy assignment (assuming the Model::State_T does), such that it can be duplicated in population markovs. The Walker provides a base implementation of the step function, which proposes a random new state, computes the cost difference, invokes a pure virtual method to decide whether to accept, and modifies the internal state if true is returned. From this we can implement a RandomWalker by always returning true and a Metropolis walker by returning the Boltzmann acceptance rate. markov::Metropolis Inheritance utils::Component markov::Walker markov::HillClimbingWalker markov::Metropolis markov::RandomWalker markov::TabuWalker Inherited Members configure render ~Component Component get_status param get_class_name Constructors Walker() The rng and model must be set before the object can be used. cost is only intialized after a call to init() Declaration markov::Walker<Model>::Walker() Methods init() Must be called after the rng_ and model_ have been set. It initializes the walker to a random state and precomputes its current cost. Note This is typically the only direct call to calculate_cost; from here the current cost_ is updated from the computed difference at each step. Declaration virtual void markov::Walker<Model>::init() init() Must be called after the model_ has been set. Initializes the walker to a specific state and precomputes its current cost. Declaration virtual void markov::Walker<Model>::init(const typename Model::State_T&state) accept() Implementations of the walker interface must provide a method to decide which steps to accept/reject. Currently this is only based on cost_difference, but the state or transition could be added. Declaration virtual bool markov::Walker<Model>::accept(const typename Model::Cost_T&cost_difference)=0 verify_cost_difference() Verify cost difference does a sanity check on the calculated energy difference by evaluating the whole cost function and comparing it to the cached value. This is very costly and happens only in DEBUG builds, not in RELEASE builds. Note An exception thrown by this sanity check means that the implementations of calculate_cost and calculate_cost_difference are not compatible (or, less likely, the model or state have been modified since the total cost was cached). Declaration bool markov::Walker<Model>::verify_cost_difference(const typename Model::Cost_T&returned_difference) apply_transition() For models derived from model::FacedGraphModel, an extra parameter for a cost object is necessary to communicate cached calculations. Declaration std::enable_if<std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, void>::type markov::Walker<Model>::apply_transition(const typename Model::Transition_T&transition, typename Model::Cost_T cost_diff) apply_transition() For all other models, no extra cost parameter is needed. Declaration std::enable_if<!std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, void>::type markov::Walker<Model>::apply_transition(const typename Model::Transition_T&transition, typename Model::Cost_T cost_diff) make_sweep() For models derived from markov::LinearSweepModel, let the model handle the entire sweep. Declaration std::enable_if<std::is_base_of<markov::LinearSweepModel<TS>, TM>::value, void>::type markov::Walker<Model>::make_sweep() make_sweep() For models with a 'size_t' transition type, do an ordered sweep. Declaration std::enable_if<std::is_same<TT, size_t>::value&&!std::is_base_of<markov::LinearSweepModel<TS>, TM>::value, void>::type markov::Walker<Model>::make_sweep() make_sweep() For any other transition type, do a random sweep. Declaration std::enable_if<!std::is_same<TT, size_t>::value&&!std::is_base_of<markov::LinearSweepModel<TS>, TM>::value, void>::type markov::Walker<Model>::make_sweep() make_sweeps() Perform multiple sweeps. Declaration void markov::Walker<Model>::make_sweeps(size_t n_sweeps) make_step() Perform a single (random) step. Declaration void markov::Walker<Model>::make_step() attempt_transition() Attempt the given transition and apply it if accepted by the walker condition. For models derived from model::FacedGraphModel, an extra parameter for a cost object is necessary to communicate cached calculations. Declaration std::enable_if<std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, typename Model::Cost_T>::type markov::Walker<Model>::attempt_transition(const typename Model::Transition_T&transition) attempt_transition() Attempt the given transition and apply it if accepted by the walker condition. For all other models, no extra cost parameter is needed. Declaration std::enable_if<!std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, typename Model::Cost_T>::type markov::Walker<Model>::attempt_transition(const typename Model::Transition_T&transition) cost() Return the cost of the current state (cached within the walker). Declaration const Model::Cost_T&markov::Walker<Model>::cost() const state() Return the current state of this walker. Declaration const Model::State_T&markov::Walker<Model>::state() const set_model() Provide a pointer to the (non-owned) model to use for cost calculations and proposing new steps. Declaration void markov::Walker<Model>::set_model(const Model *model) set_rng() Provide a pointer to the (non-owned) random number generator to use. Multithreading must either be handled via mutex within the random number generator or by the calling class. Declaration void markov::Walker<Model>::set_rng(utils::RandomGenerator *rng) model() Return a constant reference to the model being used. Declaration const Model&markov::Walker<Model>::model() rng() Return a reference to the random number generator to use. Declaration utils::RandomGenerator&markov::Walker<Model>::rng() swap_state() Swap the state with another instance. This exchanges the internal markov::State and the associated (cached) cost_ with another metropolis instance. Declaration void markov::Walker<Model>::swap_state(Walker *other) get_evaluation_counter() Declaration solver::EvaluationCounter markov::Walker<Model>::get_evaluation_counter() reset_evaluation_counter() Declaration void markov::Walker<Model>::reset_evaluation_counter() save_lowest() Unconditionally store the current state as the lowest (during init) Declaration void markov::Walker<Model>::save_lowest() check_lowest() Save the current state as new best if its cost is lower than the previous best. Declaration void markov::Walker<Model>::check_lowest() get_lowest_cost() Declaration Model::Cost_T markov::Walker<Model>::get_lowest_cost() const apply_scale_factor() Declaration void markov::Walker<Model>::apply_scale_factor(double scale_factor) get_lowest_state() Declaration const Model::State_T&markov::Walker<Model>::get_lowest_state() const memory_estimate() Estimate memory consumption using model parameters. Declaration static size_t markov::Walker<Model>::memory_estimate(const Model&model) compare() Declaration static bool markov::Walker<Model>::compare(const Walker<Model>&w1, const Walker<Model>&w2)"
  },
  "api/matcher/all-of-matcher.html": {
    "href": "api/matcher/all-of-matcher.html",
    "title": "Class matcher::AllOfMatcher | qiotoolkit",
    "keywords": "Class matcher::AllOfMatcher Inheritance matcher::AllOfMatcher Constructors AllOfMatcher() Declaration matcher::AllOfMatcher<A, B>::AllOfMatcher(const A&a, const B&b) Methods matches() Declaration bool matcher::AllOfMatcher<A, B>::matches(const T&value) const explain() Declaration std::string matcher::AllOfMatcher<A, B>::explain(const T&value) const"
  },
  "api/matcher/all-of.html": {
    "href": "api/matcher/all-of.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/any-matcher.html": {
    "href": "api/matcher/any-matcher.html",
    "title": "Class matcher::AnyMatcher | qiotoolkit",
    "keywords": "Class matcher::AnyMatcher Inheritance matcher::AnyMatcher Methods matches() Declaration bool matcher::AnyMatcher::matches(const T&) const explain() Declaration std::string matcher::AnyMatcher::explain(const T&value) const"
  },
  "api/matcher/any-of-matcher.html": {
    "href": "api/matcher/any-of-matcher.html",
    "title": "Class matcher::AnyOfMatcher | qiotoolkit",
    "keywords": "Class matcher::AnyOfMatcher Inheritance matcher::AnyOfMatcher Constructors AnyOfMatcher() Declaration matcher::AnyOfMatcher<A, B>::AnyOfMatcher(const A&a, const B&b) Methods matches() Declaration bool matcher::AnyOfMatcher<A, B>::matches(const T&value) const explain() Declaration std::string matcher::AnyOfMatcher<A, B>::explain(const T&value) const"
  },
  "api/matcher/any-of.html": {
    "href": "api/matcher/any-of.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/any.html": {
    "href": "api/matcher/any.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/ascending.html": {
    "href": "api/matcher/ascending.html",
    "title": "Class matcher::Ascending | qiotoolkit",
    "keywords": "Class matcher::Ascending Inheritance matcher::Ascending Methods in_order() Declaration bool matcher::Ascending::in_order(const A&a, const B&b) explain() Declaration std::string matcher::Ascending::explain() reverse() Declaration std::string matcher::Ascending::reverse()"
  },
  "api/matcher/descending.html": {
    "href": "api/matcher/descending.html",
    "title": "Class matcher::Descending | qiotoolkit",
    "keywords": "Class matcher::Descending Inheritance matcher::Descending Methods in_order() Declaration bool matcher::Descending::in_order(const A&a, const B&b) explain() Declaration std::string matcher::Descending::explain() reverse() Declaration std::string matcher::Descending::reverse()"
  },
  "api/matcher/each-matcher.html": {
    "href": "api/matcher/each-matcher.html",
    "title": "Class matcher::EachMatcher | qiotoolkit",
    "keywords": "Class matcher::EachMatcher Inheritance matcher::EachMatcher Constructors EachMatcher() Declaration matcher::EachMatcher<ElementMatcher>::EachMatcher(ElementMatcher element_matcher) Methods matches() Declaration bool matcher::EachMatcher<ElementMatcher>::matches(const std::vector<T>&container) const explain() Declaration std::string matcher::EachMatcher<ElementMatcher>::explain(const std::vector<T>&container) const"
  },
  "api/matcher/each.html": {
    "href": "api/matcher/each.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/equal-to-matcher.html": {
    "href": "api/matcher/equal-to-matcher.html",
    "title": "Class matcher::EqualToMatcher | qiotoolkit",
    "keywords": "Class matcher::EqualToMatcher Inheritance matcher::EqualToMatcher Constructors EqualToMatcher() Declaration matcher::EqualToMatcher<T>::EqualToMatcher(const T&expected) Methods matches() Declaration bool matcher::EqualToMatcher<T>::matches(const V&value) const explain() Declaration std::string matcher::EqualToMatcher<T>::explain(const V&value) const"
  },
  "api/matcher/equal-to.html": {
    "href": "api/matcher/equal-to.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/greater-equal-matcher.html": {
    "href": "api/matcher/greater-equal-matcher.html",
    "title": "Class matcher::GreaterEqualMatcher | qiotoolkit",
    "keywords": "Class matcher::GreaterEqualMatcher Inheritance matcher::GreaterEqualMatcher Constructors GreaterEqualMatcher() Declaration matcher::GreaterEqualMatcher<T>::GreaterEqualMatcher(const T&expected) Methods matches() Declaration bool matcher::GreaterEqualMatcher<T>::matches(const V&value) const explain() Declaration std::string matcher::GreaterEqualMatcher<T>::explain(const V&value) const"
  },
  "api/matcher/greater-equal.html": {
    "href": "api/matcher/greater-equal.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/greater-than-matcher.html": {
    "href": "api/matcher/greater-than-matcher.html",
    "title": "Class matcher::GreaterThanMatcher | qiotoolkit",
    "keywords": "Class matcher::GreaterThanMatcher Inheritance matcher::GreaterThanMatcher Constructors GreaterThanMatcher() Declaration matcher::GreaterThanMatcher<T>::GreaterThanMatcher(const T&expected) Methods matches() Declaration bool matcher::GreaterThanMatcher<T>::matches(const V&value) const explain() Declaration std::string matcher::GreaterThanMatcher<T>::explain(const V&value) const"
  },
  "api/matcher/greater-than.html": {
    "href": "api/matcher/greater-than.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/index.html": {
    "href": "api/matcher/index.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/is-empty-matcher.html": {
    "href": "api/matcher/is-empty-matcher.html",
    "title": "Class matcher::IsEmptyMatcher | qiotoolkit",
    "keywords": "Class matcher::IsEmptyMatcher Inheritance matcher::IsEmptyMatcher Methods matches() Declaration bool matcher::IsEmptyMatcher::matches(const T&value) const explain() Declaration std::string matcher::IsEmptyMatcher::explain(const T&value) const"
  },
  "api/matcher/is-empty.html": {
    "href": "api/matcher/is-empty.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/is-sorted-matcher.html": {
    "href": "api/matcher/is-sorted-matcher.html",
    "title": "Class matcher::IsSortedMatcher | qiotoolkit",
    "keywords": "Class matcher::IsSortedMatcher Inheritance matcher::IsSortedMatcher Constructors IsSortedMatcher() Declaration matcher::IsSortedMatcher<Order>::IsSortedMatcher(const Order&order) Methods matches() Declaration bool matcher::IsSortedMatcher<Order>::matches(const std::vector<T>&v) const explain() Declaration std::string matcher::IsSortedMatcher<Order>::explain(const std::vector<T>&v) const"
  },
  "api/matcher/is-sorted.html": {
    "href": "api/matcher/is-sorted.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/less-equal-matcher.html": {
    "href": "api/matcher/less-equal-matcher.html",
    "title": "Class matcher::LessEqualMatcher | qiotoolkit",
    "keywords": "Class matcher::LessEqualMatcher Inheritance matcher::LessEqualMatcher Constructors LessEqualMatcher() Declaration matcher::LessEqualMatcher<T>::LessEqualMatcher(const T&expected) Methods matches() Declaration bool matcher::LessEqualMatcher<T>::matches(const V&value) const explain() Declaration std::string matcher::LessEqualMatcher<T>::explain(const V&value) const"
  },
  "api/matcher/less-equal.html": {
    "href": "api/matcher/less-equal.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/less-than-matcher.html": {
    "href": "api/matcher/less-than-matcher.html",
    "title": "Class matcher::LessThanMatcher | qiotoolkit",
    "keywords": "Class matcher::LessThanMatcher Inheritance matcher::LessThanMatcher Constructors LessThanMatcher() Declaration matcher::LessThanMatcher<T>::LessThanMatcher(const T&expected) Methods matches() Declaration bool matcher::LessThanMatcher<T>::matches(const V&value) const explain() Declaration std::string matcher::LessThanMatcher<T>::explain(const V&value) const"
  },
  "api/matcher/less-than.html": {
    "href": "api/matcher/less-than.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/none-of-matcher.html": {
    "href": "api/matcher/none-of-matcher.html",
    "title": "Class matcher::NoneOfMatcher | qiotoolkit",
    "keywords": "Class matcher::NoneOfMatcher Inheritance matcher::NoneOfMatcher Constructors NoneOfMatcher() Declaration matcher::NoneOfMatcher<A, B>::NoneOfMatcher(const A&a, const B&b) Methods matches() Declaration bool matcher::NoneOfMatcher<A, B>::matches(const T&value) const explain() Declaration std::string matcher::NoneOfMatcher<A, B>::explain(const T&value) const"
  },
  "api/matcher/none-of.html": {
    "href": "api/matcher/none-of.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/not-matcher.html": {
    "href": "api/matcher/not-matcher.html",
    "title": "Class matcher::NotMatcher | qiotoolkit",
    "keywords": "Class matcher::NotMatcher Inheritance matcher::NotMatcher Constructors NotMatcher() Declaration matcher::NotMatcher<M>::NotMatcher(const M&m) Methods matches() Declaration bool matcher::NotMatcher<M>::matches(const T&value) const explain() Declaration std::string matcher::NotMatcher<M>::explain(const T&value) const"
  },
  "api/matcher/not.html": {
    "href": "api/matcher/not.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/size-is-matcher.html": {
    "href": "api/matcher/size-is-matcher.html",
    "title": "Class matcher::SizeIsMatcher | qiotoolkit",
    "keywords": "Class matcher::SizeIsMatcher Inheritance matcher::SizeIsMatcher Constructors SizeIsMatcher() Declaration matcher::SizeIsMatcher<SizeMatcher>::SizeIsMatcher(const SizeMatcher&size_matcher) Methods matches() Declaration bool matcher::SizeIsMatcher<SizeMatcher>::matches(const T&container) const explain() Declaration std::string matcher::SizeIsMatcher<SizeMatcher>::explain(const T&container) const"
  },
  "api/matcher/size-is.html": {
    "href": "api/matcher/size-is.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/matcher/strictly-ascending.html": {
    "href": "api/matcher/strictly-ascending.html",
    "title": "Class matcher::StrictlyAscending | qiotoolkit",
    "keywords": "Class matcher::StrictlyAscending Inheritance matcher::StrictlyAscending Methods in_order() Declaration bool matcher::StrictlyAscending::in_order(const A&a, const B&b) explain() Declaration std::string matcher::StrictlyAscending::explain() reverse() Declaration std::string matcher::StrictlyAscending::reverse()"
  },
  "api/matcher/strictly-descending.html": {
    "href": "api/matcher/strictly-descending.html",
    "title": "Class matcher::StrictlyDescending | qiotoolkit",
    "keywords": "Class matcher::StrictlyDescending Inheritance matcher::StrictlyDescending Methods in_order() Declaration bool matcher::StrictlyDescending::in_order(const A&a, const B&b) explain() Declaration std::string matcher::StrictlyDescending::explain() reverse() Declaration std::string matcher::StrictlyDescending::reverse()"
  },
  "api/model/abstract-ising.html": {
    "href": "api/model/abstract-ising.html",
    "title": "Class model::AbstractIsing | qiotoolkit",
    "keywords": "Class model::AbstractIsing Ising Model. Inheritance model::GraphModel model::AbstractIsing Inherited Members Model state_only_memory_estimate init get_initial_configuration_state estimate_min_cost_diff set_step_limit state_memory_estimate render configure ~BaseModel configure BaseModel edge get_const_cost node rescale node_count estimate_max_cost_diff get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled edges get_initial_configuration nodes get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::AbstractIsing<State_T, Cost_T>::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::AbstractIsing<State_T, Cost_T>::get_version() const override match_version() Checks the string argument against the expected version. This can be overloaded to accept other version than the one returned by get_version(). Declaration void model::AbstractIsing<State_T, Cost_T>::match_version(const std::string&version) override calculate_cost() Cost function: \\mathcal{H} = -\\sum_{j\\in\\mathrm{\\{edges\\}}} c_j\\prod_{i\\in j}s_i Declaration double model::AbstractIsing<State_T, Cost_T>::calculate_cost(const State_T&state) const override calculate_cost_difference() Cost difference: \\Delta_{i\\to i'} = 2 \\sum_{j\\in e_i} c_j\\prod_{i\\in j}s_i Declaration double model::AbstractIsing<State_T, Cost_T>::calculate_cost_difference(const State_T&state, const Transition_T&spin_id) const override get_random_state() Return a random Ising state. The number of Ising variables (terms) is determined from the number of nodes (edges) in the underlying graph. Declaration State_T model::AbstractIsing<State_T, Cost_T>::get_random_state(utils::RandomGenerator&rng) const override get_initial_configuration_state() Declaration State_T model::AbstractIsing<State_T, Cost_T>::get_initial_configuration_state() const override get_random_transition() Return a random Single spin update. We represent a single spin update as merely the index of the spin variable that will flip. Declaration Transition_T model::AbstractIsing<State_T, Cost_T>::get_random_transition(const State_T&, utils::RandomGenerator&rng) const override apply_transition() Flip the spin specified by spin_id. Declaration void model::AbstractIsing<State_T, Cost_T>::apply_transition(const Transition_T&spin_id, State_T&state) const override=0 configure() Serialize metadata and the underlying graph. The underlying graph represents the \"disorder\" of the Ising model (glass) we are simulating. Declaration void model::AbstractIsing<State_T, Cost_T>::configure(const utils::Json&json) override configure() Declaration void model::AbstractIsing<State_T, Cost_T>::configure(typename Graph::Configuration_T&configuration) render_state() Render a state of this model. Default implementation of how a state of this model is rendered (by' invoking the state's proper rendering mechanic). Overloading this method allows a model implementation to customize how a state is printed. This is relevant if the rendering needs to depend on model parameters in addition to the state. Declaration utils::Structure model::AbstractIsing<State_T, Cost_T>::render_state(const State_T&state) const override state_memory_estimate() Declaration size_t model::AbstractIsing<State_T, Cost_T>::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::AbstractIsing<State_T, Cost_T>::state_only_memory_estimate() const override estimate_max_cost_diff() Declaration double model::AbstractIsing<State_T, Cost_T>::estimate_max_cost_diff() const override estimate_min_cost_diff() Declaration double model::AbstractIsing<State_T, Cost_T>::estimate_min_cost_diff() const override get_term() Calculate the term with id term_id. Declaration virtual double model::AbstractIsing<State_T, Cost_T>::get_term(const State_T&state, size_t term_id) const =0"
  },
  "api/model/abstract-pubo.html": {
    "href": "api/model/abstract-pubo.html",
    "title": "Class model::AbstractPubo | qiotoolkit",
    "keywords": "Class model::AbstractPubo Inheritance model::GraphModel model::AbstractPubo Inherited Members Model state_only_memory_estimate init get_initial_configuration_state calculate_cost estimate_min_cost_diff set_step_limit state_memory_estimate render calculate_cost_difference configure ~BaseModel configure BaseModel edge get_const_cost node rescale node_count estimate_max_cost_diff get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled get_initial_configuration get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Constructors AbstractPubo() Declaration model::AbstractPubo<State_T, Cost_T>::AbstractPubo() Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::AbstractPubo<State_T, Cost_T>::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::AbstractPubo<State_T, Cost_T>::get_version() const override match_version() Checks the string argument against the expected version. This can be overloaded to accept other version than the one returned by get_version(). Declaration void model::AbstractPubo<State_T, Cost_T>::match_version(const std::string&version) override ~AbstractPubo() Declaration virtual model::AbstractPubo<State_T, Cost_T>::~AbstractPubo() configure() Populates model internals from the input json. Declaration void model::AbstractPubo<State_T, Cost_T>::configure(const utils::Json&json) override configure() Declaration void model::AbstractPubo<State_T, Cost_T>::configure(typename Graph::Configuration_T&configuration) get_random_state() Return a valid random state for the model. The various algorithms don't know how to initialize a valid state; this allows them to start with a random one. NOTE: The rng being passed should be used for randomness (as opposed to creating one for the model), as multiple threads can potentially use the same underlying model (albeit with separate rngs). Declaration State_T model::AbstractPubo<State_T, Cost_T>::get_random_state(utils::RandomGenerator&rng) const override get_initial_configuration_state() Declaration State_T model::AbstractPubo<State_T, Cost_T>::get_initial_configuration_state() const override get_random_transition() Return a random transition starting at state. The various algorithms don't know how to move throuhg markov space; this allows them to pick a random direction given a starting point. By definition, the next markov state should only depend on the current one, so only the last one is passed to this function. \"random\" does not necessitate equi-distributed here (you may choose a different distribution if your model dictates it). However, the the typical choise is to pick randomly from all possible moves at state with equal probability. Declaration Transition_T model::AbstractPubo<State_T, Cost_T>::get_random_transition(const State_T&, utils::RandomGenerator&rng) const override apply_transition() Apply a transition to a state. This changes the configuration represented by *state. Depending on the optimization algorithm, transition can either be applied conditionally or alaways (e.g., population dynamics). Separating the functionality into the three interfaces random_transition, calculate_cost_difference, apply_transition leaves control over the strategy with the optimization method. Declaration virtual void model::AbstractPubo<State_T, Cost_T>::apply_transition(const Transition_T&transition, State_T&state) const override=0 render_state() Render a state of this model. Default implementation of how a state of this model is rendered (by' invoking the state's proper rendering mechanic). Overloading this method allows a model implementation to customize how a state is printed. This is relevant if the rendering needs to depend on model parameters in addition to the state. Declaration utils::Structure model::AbstractPubo<State_T, Cost_T>::render_state(const State_T&state) const override state_memory_estimate() Declaration size_t model::AbstractPubo<State_T, Cost_T>::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::AbstractPubo<State_T, Cost_T>::state_only_memory_estimate() const override estimate_min_cost_diff() Declaration double model::AbstractPubo<State_T, Cost_T>::estimate_min_cost_diff() const override edges() Return a vector of all the edges. Declaration const std::vector<Edge>&model::GraphModel<State_T, Transition_T, Cost_T>::edges() const nodes() Return a vector of all the nodes. Declaration const std::vector<Node>&model::GraphModel<State_T, Transition_T, Cost_T>::nodes() const"
  },
  "api/model/base-model-configuration.html": {
    "href": "api/model/base-model-configuration.html",
    "title": "Class model::BaseModelConfiguration | qiotoolkit",
    "keywords": "Class model::BaseModelConfiguration Inheritance model::BaseModelConfiguration model::BaseModelPreviewConfiguration model::FacedGraphModelConfiguration model::GraphModelConfiguration model::MaxSatConfiguration model::PolyConfiguration model::Terms::TermsConfiguration model::TspConfiguration"
  },
  "api/model/base-model-configuration/get/type.html": {
    "href": "api/model/base-model-configuration/get/type.html",
    "title": "Struct model::BaseModelConfiguration::Get_Type | qiotoolkit",
    "keywords": "Struct model::BaseModelConfiguration::Get_Type Methods get() Declaration static std::string&model::BaseModelConfiguration::Get_Type::get(BaseModelConfiguration&m) get_key() Declaration static std::string model::BaseModelConfiguration::Get_Type::get_key()"
  },
  "api/model/base-model-configuration/get/version.html": {
    "href": "api/model/base-model-configuration/get/version.html",
    "title": "Struct model::BaseModelConfiguration::Get_Version | qiotoolkit",
    "keywords": "Struct model::BaseModelConfiguration::Get_Version Methods get() Declaration static std::string&model::BaseModelConfiguration::Get_Version::get(BaseModelConfiguration&m) get_key() Declaration static std::string model::BaseModelConfiguration::Get_Version::get_key()"
  },
  "api/model/base-model-preview-configuration.html": {
    "href": "api/model/base-model-preview-configuration.html",
    "title": "Class model::BaseModelPreviewConfiguration | qiotoolkit",
    "keywords": "Class model::BaseModelPreviewConfiguration Inheritance model::BaseModelConfiguration model::BaseModelPreviewConfiguration"
  },
  "api/model/base-model.html": {
    "href": "api/model/base-model.html",
    "title": "Class model::BaseModel | qiotoolkit",
    "keywords": "Class model::BaseModel BaseModel class (non-markov specific) Base class implementing methods needed for model selection. A pointer to this base class can be used by the owner of the model (through it needs to be dynamic-cast to a pointer to the specific model type when setting it on the solver). [!NOTE] This is different from the markov::Model class which has the interfaces to interact with markov-based optimizers. Inheritance utils::Component model::BaseModel markov::Model Inherited Members render ~Component Component get_status param get_class_name Constructors BaseModel() Declaration model::BaseModel::BaseModel()=default Methods ~BaseModel() Declaration virtual model::BaseModel::~BaseModel()=default get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration virtual std::string model::BaseModel::get_identifier() const =0 get_version() Returns the version of the input format this implementation expects. Declaration virtual std::string model::BaseModel::get_version() const =0 configure() Populates model internals from the input json. Declaration void model::BaseModel::configure(const utils::Json&json) override configure() Populates model internals from the input configuration. Declaration void model::BaseModel::configure(BaseModelConfiguration&configuration) configure() Populates the model from a related model The trelated model is considered useless after this. Declaration void model::BaseModel::configure(BaseModel *base) init() Initializes internal data structures (guaranteed to be called after configure()). Declaration void model::BaseModel::init() match_version() Checks the string argument against the expected version. This can be overloaded to accept other version than the one returned by get_version(). Declaration void model::BaseModel::match_version(const std::string&version)"
  },
  "api/model/binary-with-counter.html": {
    "href": "api/model/binary-with-counter.html",
    "title": "Class model::BinaryWithCounter | qiotoolkit",
    "keywords": "Class model::BinaryWithCounter Inheritance markov::State model::BinaryWithCounter Inherited Members configure ~Component Component get_status param get_class_name Constructors BinaryWithCounter() Declaration model::BinaryWithCounter<T>::BinaryWithCounter() BinaryWithCounter() Declaration model::BinaryWithCounter<T>::BinaryWithCounter(size_t N, size_t M) Methods copy_state_only() Declaration void model::BinaryWithCounter<T>::copy_state_only(const BinaryWithCounter&other) render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure model::BinaryWithCounter<T>::render() const override render() Declaration utils::Structure model::BinaryWithCounter<T>::render(const std::map<int, int>&ids_map) const memory_estimate() Declaration static size_t model::BinaryWithCounter<T>::memory_estimate(size_t N, size_t M) state_only_memory_estimate() Declaration static size_t model::BinaryWithCounter<T>::state_only_memory_estimate(size_t N)"
  },
  "api/model/binary.html": {
    "href": "api/model/binary.html",
    "title": "Class model::Binary | qiotoolkit",
    "keywords": "Class model::Binary Inheritance markov::State model::Binary model::PuboCompactState Inherited Members configure ~Component Component get_status param get_class_name Constructors Binary() Declaration model::Binary::Binary() Binary() Declaration model::Binary::Binary(size_t N, size_t) Methods copy_state_only() Declaration void model::Binary::copy_state_only(const Binary&other) render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure model::Binary::render() const override render() Declaration utils::Structure model::Binary::render(const std::map<int, int>&) const memory_estimate() Declaration static size_t model::Binary::memory_estimate(size_t N, size_t) state_only_memory_estimate() Declaration static size_t model::Binary::state_only_memory_estimate(size_t N)"
  },
  "api/model/blume-capel-state.html": {
    "href": "api/model/blume-capel-state.html",
    "title": "Class model::BlumeCapelState | qiotoolkit",
    "keywords": "Class model::BlumeCapelState Inheritance markov::State model::BlumeCapelState Inherited Members configure ~Component Component get_status param get_class_name Constructors BlumeCapelState() Declaration model::BlumeCapelState::BlumeCapelState() BlumeCapelState() Declaration model::BlumeCapelState::BlumeCapelState(size_t N, size_t M) Methods term() Declaration int model::BlumeCapelState::term(size_t edge_id) const render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure model::BlumeCapelState::render() const override copy_state_only() Declaration void model::BlumeCapelState::copy_state_only(const BlumeCapelState&other) get_spins() Declaration const std::vector<int>&model::BlumeCapelState::get_spins() const memory_estimate() Declaration static size_t model::BlumeCapelState::memory_estimate(size_t N, size_t M) state_only_memory_estimate() Declaration static size_t model::BlumeCapelState::state_only_memory_estimate(size_t N)"
  },
  "api/model/blume-capel-transition.html": {
    "href": "api/model/blume-capel-transition.html",
    "title": "Class model::BlumeCapelTransition | qiotoolkit",
    "keywords": "Class model::BlumeCapelTransition Inheritance markov::Transition model::BlumeCapelTransition Inherited Members configure render ~Component Component get_status param get_class_name Constructors BlumeCapelTransition() Declaration model::BlumeCapelTransition::BlumeCapelTransition() BlumeCapelTransition() Declaration model::BlumeCapelTransition::BlumeCapelTransition(size_t spin_id, int value) Methods spin_id() Declaration size_t model::BlumeCapelTransition::spin_id() const value() Declaration int model::BlumeCapelTransition::value() const operator==() Declaration bool model::BlumeCapelTransition::operator==(const BlumeCapelTransition&trans) const"
  },
  "api/model/blume-capel.html": {
    "href": "api/model/blume-capel.html",
    "title": "Class model::BlumeCapel | qiotoolkit",
    "keywords": "Class model::BlumeCapel Inheritance model::GraphModel model::BlumeCapel Inherited Members Model state_only_memory_estimate init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff set_step_limit state_memory_estimate render calculate_cost_difference get_random_transition configure match_version ~BaseModel configure BaseModel edge get_const_cost node rescale node_count estimate_max_cost_diff get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled edges get_initial_configuration nodes get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::BlumeCapel::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::BlumeCapel::get_version() const override cost_term() Declaration double model::BlumeCapel::cost_term(const State_T&state, size_t j) const cost_term() Declaration double model::BlumeCapel::cost_term(const State_T&state, size_t j, size_t modified_spin, int modified_value) const calculate_cost() Declaration double model::BlumeCapel::calculate_cost(const State_T&state) const override calculate_cost_difference() Declaration double model::BlumeCapel::calculate_cost_difference(const State_T&state, const Transition_T&transition) const override get_random_state() Return a valid random state for the model. The various algorithms don't know how to initialize a valid state; this allows them to start with a random one. NOTE: The rng being passed should be used for randomness (as opposed to creating one for the model), as multiple threads can potentially use the same underlying model (albeit with separate rngs). Declaration BlumeCapel::State_T model::BlumeCapel::get_random_state(utils::RandomGenerator&rng) const override get_random_transition() Declaration BlumeCapel::Transition_T model::BlumeCapel::get_random_transition(const State_T&state, utils::RandomGenerator&rng) const override apply_transition() Declaration void model::BlumeCapel::apply_transition(const Transition_T&transition, State_T&state) const override configure() Populates model internals from the input json. Declaration void model::BlumeCapel::configure(const utils::Json&json) override configure() Declaration void model::BlumeCapel::configure(Graph::Configuration_T&configuration) render_state() Declaration utils::Structure model::BlumeCapel::render_state(const State_T&state) const override state_memory_estimate() Declaration size_t model::BlumeCapel::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::BlumeCapel::state_only_memory_estimate() const override"
  },
  "api/model/clock-configuration.html": {
    "href": "api/model/clock-configuration.html",
    "title": "Class model::ClockConfiguration | qiotoolkit",
    "keywords": "Class model::ClockConfiguration Inheritance model::GraphModelConfiguration model::ClockConfiguration Inherited Members map_initial_configuration"
  },
  "api/model/clock-configuration/get/number/of/states.html": {
    "href": "api/model/clock-configuration/get/number/of/states.html",
    "title": "Struct model::ClockConfiguration::Get_Number_of_States | qiotoolkit",
    "keywords": "Struct model::ClockConfiguration::Get_Number_of_States Methods get() Declaration static size_t&model::ClockConfiguration::Get_Number_of_States::get(ClockConfiguration&config) get_key() Declaration static std::string model::ClockConfiguration::Get_Number_of_States::get_key()"
  },
  "api/model/clock-state.html": {
    "href": "api/model/clock-state.html",
    "title": "Class model::ClockState | qiotoolkit",
    "keywords": "Class model::ClockState Clock state representation. Represents: N discretized clock spins as a vector M state terms as {x: double, y: double} The state terms hold the sum of cosines (sines) of the discretized clock model directions, respectively. I.e., term[j].x = \\sum_{i \\in j} cos(s_i * 2\\pi/q) term[j].y = \\sum_{i \\in j} sin(s_i * 2\\pi/q) Inheritance markov::State model::ClockState Inherited Members ~Component Component get_status param get_class_name Constructors ClockState() Create an uninitialized clock state. Declaration model::ClockState::ClockState() ClockState() Create a clock state with N spins and M terms. Declaration model::ClockState::ClockState(size_t N, size_t M) Methods configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void model::ClockState::configure(const utils::Json&json) override copy_state_only() Declaration void model::ClockState::copy_state_only(const ClockState&other) render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure model::ClockState::render() const override memory_estimate() Declaration static size_t model::ClockState::memory_estimate(size_t N, size_t M) state_only_memory_estimate() Declaration static size_t model::ClockState::state_only_memory_estimate(size_t N)"
  },
  "api/model/clock-state/term.html": {
    "href": "api/model/clock-state/term.html",
    "title": "Struct model::ClockState::Term | qiotoolkit",
    "keywords": "Struct model::ClockState::Term"
  },
  "api/model/clock-transition.html": {
    "href": "api/model/clock-transition.html",
    "title": "Struct model::ClockTransition | qiotoolkit",
    "keywords": "Struct model::ClockTransition Clock transition representation. Which spin to change and to what value. Methods operator==() Declaration bool model::ClockTransition::operator==(const ClockTransition&trans) const"
  },
  "api/model/clock.html": {
    "href": "api/model/clock.html",
    "title": "Class model::Clock | qiotoolkit",
    "keywords": "Class model::Clock Inheritance model::GraphModel model::Clock Inherited Members Model state_only_memory_estimate init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff set_step_limit state_memory_estimate render calculate_cost_difference get_random_transition configure match_version ~BaseModel configure BaseModel edge get_const_cost node rescale node_count estimate_max_cost_diff get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled edges get_initial_configuration configure nodes get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Constructors Clock() Declaration model::Clock::Clock() Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::Clock::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::Clock::get_version() const override calculate_cost() Calculate the Clock Hamiltonian. The standard clock Hamiltonian is defined for edges involving two nodes: \\(\\mathcal{H}' = \\sum_{ij} \\mathrm{cost}_{ij}\\vec{s_i}\\vec{s_j}\\), Where \\(\\vec{s_i}\\) are equi-distributed 2d vectors on the unit circle. (Equivalently: \\(\\vec{s_i}\\vec{s_j} = \\cos(\\theta_i - \\theta_j)\\) with \\(\\theta_i = 2\\pi i/q\\).) Here we use a generalization to n-node hyperedges using the sum of planar vectors partcipating in the edge: \\(v_e = \\frac{1}{|e|} \\sum_{i\\in e} \\vec{s_i}\\), Where \\(\\vec{s_i}\\) is defined as above and \\(|e|\\) is the number of nodes in the hyper-edge. With this our Hamiltonian can be expressed as: $ \\mathcal{H} = \\sum_e \\mathrm{cost}_e (2*v_e^2-1), where the rescaling ensures that the generalization corresponds to the standard defintion for edges with 2 nodes (i.e, +1 when aligned, -1 when anti-aligned, 0 when perpendicular). This means that the contribution is +1 * cost_e when the clock spins are aligned (in any direction) and -1 * cost_e when the clock spins add up to zero. Declaration double model::Clock::calculate_cost(const State_T&state) const override calculate_cost_difference() Compute the difference resulting from transition This recomputes v_e for each of the terms in which transition.spin_id is involved. Declaration double model::Clock::calculate_cost_difference(const State_T&state, const Transition_T&transition) const override get_random_state() Build a random Clock state. This function decides on the number of discretized clock spins and the possible values of each to return (according to the model configuration). Partially precomputed terms (term[j].x, etc.) are populated here. Declaration Clock::State_T model::Clock::get_random_state(utils::RandomGenerator&rng) const override get_random_transition() Create a random transition for state. This picks a random spin_id and proposes a new_value that is different from the current one. Declaration Clock::Transition_T model::Clock::get_random_transition(const State_T&state, utils::RandomGenerator&rng) const override apply_transition() Change state according to transition. This sets the modified spin to its new value and updates the partially precomputed terms that are affected by this change. Declaration void model::Clock::apply_transition(const Transition_T&transition, State_T&state) const override configure() Serialize the number of states q and the underlying graph. Declaration void model::Clock::configure(const utils::Json&json) override configure() Declaration void model::Clock::configure(Configuration_T&conf) initialize_terms() Initialize the state.terms according to the model and current state.spins. Declaration void model::Clock::initialize_terms(State_T&state) const state_memory_estimate() Estimate memory consumption for state. Declaration size_t model::Clock::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::Clock::state_only_memory_estimate() const override calculate_term() This computes the Hamiltonian contribution of the edge edge_id. If specified, dx and dy are applied to the term (this is used to emulate a modified configuration within calculate_cost_difference). Declaration double model::Clock::calculate_term(const State_T&state, size_t edge_id, double dx=0, double dy=0) const set_number_of_states() Set the number of states to q and precompute the cos/sin values for the discretized angles corresponding to 1..(q-1) * 2\\pi/q. Declaration void model::Clock::set_number_of_states(size_t q)"
  },
  "api/model/cost-cache.html": {
    "href": "api/model/cost-cache.html",
    "title": "Class model::CostCache | qiotoolkit",
    "keywords": "Class model::CostCache Inheritance model::CostCache Constructors CostCache() Declaration model::CostCache::CostCache() CostCache() Declaration model::CostCache::CostCache(double v) CostCache() Declaration model::CostCache::CostCache(double v, std::vector<double>c) Methods operator double() Declaration model::CostCache::operator double() const render() Declaration utils::Structure model::CostCache::render() const"
  },
  "api/model/faced-graph-model-configuration.html": {
    "href": "api/model/faced-graph-model-configuration.html",
    "title": "Class model::FacedGraphModelConfiguration | qiotoolkit",
    "keywords": "Class model::FacedGraphModelConfiguration Fced Graph Model Configuration. This is a base class for an intermidiate loading of data for various Faced Graph Models. Graph Models will be configured form this class by moving its data. Inheritance model::BaseModelConfiguration graph::FacedGraphConfiguration model::FacedGraphModelConfiguration"
  },
  "api/model/faced-graph-model.html": {
    "href": "api/model/faced-graph-model.html",
    "title": "Class model::FacedGraphModel | qiotoolkit",
    "keywords": "Class model::FacedGraphModel Faced Graph Model. This is a base class for models with a cost function that can be expressed as a graph with faces. Inheritance markov::Model model::FacedGraphModel Inherited Members get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model state_only_memory_estimate get_term_count get_sweep_size init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate render get_random_transition get_const_cost configure get_random_state match_version get_version ~BaseModel configure BaseModel get_identifier render ~Component Component get_status param get_class_name Methods nodes() Return a vector of all the nodes. Declaration const std::vector<Node>&model::FacedGraphModel<State_T, Transition_T, Cost_T>::nodes() const node_count() Return the number of nodes. Declaration size_t model::FacedGraphModel<State_T, Transition_T, Cost_T>::node_count() const node() Return a specific node by node ID. Declaration const Node&model::FacedGraphModel<State_T, Transition_T, Cost_T>::node(size_t id) const edges() Return a vector of all the edges. Declaration const std::vector<Edge>&model::FacedGraphModel<State_T, Transition_T, Cost_T>::edges() const edge() Return a specific edge by edge ID. Declaration const Edge&model::FacedGraphModel<State_T, Transition_T, Cost_T>::edge(size_t id) const edge_count() Return the number of edges. Declaration size_t model::FacedGraphModel<State_T, Transition_T, Cost_T>::edge_count() const faces() Return a vector of all the faces. Declaration const std::vector<Face>&model::FacedGraphModel<State_T, Transition_T, Cost_T>::faces() const face() Return a specific edge by face ID. Declaration const Face&model::FacedGraphModel<State_T, Transition_T, Cost_T>::face(size_t id) const configure() Configure the graph from input. Declaration void model::FacedGraphModel<State_T, Transition_T, Cost_T>::configure(const utils::Json&json) override configure() Declaration void model::FacedGraphModel<State_T, Transition_T, Cost_T>::configure(Configuration_T&configuration) calculate_cost_difference() Partial evaluation of the cost function. This method should calculate the difference in cost if we move from state (=before) to the one resulting from applying transition to state (=after): \\Delta_{C} = C_{\\mathrm{after}} - C_{\\mathrm{before}} In code: State state = get_random_state(rng); Transition transition = get_random_transition(rng); double cost_before = calculate_cost(state); double cost_diff = calculate_cost_difference(state, transition); apply_transition(transition, state); // modify state double cost_after = calculate_cost(state); // before + diff should correspond to after (up to double precision) assert(cost_before + cost_diff == cost_after); Declaration Cost_T model::FacedGraphModel<State_T, Transition_T, Cost_T>::calculate_cost_difference(const State_T&, const Transition_T&) const override calculate_cost_difference() Declaration virtual Cost_T model::FacedGraphModel<State_T, Transition_T, Cost_T>::calculate_cost_difference(const State_T&state, const Transition_T&transition, const Cost_T *cost) const =0 apply_transition() Declaration virtual void model::FacedGraphModel<State_T, Transition_T, Cost_T>::apply_transition(const Transition_T&transition, State_T&state, Cost_T *cost) const =0 get_sweep_size() Declaration size_t model::FacedGraphModel<State_T, Transition_T, Cost_T>::get_sweep_size() const override get_term_count() Declaration size_t model::FacedGraphModel<State_T, Transition_T, Cost_T>::get_term_count() const override get_benchmark_properties() Fill the graph benchmarking properties. Declaration utils::Structure model::FacedGraphModel<State_T, Transition_T, Cost_T>::get_benchmark_properties() const override is_rescaled() Declaration bool model::FacedGraphModel<State_T, Transition_T, Cost_T>::is_rescaled() const override rescale() Declaration void model::FacedGraphModel<State_T, Transition_T, Cost_T>::rescale() override get_scale_factor() Declaration Cost_T model::FacedGraphModel<State_T, Transition_T, Cost_T>::get_scale_factor() const override get_const_cost() Declaration Cost_T model::FacedGraphModel<State_T, Transition_T, Cost_T>::get_const_cost() const override is_empty() Declaration bool model::FacedGraphModel<State_T, Transition_T, Cost_T>::is_empty() const override collate() Manage the coefficient collation logic for a single term with degree at most 2. Declaration void model::FacedGraphModel<State_T, Transition_T, Cost_T>::collate(std::map<int, double>&coefficients, const double&term_cost, const std::vector<int>&term_nodes, int node_id, const graph::FaceType&ftype) const estimate_max_cost_diff() Compute an upper bound to the maximum cost difference from a single state flip using the triangle inequality. Implicitly expands SLC terms in cost function to obtain a tighter bound at the cost of additional memory. Declaration virtual double model::FacedGraphModel<State_T, Transition_T, Cost_T>::estimate_max_cost_diff() const override insert_val_to_bbstrees() Declaration virtual void model::FacedGraphModel<State_T, Transition_T, Cost_T>::insert_val_to_bbstrees(double val, std::multiset<double>&tree_A, std::multiset<double>*tree_B=nullptr) const =0 amend_collation() Declaration virtual void model::FacedGraphModel<State_T, Transition_T, Cost_T>::amend_collation(std::map<int, double>&coeffs, int node_id) const =0 populate_bbstrees() Simulate the expanded form of grouped terms to collate quadratic and linear coefficients of terms including a given node ID to populate balanced binary search trees of term weights for maximum cost difference estimation. This function is used by children models to implement estimate_min_cost_diff by overriding the functions insert_val_to_bbstrees and amend_collation. Declaration void model::FacedGraphModel<State_T, Transition_T, Cost_T>::populate_bbstrees(int node_id, std::multiset<double>&tree_A, std::multiset<double>*tree_B=nullptr) const"
  },
  "api/model/get/model.html": {
    "href": "api/model/get/model.html",
    "title": "Struct model::Get_Model | qiotoolkit",
    "keywords": "Struct model::Get_Model Methods get() Declaration static ConfigurationType&model::Get_Model<ConfigurationType>::get(ConfigurationType&m) get_key() Declaration static std::string model::Get_Model<ConfigurationType>::get_key()"
  },
  "api/model/graph-compact-model.html": {
    "href": "api/model/graph-compact-model.html",
    "title": "Class model::GraphCompactModel | qiotoolkit",
    "keywords": "Class model::GraphCompactModel Graph Compact Model. This is with a cost function that can be expressed as a compact graph. GraphCompactModel is samilar to GraphModel except GraphCompactModel use a compact representation and it only allow sequential visiting Inheritance markov::Model model::GraphCompactModel Inherited Members get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model state_only_memory_estimate get_term_count get_sweep_size get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate render calculate_cost_difference get_random_transition get_const_cost configure get_random_state match_version get_version ~BaseModel configure BaseModel get_identifier render ~Component Component get_status param get_class_name Constructors GraphCompactModel() Declaration model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::GraphCompactModel() Methods ~GraphCompactModel() Declaration virtual model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::~GraphCompactModel() configure() Configure the graph from JSON document. Declaration void model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::configure(const utils::Json&json) override configure() Configure the graph from preloaded graph. Declaration void model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::configure(Configuration_T&configuration) init() Initializes internal data structures (guaranteed to be called after configure()). Declaration void model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::init() override get_sweep_size() Declaration size_t model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::get_sweep_size() const override get_term_count() Declaration size_t model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::get_term_count() const override node_count() Return the number of nodes. Declaration size_t model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::node_count() const edge_count() Return the number of edges. Declaration size_t model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::edge_count() const edge() Declaration const Edge_T&model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::edge(size_t edge_id) const get_benchmark_properties() Fill the graph benchmarking properties. Declaration utils::Structure model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::get_benchmark_properties() const override is_rescaled() Declaration bool model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::is_rescaled() const override rescale() Declaration void model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::rescale() override get_scale_factor() Declaration double model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::get_scale_factor() const override get_const_cost() Declaration double model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::get_const_cost() const override is_empty() Declaration bool model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::is_empty() const override estimate_max_cost_diff() Declaration virtual double model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::estimate_max_cost_diff() const override has_initial_configuration() Declaration bool model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::has_initial_configuration() const override get_initial_configuration() Declaration const std::vector<std::pair<int, int>>&model::GraphCompactModel<State_T, Transition_T, Element_T, Cost_T>::get_initial_configuration() const"
  },
  "api/model/graph-model-configuration.html": {
    "href": "api/model/graph-model-configuration.html",
    "title": "Class model::GraphModelConfiguration | qiotoolkit",
    "keywords": "Class model::GraphModelConfiguration Graph Model Configuration. This is a base class for an intermidiate loading of data for various Graph Models. Graph Models will be configured form this class by moving its data. Inheritance model::BaseModelConfiguration graph::GraphConfiguration model::GraphModelConfiguration model::ClockConfiguration Methods map_initial_configuration() Declaration void model::GraphModelConfiguration::map_initial_configuration(const std::map<int, int>&node_name_to_id_map, std::vector<std::pair<int, int>>&id_val_configuration)"
  },
  "api/model/graph-model-configuration/get/initial/configuration.html": {
    "href": "api/model/graph-model-configuration/get/initial/configuration.html",
    "title": "Struct model::GraphModelConfiguration::Get_Initial_Configuration | qiotoolkit",
    "keywords": "Struct model::GraphModelConfiguration::Get_Initial_Configuration Methods get() Declaration static std::vector<std::pair<int, int>>&model::GraphModelConfiguration::Get_Initial_Configuration::get(class GraphModelConfiguration&config) get_key() Declaration static std::string model::GraphModelConfiguration::Get_Initial_Configuration::get_key()"
  },
  "api/model/graph-model.html": {
    "href": "api/model/graph-model.html",
    "title": "Class model::GraphModel | qiotoolkit",
    "keywords": "Class model::GraphModel Graph Model. This is a base class for models with a cost function that can be expressed as a graph. It takes care of the boiler plate to initialize and access the different graph components. Note Currently, it is limited to a graph with blank nodes and edges that have a double-valued cost attributes. If necessary, this can be made configurable in the future either by extending the list of template arguments here or by using multiple inheritance (i.e., removing the markov::Model base class from GraphModel). Inheritance markov::Model model::GraphModel Inherited Members get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model state_only_memory_estimate get_term_count get_sweep_size init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate render calculate_cost_difference get_random_transition get_const_cost configure get_random_state match_version get_version ~BaseModel configure BaseModel get_identifier render ~Component Component get_status param get_class_name Methods nodes() Return a vector of all the nodes. Declaration const std::vector<Node>&model::GraphModel<State_T, Transition_T, Cost_T>::nodes() const node_count() Return the number of nodes. Declaration size_t model::GraphModel<State_T, Transition_T, Cost_T>::node_count() const node() Return a specific node by NodeId. Declaration const Node&model::GraphModel<State_T, Transition_T, Cost_T>::node(size_t id) const edges() Return a vector of all the edges. Declaration const std::vector<Edge>&model::GraphModel<State_T, Transition_T, Cost_T>::edges() const edge_count() Return the number of edges. Declaration size_t model::GraphModel<State_T, Transition_T, Cost_T>::edge_count() const edge() Return a specific edge by EdgeId. Declaration const Edge&model::GraphModel<State_T, Transition_T, Cost_T>::edge(size_t id) const configure() Configure the graph from JSON document. Declaration void model::GraphModel<State_T, Transition_T, Cost_T>::configure(const utils::Json&json) override configure() Configure the graph from preloaded graph. Declaration void model::GraphModel<State_T, Transition_T, Cost_T>::configure(Configuration_T&configuration) get_sweep_size() Declaration size_t model::GraphModel<State_T, Transition_T, Cost_T>::get_sweep_size() const override get_term_count() Declaration size_t model::GraphModel<State_T, Transition_T, Cost_T>::get_term_count() const override get_benchmark_properties() Fill the graph benchmarking properties. Declaration utils::Structure model::GraphModel<State_T, Transition_T, Cost_T>::get_benchmark_properties() const override is_rescaled() Declaration bool model::GraphModel<State_T, Transition_T, Cost_T>::is_rescaled() const override rescale() Declaration void model::GraphModel<State_T, Transition_T, Cost_T>::rescale() override get_scale_factor() Declaration double model::GraphModel<State_T, Transition_T, Cost_T>::get_scale_factor() const override get_const_cost() Declaration double model::GraphModel<State_T, Transition_T, Cost_T>::get_const_cost() const override is_empty() Declaration bool model::GraphModel<State_T, Transition_T, Cost_T>::is_empty() const override estimate_max_cost_diff() Declaration virtual double model::GraphModel<State_T, Transition_T, Cost_T>::estimate_max_cost_diff() const override has_initial_configuration() Declaration bool model::GraphModel<State_T, Transition_T, Cost_T>::has_initial_configuration() const override get_initial_configuration() Declaration const std::vector<std::pair<int, int>>&model::GraphModel<State_T, Transition_T, Cost_T>::get_initial_configuration() const"
  },
  "api/model/index.html": {
    "href": "api/model/index.html",
    "title": "model:: | qiotoolkit",
    "keywords": "Model"
  },
  "api/model/ising-compact-state.html": {
    "href": "api/model/ising-compact-state.html",
    "title": "Class model::IsingCompactState | qiotoolkit",
    "keywords": "Class model::IsingCompactState ISING model state for IsingCompact. Inheritance model::IsingState model::IsingCompactState Inherited Members copy_state_only IsingState render render memory_estimate state_only_memory_estimate IsingState configure ~Component Component get_status param get_class_name Constructors IsingCompactState() Declaration model::IsingCompactState::IsingCompactState() IsingCompactState() Create an Ising state with N spins and M terms. All spins are initialized to 0 (\"up\" or \"+1\"). A true value represents \"down\" or \"-1\". As such, the boolean value can be considered as representing the sign. Declaration model::IsingCompactState::IsingCompactState(size_t N, size_t)"
  },
  "api/model/ising-compact.html": {
    "href": "api/model/ising-compact.html",
    "title": "Class model::IsingCompact | qiotoolkit",
    "keywords": "Class model::IsingCompact Ising model which use GraphCompactModel as model base. Inheritance model::GraphCompactModel model::IsingCompact Inherited Members Model state_only_memory_estimate get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff set_step_limit state_memory_estimate render calculate_cost_difference get_random_transition configure ~BaseModel configure BaseModel init get_const_cost get_sweep_size is_empty get_term_count GraphCompactModel has_initial_configuration is_rescaled rescale edge_count estimate_max_cost_diff edge get_scale_factor get_initial_configuration node_count ~GraphCompactModel get_benchmark_properties render ~Component Component get_status param get_class_name Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::IsingCompact<Element_T>::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::IsingCompact<Element_T>::get_version() const override match_version() Checks the string argument against the expected version. This can be overloaded to accept other version than the one returned by get_version(). Declaration void model::IsingCompact<Element_T>::match_version(const std::string&version) override calculate_cost() Cost function: \\mathcal{H} = -\\sum_{j\\in\\mathrm{\\{edges\\}}} c_j\\prod_{i\\in j}s_i Declaration double model::IsingCompact<Element_T>::calculate_cost(const State_T&state) const override calculate_cost_difference() Cost difference: \\Delta_{i\\to i'} = 2 \\sum_{j\\in e_i} c_j\\prod_{i\\in j}s_i Declaration double model::IsingCompact<Element_T>::calculate_cost_difference(const IsingCompactState&state, const size_t&spin_id) const override get_random_state() Return a random Ising state. The number of Ising variables (terms) is determined from the number of nodes (edges) in the underlying graph. Declaration IsingCompactState model::IsingCompact<Element_T>::get_random_state(utils::RandomGenerator&rng) const override get_initial_configuration_state() Declaration IsingCompactState model::IsingCompact<Element_T>::get_initial_configuration_state() const override get_random_transition() Return a random Single spin update. We represent a single spin update as merely the index of the spin variable that will flip. Declaration size_t model::IsingCompact<Element_T>::get_random_transition(const IsingCompactState&, utils::RandomGenerator&rng) const override apply_transition() Flip the spin specified by spin_id. Declaration void model::IsingCompact<Element_T>::apply_transition(const size_t&spin_id, IsingCompactState&state) const override configure() Serialize metadata and the underlying graph. The underlying graph represents the \"disorder\" of the Ising model (glass) we are simulating. Declaration void model::IsingCompact<Element_T>::configure(const utils::Json&json) override configure() Declaration void model::IsingCompact<Element_T>::configure(typename GraphCompact::Configuration_T&configuration) render_state() Declaration utils::Structure model::IsingCompact<Element_T>::render_state(const IsingCompactState&state) const override state_memory_estimate() Declaration size_t model::IsingCompact<Element_T>::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::IsingCompact<Element_T>::state_only_memory_estimate() const override estimate_min_cost_diff() Declaration double model::IsingCompact<Element_T>::estimate_min_cost_diff() const override estimate_max_cost_diff() Declaration double model::IsingCompact<Element_T>::estimate_max_cost_diff() const override get_term() Declaration double model::IsingCompact<Element_T>::get_term(const State_T&state, size_t term_id) const"
  },
  "api/model/ising-grouped.html": {
    "href": "api/model/ising-grouped.html",
    "title": "Class model::IsingGrouped | qiotoolkit",
    "keywords": "Class model::IsingGrouped Inheritance model::FacedGraphModel model::IsingGrouped Inherited Members has_initial_configuration Model state_only_memory_estimate init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff set_step_limit state_memory_estimate render calculate_cost_difference get_random_transition configure ~BaseModel configure BaseModel estimate_max_cost_diff get_benchmark_properties node faces get_term_count get_sweep_size calculate_cost_difference edge_count nodes get_scale_factor get_const_cost is_empty edge face is_rescaled node_count collate populate_bbstrees edges rescale render ~Component Component get_status param get_class_name Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::IsingGrouped::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::IsingGrouped::get_version() const override match_version() Checks the string argument against the expected version. This can be overloaded to accept other version than the one returned by get_version(). Declaration void model::IsingGrouped::match_version(const std::string&version) override configure() Populates model internals from the input json. Declaration void model::IsingGrouped::configure(const utils::Json&json) override configure() Declaration void model::IsingGrouped::configure(typename Base_T::Configuration_T&configuration) get_random_state() Return a valid random state for the model. The various algorithms don't know how to initialize a valid state; this allows them to start with a random one. NOTE: The rng being passed should be used for randomness (as opposed to creating one for the model), as multiple threads can potentially use the same underlying model (albeit with separate rngs). Declaration State_T model::IsingGrouped::get_random_state(utils::RandomGenerator&rng) const override get_random_transition() Return a random Single spin update. We represent a single spin update as merely the index of the spin variable that will flip. Declaration Transition_T model::IsingGrouped::get_random_transition(const State_T&, utils::RandomGenerator&rng) const override render_state() Declaration utils::Structure model::IsingGrouped::render_state(const State_T&state) const override state_memory_estimate() Declaration size_t model::IsingGrouped::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::IsingGrouped::state_only_memory_estimate() const override calculate_cost() Declaration Cost_T model::IsingGrouped::calculate_cost(const State_T&state) const calculate_cost_difference() Declaration Cost_T model::IsingGrouped::calculate_cost_difference(const State_T&state, const Transition_T&transition, const Cost_T *cost) const override apply_transition() Declaration void model::IsingGrouped::apply_transition(const Transition_T&transition, State_T&state, Cost_T *cost) const override apply_transition() Declaration void model::IsingGrouped::apply_transition(const Transition_T&transition, State_T&state) const override get_term() Evaluate the term with id edge_id. Declaration double model::IsingGrouped::get_term(const State_T&state, size_t edge_id) const insert_val_to_bbstrees() Declaration void model::IsingGrouped::insert_val_to_bbstrees(double val, std::multiset<double>&tree_A, std::multiset<double>*tree_B=nullptr) const override amend_collation() Declaration void model::IsingGrouped::amend_collation(std::map<int, double>&coeffs, int node_id) const override estimate_min_cost_diff() Declaration double model::IsingGrouped::estimate_min_cost_diff() const"
  },
  "api/model/ising-state.html": {
    "href": "api/model/ising-state.html",
    "title": "Class model::IsingState | qiotoolkit",
    "keywords": "Class model::IsingState Ising state representation. We represent an ising state as both the boolean value of each spin and the product of the spins in each term. Inheritance markov::State model::IsingState model::IsingCompactState model::IsingTermCachedState Inherited Members configure ~Component Component get_status param get_class_name Constructors IsingState() Declaration model::IsingState::IsingState() IsingState() Create an Ising state with N spins and M terms. All spins are initialized to 0 (\"up\" or \"+1\"). A true value represents \"down\" or \"-1\". As such, the boolean value can be considered as representing the sign. Declaration model::IsingState::IsingState(size_t N, size_t) Methods render() Render the spins of the state. Declaration utils::Structure model::IsingState::render() const override render() Declaration utils::Structure model::IsingState::render(const std::map<int, int>&) const copy_state_only() Declaration void model::IsingState::copy_state_only(const IsingState&other) memory_estimate() Declaration static size_t model::IsingState::memory_estimate(size_t N, size_t) state_only_memory_estimate() Declaration static size_t model::IsingState::state_only_memory_estimate(size_t N)"
  },
  "api/model/ising-term-cached-state.html": {
    "href": "api/model/ising-term-cached-state.html",
    "title": "Class model::IsingTermCachedState | qiotoolkit",
    "keywords": "Class model::IsingTermCachedState Ising state representation with term cache. Inheritance model::IsingState model::IsingTermCachedState Inherited Members copy_state_only IsingState render render IsingState configure ~Component Component get_status param get_class_name Constructors IsingTermCachedState() Declaration model::IsingTermCachedState::IsingTermCachedState() IsingTermCachedState() Declaration model::IsingTermCachedState::IsingTermCachedState(size_t N, size_t M) Methods copy_state_only() Declaration void model::IsingTermCachedState::copy_state_only(const IsingTermCachedState&other) memory_estimate() Declaration static size_t model::IsingTermCachedState::memory_estimate(size_t N, size_t M) state_only_memory_estimate() Declaration static size_t model::IsingTermCachedState::state_only_memory_estimate(size_t N)"
  },
  "api/model/ising-term-cached.html": {
    "href": "api/model/ising-term-cached.html",
    "title": "Class model::IsingTermCached | qiotoolkit",
    "keywords": "Class model::IsingTermCached Inheritance model::AbstractIsing model::IsingTermCached Inherited Members Model init render_state calculate_cost apply_transition set_step_limit render calculate_cost_difference get_random_transition configure configure estimate_min_cost_diff render_state get_identifier match_version configure state_only_memory_estimate get_random_transition get_initial_configuration_state get_version get_random_state estimate_max_cost_diff state_memory_estimate calculate_cost calculate_cost_difference get_version ~BaseModel configure BaseModel get_identifier edge get_const_cost node rescale node_count get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled edges get_initial_configuration configure nodes get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Methods apply_transition() Flip the spin specified by spin_id. Declaration void model::IsingTermCached::apply_transition(const Transition_T&spin_id, IsingTermCachedState&state) const override get_term() Get the term with id term_id from cache. Declaration double model::IsingTermCached::get_term(const IsingTermCachedState&state, size_t term_id) const override"
  },
  "api/model/ising.html": {
    "href": "api/model/ising.html",
    "title": "Class model::Ising | qiotoolkit",
    "keywords": "Class model::Ising Inheritance model::AbstractIsing model::Ising Inherited Members Model init render_state calculate_cost set_step_limit render calculate_cost_difference get_random_transition configure configure estimate_min_cost_diff render_state get_identifier match_version configure state_only_memory_estimate get_random_transition get_initial_configuration_state get_version get_random_state estimate_max_cost_diff state_memory_estimate calculate_cost calculate_cost_difference apply_transition get_version ~BaseModel configure BaseModel get_identifier edge get_const_cost node rescale node_count get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled edges get_initial_configuration configure nodes get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Methods apply_transition() Flip the spin specified by spin_id. Declaration void model::Ising::apply_transition(const size_t&spin_id, State_T&state) const override get_term() Calculate the term with id term_id. Declaration double model::Ising::get_term(const IsingState&state, size_t term_id) const override"
  },
  "api/model/max-sat-configuration.html": {
    "href": "api/model/max-sat-configuration.html",
    "title": "Class model::MaxSatConfiguration | qiotoolkit",
    "keywords": "Class model::MaxSatConfiguration MaxSat stream configuration. Inheritance model::BaseModelConfiguration model::MaxSatConfiguration"
  },
  "api/model/max-sat-configuration/get/terms.html": {
    "href": "api/model/max-sat-configuration/get/terms.html",
    "title": "Struct model::MaxSatConfiguration::Get_Terms | qiotoolkit",
    "keywords": "Struct model::MaxSatConfiguration::Get_Terms Methods get() Declaration static std::vector<utils::Dimacs::Clause>&model::MaxSatConfiguration::Get_Terms::get(MaxSatConfiguration&config) get_key() Declaration static std::string model::MaxSatConfiguration::Get_Terms::get_key()"
  },
  "api/model/max-sat-state.html": {
    "href": "api/model/max-sat-state.html",
    "title": "Class model::MaxSatState | qiotoolkit",
    "keywords": "Class model::MaxSatState Representation of a MaxSat state. We store the variable values and a counter for each clause. Inheritance model::MaxSatState Constructors MaxSatState() Default constructor for containers. Declaration model::MaxSatState<Counter_T>::MaxSatState() MaxSatState() Create a MaxSatState with nvar variables and ncl clauses. [!NOTE] MaxSat will always create an extra clause at the begining such that the actual clauses can be accessed with 1-based indices. Declaration model::MaxSatState<Counter_T>::MaxSatState(size_t nvar, size_t ncl) Methods copy_state_only() Copy only the state from another MaxSatState. Declaration void model::MaxSatState<Counter_T>::copy_state_only(const MaxSatState&other) render() Render the state by dumping the variable values. [!NOTE] To get correct labelling, use the MaxSat::render_state(...) method. Declaration utils::Structure model::MaxSatState<Counter_T>::render() const memory_estimate() Declaration static size_t model::MaxSatState<Counter_T>::memory_estimate(size_t variables, size_t clauses) state_only_memory_estimate() Declaration static size_t model::MaxSatState<Counter_T>::state_only_memory_estimate(size_t variables)"
  },
  "api/model/max-sat.html": {
    "href": "api/model/max-sat.html",
    "title": "Class model::MaxSat | qiotoolkit",
    "keywords": "Class model::MaxSat MaxSatModel. Natively simulates weighted satisfiability problems. Inheritance markov::Model model::MaxSat Inherited Members get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model state_only_memory_estimate get_term_count get_sweep_size init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate rescale render calculate_cost_difference get_random_transition get_const_cost configure match_version ~BaseModel BaseModel render ~Component Component get_status param get_class_name Constructors MaxSat() Declaration model::MaxSat<Counter_T>::MaxSat() Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::MaxSat<Counter_T>::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::MaxSat<Counter_T>::get_version() const override create_state() Create an all-false state with the correct number of variables and clause-counters for this model. Declaration State_T model::MaxSat<Counter_T>::create_state() const create_state() Create a state with specific initial variable values. Declaration State_T model::MaxSat<Counter_T>::create_state(std::vector<bool>variables) const get_random_state() Create a state with random initial variable values. Declaration State_T model::MaxSat<Counter_T>::get_random_state(utils::RandomGenerator&rng) const override get_random_transition() Pick a random transition. Declaration size_t model::MaxSat<Counter_T>::get_random_transition(const State_T&, utils::RandomGenerator&rng) const override calculate_cost() Calculate the cost (sum of active weights) for state. The cost for (weighted) MaxSat is defined as the sum of the weights of the UNSATISIFIED clauses. Declaration Cost_T model::MaxSat<Counter_T>::calculate_cost(const State_T&state) const override calculate_cost_difference() Calculate the change in cost if transition were applied. Declaration Cost_T model::MaxSat<Counter_T>::calculate_cost_difference(const State_T&state, const size_t&transition) const override apply_transition() Apply the effects of transition to the variable and counters in state. Declaration void model::MaxSat<Counter_T>::apply_transition(const size_t&transition, State_T&state) const override configure() Read a max-sat problem from json. Declaration void model::MaxSat<Counter_T>::configure(const utils::Json&json) override configure() Configure using stream configuration. Declaration void model::MaxSat<Counter_T>::configure(Configuration_T&config) configure() Read a max-sat problem from dimacs. Declaration void model::MaxSat<Counter_T>::configure(const utils::Dimacs&dimacs) configure() Turn a list of clauses into adj-list representation for simulation. This uses the position in variables as the variable_id and makes clauses 1-indexed instead (with a negation in the adj-list denoting negated participation in a clause). Declaration void model::MaxSat<Counter_T>::configure(const std::vector<utils::Dimacs::Clause>&clauses) configure() Take the configuration from another model. (this allows changing the counter type after configuration) Declaration void model::MaxSat<Counter_T>::configure(model::BaseModel *base) override render_state() Render a state with the original variable names. Example: [1, 2, 3, -4, -5, 6] mean there were six variables in the original input with consecutive names 1..6 and the solution found has variables 4 and 5 negated. Declaration virtual utils::Structure model::MaxSat<Counter_T>::render_state(const State_T&state) const override is_empty() Declaration bool model::MaxSat<Counter_T>::is_empty() const override state_memory_estimate() Declaration size_t model::MaxSat<Counter_T>::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::MaxSat<Counter_T>::state_only_memory_estimate() const override get_term_count() Declaration size_t model::MaxSat<Counter_T>::get_term_count() const override get_max_weight() Declaration Cost_T model::MaxSat<Counter_T>::get_max_weight() const get_max_vars_in_clause() Declaration size_t model::MaxSat<Counter_T>::get_max_vars_in_clause() const estimate_max_cost_diff() Declaration double model::MaxSat<Counter_T>::estimate_max_cost_diff() const override estimate_min_cost_diff() Declaration double model::MaxSat<Counter_T>::estimate_min_cost_diff() const override"
  },
  "api/model/model-registrar.html": {
    "href": "api/model/model-registrar.html",
    "title": "Struct model::ModelRegistrar | qiotoolkit",
    "keywords": "Struct model::ModelRegistrar Helper struct to statically fill the registry. Constructors ModelRegistrar() Declaration model::ModelRegistrar<Model_T>::ModelRegistrar(int)"
  },
  "api/model/model-registration-impl.html": {
    "href": "api/model/model-registration-impl.html",
    "title": "Class model::ModelRegistrationImpl | qiotoolkit",
    "keywords": "Class model::ModelRegistrationImpl Shared implementation of the Registry entry (templated) Inheritance model::ModelRegistrationInterface model::ModelRegistrationImpl Inherited Members ~ModelRegistrationInterface Methods create() Create an instance of the model. Declaration BaseModel* model::ModelRegistrationImpl<Model_T>::create() const override"
  },
  "api/model/model-registration-interface.html": {
    "href": "api/model/model-registration-interface.html",
    "title": "Class model::ModelRegistrationInterface | qiotoolkit",
    "keywords": "Class model::ModelRegistrationInterface Interface for an individual Model's registry entry. For now this only allows instantiating an instance of the model. Inheritance model::ModelRegistrationInterface model::ModelRegistrationImpl Methods ~ModelRegistrationInterface() Declaration virtual model::ModelRegistrationInterface::~ModelRegistrationInterface() create() Create an instance of the model. Declaration virtual BaseModel* model::ModelRegistrationInterface::create() const =0"
  },
  "api/model/model-registry.html": {
    "href": "api/model/model-registry.html",
    "title": "Class model::ModelRegistry | qiotoolkit",
    "keywords": "Class model::ModelRegistry Registry for model selection. The model registry allows models to be instantiated from their identifier Usage: Registering a new model: class MyModel : public [model::BaseModel](xref:classmodel_1_1BaseModel) { public: // Must have a constructor without arguments. MyModel() = default; // This is the identifier the model is registered with. std::string get_identifier() const override { return \"my_model\"; } ... } [REGISTER_MODEL(MyModel)](xref:model__registry_8h_1ad6bd7a2bf339ad155cb53db753d98cbc); ``` * Instantiating a model by identifier: ```c++ auto* my_model = [model::ModelRegistry::create](xref:classmodel_1_1ModelRegistry_1abab837dbb1601ca2bdb9a7d0ea91217d)(\"my_model\"); ``` Inheritance model::ModelRegistry Constructors ModelRegistry() This is a singleton class. Declaration model::ModelRegistry::ModelRegistry() Methods has() Check whether a model with that identifier is registered. Declaration static bool model::ModelRegistry::has(const std::string&identifier) get() Find and return the model registration. Declaration static const ModelRegistrationInterface* model::ModelRegistry::get(const std::string&identifier) create() Find the model registration and create an instance. Declaration static BaseModel* model::ModelRegistry::create(const std::string&identifier) add() Add an entry to the model registry. This is used by the ModelRegistrar in the REGISTER_MODEL macro, you should not need to invoke it directly. Declaration static void model::ModelRegistry::add(const std::string&identifier, std::unique_ptr<ModelRegistrationInterface>&&registration) instance() Access the singleton. Declaration static ModelRegistry&model::ModelRegistry::instance()"
  },
  "api/model/partition.html": {
    "href": "api/model/partition.html",
    "title": "Class model::Partition | qiotoolkit",
    "keywords": "Class model::Partition Partition state. This implements a markov::State that is a partition of N elements into two sets (with the first one of size K). It is optimized for two operations: Querying whether a given number is in the first set 2) Swapping two random elements from different groups For this, two internal vectors of the respective elements are kept. These are NOT sorted and the class does not provide an interface to select a specific number to swap (they can only be selected by their index, while the current ordering is unknown). Note If you need more sets or don't want the respective set sizes to be fixed, the PottsState is likely a better candidate for your use case. Inheritance markov::State model::Partition Inherited Members configure ~Component Component get_status param get_class_name Constructors Partition() Create an uninitialized partition. Declaration model::Partition::Partition() Methods in_first() query whether number is in the first set. Declaration bool model::Partition::in_first(size_t number) const swap_indices() Swap the indices idx_first, idx_second The indices refer to the current (unsorted) internal list of the elements in the first and second set. Declaration void model::Partition::swap_indices(size_t idx_first, size_t idx_second) render() Render the two sets. Declaration utils::Structure model::Partition::render() const override copy_state_only() Declaration void model::Partition::copy_state_only(const Partition&other) random() Generate a random partition of N elements with the first set of size K. Declaration Partition model::Partition::random(size_t N, size_t K, utils::RandomGenerator&rng)"
  },
  "api/model/permutation.html": {
    "href": "api/model/permutation.html",
    "title": "Class model::Permutation | qiotoolkit",
    "keywords": "Class model::Permutation Inheritance markov::State model::Permutation Inherited Members configure ~Component Component get_status param get_class_name Constructors Permutation() Declaration model::Permutation::Permutation() Permutation() Declaration model::Permutation::Permutation(size_t N) Methods operator[]() Declaration size_t model::Permutation::operator[](size_t i) const size() Declaration size_t model::Permutation::size() const move_node() Declaration void model::Permutation::move_node(size_t a, size_t b) swap_nodes() Declaration void model::Permutation::swap_nodes(size_t a, size_t b) swap_edges() Declaration void model::Permutation::swap_edges(size_t a, size_t b) render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure model::Permutation::render() const override copy_state_only() Declaration void model::Permutation::copy_state_only(const Permutation&other) random() Declaration Permutation model::Permutation::random(size_t N, utils::RandomGenerator&rng) memory_estimate() Declaration static size_t model::Permutation::memory_estimate(size_t N) state_only_memory_estimate() Declaration static size_t model::Permutation::state_only_memory_estimate(size_t N)"
  },
  "api/model/poly-configuration.html": {
    "href": "api/model/poly-configuration.html",
    "title": "Class model::PolyConfiguration | qiotoolkit",
    "keywords": "Class model::PolyConfiguration Inheritance model::BaseModelConfiguration model::PolyConfiguration"
  },
  "api/model/poly-configuration/get/terms.html": {
    "href": "api/model/poly-configuration/get/terms.html",
    "title": "Struct model::PolyConfiguration::Get_Terms | qiotoolkit",
    "keywords": "Struct model::PolyConfiguration::Get_Terms Methods get() Declaration static std::vector<PolyTermConfiguration>&model::PolyConfiguration::Get_Terms::get(PolyConfiguration&config) get_key() Declaration static std::string model::PolyConfiguration::Get_Terms::get_key()"
  },
  "api/model/poly-spins-configuration.html": {
    "href": "api/model/poly-spins-configuration.html",
    "title": "Class model::PolySpinsConfiguration | qiotoolkit",
    "keywords": "Class model::PolySpinsConfiguration Inheritance model::PolySpinsConfiguration model::PolyTermConfiguration Constructors PolySpinsConfiguration() Declaration model::PolySpinsConfiguration::PolySpinsConfiguration()"
  },
  "api/model/poly-spins-configuration/get/constant.html": {
    "href": "api/model/poly-spins-configuration/get/constant.html",
    "title": "Struct model::PolySpinsConfiguration::Get_Constant | qiotoolkit",
    "keywords": "Struct model::PolySpinsConfiguration::Get_Constant Methods get() Declaration static double&model::PolySpinsConfiguration::Get_Constant::get(PolySpinsConfiguration&config) get_key() Declaration static std::string model::PolySpinsConfiguration::Get_Constant::get_key()"
  },
  "api/model/poly-spins-configuration/get/ids.html": {
    "href": "api/model/poly-spins-configuration/get/ids.html",
    "title": "Struct model::PolySpinsConfiguration::Get_Ids | qiotoolkit",
    "keywords": "Struct model::PolySpinsConfiguration::Get_Ids Methods get() Declaration static std::vector<size_t>&model::PolySpinsConfiguration::Get_Ids::get(PolySpinsConfiguration&config) get_key() Declaration static std::string model::PolySpinsConfiguration::Get_Ids::get_key()"
  },
  "api/model/poly-state.html": {
    "href": "api/model/poly-state.html",
    "title": "Class model::PolyState | qiotoolkit",
    "keywords": "Class model::PolyState PolyState. besides storing the Ising spins, PolyState is also initialized by the model to store intermediate results of the cost function evaluations for each term. This allows efficient evaluation and application of the cost difference a specific update would cause. Inheritance markov::State model::PolyState Inherited Members configure ~Component Component get_status param get_class_name Methods render() Render the current state of the spins in the model output format: {\"0\": +1, \"1\": -1, ...}. Declaration utils::Structure model::PolyState::render() const copy_state_only() Declaration void model::PolyState::copy_state_only(const PolyState&other) memory_estimate() Declaration static size_t model::PolyState::memory_estimate(size_t n_spins, size_t n_terms, size_t n_parameters) state_only_memory_estimate() Declaration static size_t model::PolyState::state_only_memory_estimate(size_t n_spins)"
  },
  "api/model/poly-term-configuration.html": {
    "href": "api/model/poly-term-configuration.html",
    "title": "Class model::PolyTermConfiguration | qiotoolkit",
    "keywords": "Class model::PolyTermConfiguration Inheritance model::PolySpinsConfiguration model::PolyTermConfiguration Inherited Members PolySpinsConfiguration Constructors PolyTermConfiguration() Declaration model::PolyTermConfiguration::PolyTermConfiguration()"
  },
  "api/model/poly-term-configuration/get/exponent.html": {
    "href": "api/model/poly-term-configuration/get/exponent.html",
    "title": "Struct model::PolyTermConfiguration::Get_Exponent | qiotoolkit",
    "keywords": "Struct model::PolyTermConfiguration::Get_Exponent Methods get() Declaration static int&model::PolyTermConfiguration::Get_Exponent::get(PolyTermConfiguration&config) get_key() Declaration static std::string model::PolyTermConfiguration::Get_Exponent::get_key()"
  },
  "api/model/poly-term-configuration/get/parameter.html": {
    "href": "api/model/poly-term-configuration/get/parameter.html",
    "title": "Struct model::PolyTermConfiguration::Get_Parameter | qiotoolkit",
    "keywords": "Struct model::PolyTermConfiguration::Get_Parameter Methods get() Declaration static std::string&model::PolyTermConfiguration::Get_Parameter::get(PolyTermConfiguration&config) get_key() Declaration static std::string model::PolyTermConfiguration::Get_Parameter::get_key()"
  },
  "api/model/poly-term-configuration/get/terms.html": {
    "href": "api/model/poly-term-configuration/get/terms.html",
    "title": "Struct model::PolyTermConfiguration::Get_Terms | qiotoolkit",
    "keywords": "Struct model::PolyTermConfiguration::Get_Terms Methods get() Declaration static std::vector<PolySpinsConfiguration>&model::PolyTermConfiguration::Get_Terms::get(PolyTermConfiguration&config) get_key() Declaration static std::string model::PolyTermConfiguration::Get_Terms::get_key()"
  },
  "api/model/poly-transition.html": {
    "href": "api/model/poly-transition.html",
    "title": "Class model::PolyTransition | qiotoolkit",
    "keywords": "Class model::PolyTransition PolyTransition. We define two types of transitions: If spin_id is positive, the transition applies a spin-flip to the spin with this id. Otherwise the transition changes the parameter values to the ones denoted in parameter_values (all may change concurrently). Inheritance markov::Transition model::PolyTransition Inherited Members configure render ~Component Component get_status param get_class_name Methods is_spin_flip() Declaration bool model::PolyTransition::is_spin_flip() const spin_id() Declaration size_t model::PolyTransition::spin_id() const set_spin_id() Declaration void model::PolyTransition::set_spin_id(size_t spin_id) parameter_values() Declaration const std::vector<double>&model::PolyTransition::parameter_values() const set_parameter_values() Declaration void model::PolyTransition::set_parameter_values(const std::vector<double>&values)"
  },
  "api/model/poly.html": {
    "href": "api/model/poly.html",
    "title": "Class model::Poly | qiotoolkit",
    "keywords": "Class model::Poly Poly Model. Defines an Ising model with polynomial cost function. The cost function can be defined via recursive nesting of terms of the form: term_i = c_i p_i (\\sum_j term_j) ^ e_i or - term_i = c_i p_i \\prod_j s_j where c_i is a numeric constant p_i is a parameter (which can be changed during the simulation) e_i is an integer exponent s_i is an Ising spin Note The limitation to Ising spins and products only in non-leaf of such spins is deliberate (this avoids needing to track the number of zeros in products). See Poly::configure() for the input format definition. Inheritance markov::Model model::Poly Inherited Members get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model state_only_memory_estimate get_term_count get_sweep_size init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate rescale render calculate_cost_difference get_random_transition get_const_cost configure match_version ~BaseModel configure BaseModel render ~Component Component get_status param get_class_name Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::Poly::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::Poly::get_version() const override get_const_cost() Declaration double model::Poly::get_const_cost() const override is_empty() Declaration bool model::Poly::is_empty() const override create_state() Create a state with specific spin and parameter values. Parameters: spins array of initial spin values {-1, 1} parameters array of initial parameter values (defaults to all-1). Returns: An initialized state with intermediate term values populated. Declaration PolyState model::Poly::create_state(std::vector<int>spins, std::vector<double>parameters={}) const get_random_state() Create a random state. This fill the state with random spin values {-1, +1} and all parameters set to 1.0. Declaration PolyState model::Poly::get_random_state(utils::RandomGenerator&rng) const override create_spin_flip() Create a specific spin flip. This returns a transition which spins flip spin_id. Declaration PolyTransition model::Poly::create_spin_flip(size_t spin_id) const create_parameter_change() Create a parameter change. This returns a transition which substitutes the model parameters. Declaration PolyTransition model::Poly::create_parameter_change(const std::vector<double>values) const get_random_transition() Return a random spin flip. The default markov transition for Poly is a random spin flip. create_parameter_change to manually create a parameter transition. Declaration PolyTransition model::Poly::get_random_transition(const PolyState&state, utils::RandomGenerator&rng) const override calculate_cost() Get the full cost function. We can entirely rely on the root term's intermediate value. Declaration double model::Poly::calculate_cost(const PolyState&state) const override calculate_cost_difference() Calculate the cost difference for a transition. Declaration double model::Poly::calculate_cost_difference(const PolyState&state, const PolyTransition&transition) const override apply_transition() Apply a transition to a state. Declaration void model::Poly::apply_transition(const PolyTransition&transition, PolyState&state) const override get_sweep_size() Declaration size_t model::Poly::get_sweep_size() const override configure() Populates model internals from the input json. Declaration void model::Poly::configure(const utils::Json&json) override configure() Declaration void model::Poly::configure(Configuration_T&configuration) print() Declaration std::string model::Poly::print() const get_parameters() Declaration std::vector<std::string>model::Poly::get_parameters() const get_spin_overlap() Declaration double model::Poly::get_spin_overlap(const PolyState&s, const PolyState&t) const get_term_overlap() Declaration double model::Poly::get_term_overlap(const PolyState&s, const PolyState&t) const state_memory_estimate() Declaration size_t model::Poly::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::Poly::state_only_memory_estimate() const override configure_term() Read a term from the input configuration. Declaration void model::Poly::configure_term(const utils::Json&json, int parent=-1) configure_term() Declaration void model::Poly::configure_term(PolySpinsConfiguration&input, int parent=-1) configure_term() Declaration void model::Poly::configure_term(PolyTermConfiguration&input, int parent=-1) configure_term() Declaration void model::Poly::configure_term(std::vector<PolyTermConfiguration>&, int parent=-1) initialize_terms() Initialize a state. This takes a state with spins and parameters set and populates its intermediate evaluation results in terms. This is the only time we evaluate all of the terms. All subsequent calls to calculate_cost(state) and calculate_cost_difference(state, transition) rely on these terms. Declaration void model::Poly::initialize_terms(PolyState&state) const calculate_term_differences() Calculate difference for each term. This calculates all the nodes affected by a transition and bubbles those changes up to the root node of the poly tree. Declaration std::map<size_t, double>model::Poly::calculate_term_differences(const PolyState&state, const PolyTransition&transition) const term_factor() Return the factor preceding term term_id This computes the product constant * parameter where the parameter is optional and taken from the set of parameters passed. Declaration double model::Poly::term_factor(const std::vector<double>parameters, size_t term_id) const term_to_string() Render a term's string representation (recursively) Declaration std::string model::Poly::term_to_string(size_t term_id) const"
  },
  "api/model/poly/term.html": {
    "href": "api/model/poly/term.html",
    "title": "Struct model::Poly::Term | qiotoolkit",
    "keywords": "Struct model::Poly::Term Representation of a recursive term in the polynomial. A term takes the form constant * parameter (sum_j term_j) ^ exponent or - constant * parameter (prod_j spin_j) The term stores its own properties (constant, parameter_id, exponent) and the id of its parent term. It does not track a list of its child terms or spins, since we typically evaluate the cost function bottom-up. (top-down traversel, as needed by term_to_string is inefficient). Note For terms of the second kind, there is no exponent. Constructors Term() Initialize a default term. Declaration model::Poly::Term::Term(int parent_id_=-1) Methods is_leaf_node() Declaration bool model::Poly::Term::is_leaf_node() const has_constant() Declaration bool model::Poly::Term::has_constant() const has_parameter() Declaration bool model::Poly::Term::has_parameter() const"
  },
  "api/model/potts-state.html": {
    "href": "api/model/potts-state.html",
    "title": "Class model::PottsState | qiotoolkit",
    "keywords": "Class model::PottsState Potts state representation. Represents N potts spins as a vector . Inheritance markov::State model::PottsState Inherited Members configure ~Component Component get_status param get_class_name Constructors PottsState() Declaration model::PottsState::PottsState() PottsState() Initialize a state of N potts spins to 0. Declaration model::PottsState::PottsState(size_t N) Methods render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure model::PottsState::render() const override copy_state_only() Declaration void model::PottsState::copy_state_only(const PottsState&other) memory_estimate() Declaration static size_t model::PottsState::memory_estimate(size_t N) state_only_memory_estimate() Declaration static size_t model::PottsState::state_only_memory_estimate(size_t N)"
  },
  "api/model/potts-transition.html": {
    "href": "api/model/potts-transition.html",
    "title": "Class model::PottsTransition | qiotoolkit",
    "keywords": "Class model::PottsTransition Potts transition representation. Which spin to change and to what value. Inheritance markov::Transition model::PottsTransition Inherited Members configure render ~Component Component get_status param get_class_name Constructors PottsTransition() Declaration model::PottsTransition::PottsTransition() PottsTransition() Initialize a transition of spin_id to value. Note The choice of value (both its range and w.r.t. the current value of spins[spin_id] is handled by the model. Declaration model::PottsTransition::PottsTransition(size_t spin_id, size_t value) Methods apply() Apply the transition to the given state. Declaration void model::PottsTransition::apply(PottsState&state) const spin_id() Declaration size_t model::PottsTransition::spin_id() const value() Declaration size_t model::PottsTransition::value() const"
  },
  "api/model/potts.html": {
    "href": "api/model/potts.html",
    "title": "Class model::Potts | qiotoolkit",
    "keywords": "Class model::Potts Potts Model. This model implements a hamiltonian of the form \\(\\mathcal{H} = \\sum_j c_j \\delta_{s_{j,1}, s{j,2}, \\ldots}\\), where \\(\\delta\\) is the kronecker delta, which is 1 iff all its subscripts are identical (and 0 otherwise). That is: Each term in this cost function will contribute only if all Potts spins participating in it are of the same value. Inheritance model::GraphModel model::Potts Inherited Members Model state_only_memory_estimate init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff set_step_limit state_memory_estimate render calculate_cost_difference get_random_transition configure match_version ~BaseModel configure BaseModel edge get_const_cost node rescale node_count estimate_max_cost_diff get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled edges get_initial_configuration configure nodes get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Constructors Potts() Declaration model::Potts::Potts() Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::Potts::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::Potts::get_version() const override calculate_cost() Calculate the Potts Hamiltonian. Declaration double model::Potts::calculate_cost(const PottsState&state) const override calculate_cost_difference() Recompute the parts affected by transition. Declaration double model::Potts::calculate_cost_difference(const PottsState&state, const PottsTransition&transition) const override get_random_state() Return a random Potts state. This function decides on the number of potts spins and the possible values of each to return (according to the model configuration). Declaration PottsState model::Potts::get_random_state(utils::RandomGenerator&rng) const override get_random_transition() Create a random transition for state. Declaration PottsTransition model::Potts::get_random_transition(const PottsState&state, utils::RandomGenerator&rng) const override apply_transition() Change state according to transition. Declaration void model::Potts::apply_transition(const PottsTransition&transition, PottsState&state) const override configure() Serialize q and the underlying graph. Declaration void model::Potts::configure(const utils::Json&json) override state_memory_estimate() Declaration size_t model::Potts::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::Potts::state_only_memory_estimate() const override calculate_term() Declaration double model::Potts::calculate_term(const PottsState&state, size_t edge_id) const calculate_term() Declaration double model::Potts::calculate_term(const PottsState&state, size_t edge_id, const PottsTransition&transition) const"
  },
  "api/model/pubo-adaptive.html": {
    "href": "api/model/pubo-adaptive.html",
    "title": "Class model::PuboAdaptive | qiotoolkit",
    "keywords": "Class model::PuboAdaptive Adaptive Pubo Model. This pubo implementation aims for adaptability to per-state memory constraints while using a cache-friendly graph representation. The numeric type used for indexing can be templated. This type needs to be chosen such that #variables < std::numeric_limits ::max() - 2. (the last two values of the range are used as sentinels in the graph representation). If the graph has high-locality terms, there should (ideally) be some unused range between #variables and that upper limit; this range is used to index into the cache. The number of terms for which caching can be used is limited by the width of that empty range. The graph representation uses a contigous chunk of memory initialized with an adjacency list for each node. For each term, the first 8 bytes contain the coefficient, followed by a sentinel terminated list of variable_ids (or cache_ids) participating in the term. The sentinel type (NEXT_TERM or NEXT_VAR) denotes when the last term of a variable is reached. During configuration of the graph representaion, the maximum number of bytes for each state can be specified. If this number exceeds the required bytes to store the boolean variables, the extra bytes are used to preferentially cache the number of zeros in high-locality terms (using either 32-bit or 8-bit counters). This has the effect of both reducing the graph size and time to compute a term. Inheritance markov::LinearSweepModel model::PuboAdaptive Inherited Members make_linear_sweep get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model state_only_memory_estimate get_term_count get_sweep_size init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate rescale render calculate_cost_difference get_random_transition get_const_cost configure ~BaseModel configure BaseModel render ~Component Component get_status param get_class_name Constructors PuboAdaptive() Create an unconfigured PuboAdaptive model. Declaration model::PuboAdaptive<Index>::PuboAdaptive() Methods get_identifier() Return the identifier of this model. Declaration std::string model::PuboAdaptive<Index>::get_identifier() const override get_version() Return the version of this model. Declaration std::string model::PuboAdaptive<Index>::get_version() const override match_version() Accept both version 1.0 and 1.1. Declaration void model::PuboAdaptive<Index>::match_version(const std::string&version) override configure() Configure the model from input. Declaration void model::PuboAdaptive<Index>::configure(const utils::Json&json) override configure() Declaration void model::PuboAdaptive<Index>::configure(Configuration_T&config) get_const_cost() Declaration double model::PuboAdaptive<Index>::get_const_cost() const override is_empty() Declaration bool model::PuboAdaptive<Index>::is_empty() const override configure() Declaration void model::PuboAdaptive<Index>::configure(model::Terms&&terms, size_t max_state_size_in_bytes) get_random_state() Create a random initial state. Declaration State_T model::PuboAdaptive<Index>::get_random_state(utils::RandomGenerator&rng) const override state_memory_estimate() Get memory estimation for state. Declaration size_t model::PuboAdaptive<Index>::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::PuboAdaptive<Index>::state_only_memory_estimate() const override get_random_transition() Create a random transition. Declaration Transition_T model::PuboAdaptive<Index>::get_random_transition(const State_T&, utils::RandomGenerator&rng) const override apply_transition() Applying a random transition is done by advancing the graph pointer to the offset corresponding to the variable being flipped. Declaration void model::PuboAdaptive<Index>::apply_transition(const Transition_T&transition, State_T&state) const override calculate_cost() Evaluate the whole cost function. Note The adjacency list representation is inefficient for this purpose, but we don't expect to evaluate this frequently. For non-cached terms, double counting is avoided by ensuring neighbor ids are larger than the variable we are processing For cached terms we use a std::vector to keep track which ones have been counted. Declaration double model::PuboAdaptive<Index>::calculate_cost(const State_T&state) const override calculate_cost_difference() The cost difference of an arbitrary transition is evaluated by setting up the state pointers and advancing the graph pointer to the offset corresponding to the variable being flipped. Declaration double model::PuboAdaptive<Index>::calculate_cost_difference(const State_T&state, const Transition_T&transition) const override make_linear_sweep() For a linear sweep, we can avoid the overhead of repeatedly setting up state pointers and jumping to specific offsets in the graph; it can simply be processed in order. Declaration void model::PuboAdaptive<Index>::make_linear_sweep(double&cost, State_T&state, std::function<bool(double)>accept, std::function<void(void)>check_lowest) const override render_state() Render the state as a structure mapping (original) names to pubo values. NOTE: the mapping is true=zero, false=1. Declaration utils::Structure model::PuboAdaptive<Index>::render_state(const State_T&state) const override get_sweep_size() One sweep consists of attempting to update each variable once. Declaration size_t model::PuboAdaptive<Index>::get_sweep_size() const override flip_spin() Flip a spin by applying xor with a shifted bit. Declaration void model::PuboAdaptive<Index>::flip_spin(uint64_t *spins, Index index) const get_spin_value() Extract a bit by masking with a shifted bit. Declaration bool model::PuboAdaptive<Index>::get_spin_value(const uint64_t *spins, Index index) const get_cache() Select the appropriate (type and index) of the cache for a given cache_id. NOTE: the cache_id=index-var_count_ is in [0..cache_count_). Declaration uint32_t model::PuboAdaptive<Index>::get_cache(size_t cache_id, const uint32_t *cache32, const uint8_t *cache8) const update_cache() Add the cache_change to the appropriate (type and index) of the cache. Declaration void model::PuboAdaptive<Index>::update_cache(size_t cache_id, uint32_t *cache32, uint8_t *cache8, uint8_t cache_change) const configure_state_size() Calculate how many terms we want to and can afford to cache. After a call to this method, state_size_ and the cache counters/offsets have been initialized to the correct values. Declaration void model::PuboAdaptive<Index>::configure_state_size(const model::Terms&terms, size_t max_state_size_in_bytes) configure_graph() Allocate and fill the adjacency list graph representation. Declaration void model::PuboAdaptive<Index>::configure_graph(const model::Terms&terms) calculate_term_difference() Declaration double model::PuboAdaptive<Index>::calculate_term_difference(uint8_t *&p, bool spin_value, const uint64_t *spins, const uint32_t *cache32, const uint8_t *cache8) const update_caches() Declaration void model::PuboAdaptive<Index>::update_caches(uint8_t *&p, bool spin, uint32_t *cache32, uint8_t *cache8) const take_index() Interpret the next sizeof(Index) bytes as an Index and advance the pointer. Declaration static Index model::PuboAdaptive<Index>::take_index(uint8_t *&p) take_double() Interpret the next 8 bytes as a double and advance the pointer. Declaration static double model::PuboAdaptive<Index>::take_double(uint8_t *&p) put_double() Declaration static void model::PuboAdaptive<Index>::put_double(uint8_t *&p, double d) put_index() Declaration static void model::PuboAdaptive<Index>::put_index(uint8_t *&p, Index index)"
  },
  "api/model/pubo-binary-adaptive.html": {
    "href": "api/model/pubo-binary-adaptive.html",
    "title": "Class model::PuboBinaryAdaptive | qiotoolkit",
    "keywords": "Class model::PuboBinaryAdaptive Adaptive Binary State. This adaptive state stores its state in a contigous (self allocated) chunk of heap memory (pointed to by data_). This chunk is organized as follows: +-----------------------------------------------------------------------+ | Binary values stored in uint64_t | 32-bit counters | 8-bit counters | +---------------------------------+-----+-----++++++ | 0101010100001010011111011110101101 | 256 | 42 | 3 | 0 | 7 | 2 | 1 | +---------------------------------+-----+-----++++++ The first section consists of as many uint64_t as needed to store the state of each variable in the problem Counters are used to cache the number of zeros in a given term. Their usage is allocated problem-dependent to the highest-locality terms and according to memory allowance passed during configuration. Note In addition to limiting per-state memory, PuboAdaptive can be templated for the index type (i.e. uint16_t). Because chache ids are appended to the variable ids; they must fit within the numeric range of the type used for indexing as well (see below). Inheritance markov::State model::PuboBinaryAdaptive Inherited Members configure render ~Component Component get_status param get_class_name Constructors PuboBinaryAdaptive() Create an empty adaptive state (used by std::vector) Declaration model::PuboBinaryAdaptive::PuboBinaryAdaptive() PuboBinaryAdaptive() Create an adaptive state of the desired size and initialize it to zero. Declaration model::PuboBinaryAdaptive::PuboBinaryAdaptive(size_t size) PuboBinaryAdaptive() Declaration model::PuboBinaryAdaptive::PuboBinaryAdaptive(const PuboBinaryAdaptive&other) PuboBinaryAdaptive() Declaration model::PuboBinaryAdaptive::PuboBinaryAdaptive(PuboBinaryAdaptive&&other) Methods operator=() Copy another state (allocating memory if it is empty). Declaration PuboBinaryAdaptive&model::PuboBinaryAdaptive::operator=(const PuboBinaryAdaptive&other) operator=() Move assignment can be done without copy. Declaration PuboBinaryAdaptive&model::PuboBinaryAdaptive::operator=(PuboBinaryAdaptive&&other) copy_state_only() We need to copy the whole state (including caches); the model cannot compute the cost function without them. This functionality is exclusively used to keep track of the best cost milestones I expect this to happen rarely enough that we can afford the overhead. Declaration void model::PuboBinaryAdaptive::copy_state_only(const PuboBinaryAdaptive&other) memory_estimate() Declaration static size_t model::PuboBinaryAdaptive::memory_estimate(size_t size) state_only_memory_estimate() Declaration static size_t model::PuboBinaryAdaptive::state_only_memory_estimate(size_t size)"
  },
  "api/model/pubo-compact-state.html": {
    "href": "api/model/pubo-compact-state.html",
    "title": "Class model::PuboCompactState | qiotoolkit",
    "keywords": "Class model::PuboCompactState PUBO model state for IsingCompact. Inheritance model::Binary model::PuboCompactState Inherited Members copy_state_only render Binary memory_estimate Binary render state_only_memory_estimate configure ~Component Component get_status param get_class_name Constructors PuboCompactState() Declaration model::PuboCompactState::PuboCompactState() PuboCompactState() Create an Ising state with N spins and M terms. All spins are initialized to 0 (\"True\"). A true value represents \"False\". As such, the boolean value can be considered as representing the sign. Declaration model::PuboCompactState::PuboCompactState(size_t N, size_t)"
  },
  "api/model/pubo-compact.html": {
    "href": "api/model/pubo-compact.html",
    "title": "Class model::PuboCompact | qiotoolkit",
    "keywords": "Class model::PuboCompact Inheritance model::GraphCompactModel model::PuboCompact Inherited Members Model state_only_memory_estimate get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff set_step_limit state_memory_estimate render calculate_cost_difference get_random_transition configure ~BaseModel configure BaseModel init get_const_cost get_sweep_size is_empty get_term_count GraphCompactModel has_initial_configuration is_rescaled rescale edge_count estimate_max_cost_diff edge get_scale_factor get_initial_configuration node_count ~GraphCompactModel get_benchmark_properties render ~Component Component get_status param get_class_name Constructors PuboCompact() Declaration model::PuboCompact<Element_T>::PuboCompact() Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::PuboCompact<Element_T>::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::PuboCompact<Element_T>::get_version() const override match_version() Checks the string argument against the expected version. This can be overloaded to accept other version than the one returned by get_version(). Declaration void model::PuboCompact<Element_T>::match_version(const std::string&version) override calculate_cost() Cost function: \\mathcal{H} = -\\sum_{j\\in\\mathrm{\\{edges\\}}} c_j\\prod_{i\\in j}s_i Declaration double model::PuboCompact<Element_T>::calculate_cost(const State_T&state) const override calculate_cost_difference() Cost difference: \\Delta_{i\\to i'} = 2 \\sum_{j\\in e_i} c_j\\prod_{i\\in j}s_i Declaration double model::PuboCompact<Element_T>::calculate_cost_difference(const State_T&state, const Transition_T&spin_id) const override get_random_state() Return a random Pubo state. The number of variables (terms) is determined from the number of nodes (edges) in the underlying graph. Declaration State_T model::PuboCompact<Element_T>::get_random_state(utils::RandomGenerator&rng) const override get_initial_configuration_state() Return a state created from initial configuration. Declaration State_T model::PuboCompact<Element_T>::get_initial_configuration_state() const override get_random_transition() Return a random Single spin update. We represent a single spin update as merely the index of the spin variable that will flip. Declaration Transition_T model::PuboCompact<Element_T>::get_random_transition(const State_T&, utils::RandomGenerator&rng) const override apply_transition() Flip the spin specified by spin_id. Declaration void model::PuboCompact<Element_T>::apply_transition(const Transition_T&transition, State_T&state) const override configure() Serialize metadata and the underlying graph. The underlying graph represents the \"disorder\" of the Ising model (glass) we are simulating. Declaration void model::PuboCompact<Element_T>::configure(const utils::Json&json) override configure() Declaration void model::PuboCompact<Element_T>::configure(typename GraphCompact::Configuration_T&configuration) render_state() Declaration utils::Structure model::PuboCompact<Element_T>::render_state(const State_T&state) const override state_memory_estimate() Declaration size_t model::PuboCompact<Element_T>::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::PuboCompact<Element_T>::state_only_memory_estimate() const override estimate_min_cost_diff() Declaration double model::PuboCompact<Element_T>::estimate_min_cost_diff() const override get_term() Declaration double model::PuboCompact<Element_T>::get_term(const State_T&state, size_t term_id) const"
  },
  "api/model/pubo-grouped.html": {
    "href": "api/model/pubo-grouped.html",
    "title": "Class model::PuboGrouped | qiotoolkit",
    "keywords": "Class model::PuboGrouped Inheritance model::FacedGraphModel model::PuboGrouped Inherited Members has_initial_configuration Model state_only_memory_estimate init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff set_step_limit state_memory_estimate render calculate_cost_difference get_random_transition configure ~BaseModel configure BaseModel estimate_max_cost_diff get_benchmark_properties node faces get_term_count get_sweep_size calculate_cost_difference edge_count nodes get_scale_factor get_const_cost is_empty edge face is_rescaled node_count collate populate_bbstrees edges rescale render ~Component Component get_status param get_class_name Constructors PuboGrouped() Declaration model::PuboGrouped<T>::PuboGrouped() Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::PuboGrouped<T>::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::PuboGrouped<T>::get_version() const override match_version() Checks the string argument against the expected version. This can be overloaded to accept other version than the one returned by get_version(). Declaration void model::PuboGrouped<T>::match_version(const std::string&version) override configure() Populates model internals from the input json. Declaration void model::PuboGrouped<T>::configure(const utils::Json&json) override configure() Declaration void model::PuboGrouped<T>::configure(typename Base_T::Configuration_T&configuration) get_random_state() Return a valid random state for the model. The various algorithms don't know how to initialize a valid state; this allows them to start with a random one. NOTE: The rng being passed should be used for randomness (as opposed to creating one for the model), as multiple threads can potentially use the same underlying model (albeit with separate rngs). Declaration State_T model::PuboGrouped<T>::get_random_state(utils::RandomGenerator&rng) const override get_random_transition() Declaration Transition_T model::PuboGrouped<T>::get_random_transition(const State_T&, utils::RandomGenerator&rng) const override render_state() Declaration utils::Structure model::PuboGrouped<T>::render_state(const State_T&state) const override calculate_cost() Declaration Cost_T model::PuboGrouped<T>::calculate_cost(const State_T&state) const calculate_cost_difference() Declaration Cost_T model::PuboGrouped<T>::calculate_cost_difference(const State_T&state, const Transition_T&transition, const Cost_T *cost) const apply_transition() Declaration void model::PuboGrouped<T>::apply_transition(const Transition_T&transition, State_T&state, Cost_T *cost) const apply_transition() Declaration void model::PuboGrouped<T>::apply_transition(const Transition_T&transition, State_T&state) const override state_memory_estimate() Declaration size_t model::PuboGrouped<T>::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::PuboGrouped<T>::state_only_memory_estimate() const override insert_val_to_bbstrees() Declaration virtual void model::PuboGrouped<T>::insert_val_to_bbstrees(double val, std::multiset<double>&tree_A, std::multiset<double>*tree_B) const override amend_collation() Declaration virtual void model::PuboGrouped<T>::amend_collation(std::map<int, double>&coeffs, int node_id) const override estimate_min_cost_diff() Declaration double model::PuboGrouped<T>::estimate_min_cost_diff() const"
  },
  "api/model/pubo-with-counter.html": {
    "href": "api/model/pubo-with-counter.html",
    "title": "Class model::PuboWithCounter | qiotoolkit",
    "keywords": "Class model::PuboWithCounter Inheritance model::AbstractPubo model::PuboWithCounter Inherited Members Model init render_state calculate_cost apply_transition set_step_limit render calculate_cost_difference get_random_transition configure state_memory_estimate state_only_memory_estimate estimate_min_cost_diff apply_transition configure render_state get_random_state get_identifier ~AbstractPubo edges match_version AbstractPubo nodes get_random_transition get_initial_configuration_state get_version get_version ~BaseModel configure BaseModel get_identifier edge get_const_cost node rescale node_count estimate_max_cost_diff get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled get_initial_configuration get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Methods configure() Populates model internals from the input json. Declaration void model::PuboWithCounter<T>::configure(const utils::Json&json) override configure() Declaration void model::PuboWithCounter<T>::configure(typename Base_T::Configuration_T&configuration) calculate_cost() Declaration double model::PuboWithCounter<T>::calculate_cost(const State_T&state) const override calculate_cost_difference() Declaration double model::PuboWithCounter<T>::calculate_cost_difference(const State_T&state, const Transition_T&transition) const override apply_transition() Declaration void model::PuboWithCounter<T>::apply_transition(const Transition_T&transition, State_T&state) const override"
  },
  "api/model/pubo.html": {
    "href": "api/model/pubo.html",
    "title": "Class model::Pubo | qiotoolkit",
    "keywords": "Class model::Pubo Inheritance model::AbstractPubo model::Pubo Inherited Members Model init render_state set_step_limit render get_random_transition configure state_memory_estimate state_only_memory_estimate estimate_min_cost_diff apply_transition configure render_state get_random_state get_identifier ~AbstractPubo edges configure match_version AbstractPubo nodes get_random_transition get_initial_configuration_state get_version get_version ~BaseModel configure BaseModel get_identifier edge get_const_cost node rescale node_count estimate_max_cost_diff get_term_count get_scale_factor is_empty has_initial_configuration is_rescaled get_initial_configuration configure get_sweep_size get_benchmark_properties edge_count render ~Component Component get_status param get_class_name Methods calculate_cost() Definition of the cost function. Evaluate the entire cost function for the state being passed. For instance, in the case of a model from statistical mechanics, this would be the Hamiltonian. Declaration double model::Pubo::calculate_cost(const State_T&state) const override calculate_cost_difference() Partial evaluation of the cost function. This method should calculate the difference in cost if we move from state (=before) to the one resulting from applying transition to state (=after): \\Delta_{C} = C_{\\mathrm{after}} - C_{\\mathrm{before}} In code: State state = get_random_state(rng); Transition transition = get_random_transition(rng); double cost_before = calculate_cost(state); double cost_diff = calculate_cost_difference(state, transition); apply_transition(transition, state); // modify state double cost_after = calculate_cost(state); // before + diff should correspond to after (up to double precision) assert(cost_before + cost_diff == cost_after); Declaration double model::Pubo::calculate_cost_difference(const State_T&state, const Transition_T&transition) const override apply_transition() Apply a transition to a state. This changes the configuration represented by *state. Depending on the optimization algorithm, transition can either be applied conditionally or alaways (e.g., population dynamics). Separating the functionality into the three interfaces random_transition, calculate_cost_difference, apply_transition leaves control over the strategy with the optimization method. Declaration void model::Pubo::apply_transition(const Transition_T&transition, State_T&state) const override"
  },
  "api/model/terms.html": {
    "href": "api/model/terms.html",
    "title": "Class model::Terms | qiotoolkit",
    "keywords": "Class model::Terms Inheritance utils::Component model::Terms Inherited Members render ~Component Component get_status param get_class_name Methods configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void model::Terms::configure(const utils::Json&json) override configure() Declaration void model::Terms::configure(Configuration_T&config) init() Declaration void model::Terms::init()"
  },
  "api/model/terms/stats.html": {
    "href": "api/model/terms/stats.html",
    "title": "Class model::Terms::Stats | qiotoolkit",
    "keywords": "Class model::Terms::Stats Inheritance model::Terms::Stats Constructors Stats() Declaration model::Terms::Stats::Stats() Methods clear() Declaration void model::Terms::Stats::clear()"
  },
  "api/model/terms/term.html": {
    "href": "api/model/terms/term.html",
    "title": "Class model::Terms::Term | qiotoolkit",
    "keywords": "Class model::Terms::Term Inheritance utils::Component model::Terms::Term Inherited Members ~Component Component get_status param Methods configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void model::Terms::Term::configure(const utils::Json&json) override <() Declaration bool model::Terms::Term::operator<(const Term&other) const <() Declaration bool model::Terms::Term::operator<(const Term&other) const render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure model::Terms::Term::render() const override get_class_name() get_class_name shows an identifier of the (derived) class name Its primary use is for type identification during stream output and logging, where a component is rendered as <ClassName: status>. Example: {c++} MyClass : public Component {} // This will render as '<MyClass>' MyClass my_object; std::cout << my_object << std::endl; Note You do not need to overload this method unless you want to change the output of the default implementation. Declaration std::string model::Terms::Term::get_class_name() const override"
  },
  "api/model/terms/term/get/cost.html": {
    "href": "api/model/terms/term/get/cost.html",
    "title": "Struct model::Terms::Term::Get_Cost | qiotoolkit",
    "keywords": "Struct model::Terms::Term::Get_Cost Methods get() Declaration static double&model::Terms::Term::Get_Cost::get(Term&term) get_key() Declaration static std::string model::Terms::Term::Get_Cost::get_key()"
  },
  "api/model/terms/term/get/variable/names.html": {
    "href": "api/model/terms/term/get/variable/names.html",
    "title": "Struct model::Terms::Term::Get_Variable_Names | qiotoolkit",
    "keywords": "Struct model::Terms::Term::Get_Variable_Names Methods get() Declaration static std::set<int>&model::Terms::Term::Get_Variable_Names::get(Term&term) get_key() Declaration static std::string model::Terms::Term::Get_Variable_Names::get_key()"
  },
  "api/model/terms/terms-configuration.html": {
    "href": "api/model/terms/terms-configuration.html",
    "title": "Class model::Terms::TermsConfiguration | qiotoolkit",
    "keywords": "Class model::Terms::TermsConfiguration Terms Configuration. This is a class for an intermidiate loading of data for Terms class, which will be configured form this class by moving its data. Model configuration is used due to using same data in tests. Inheritance model::BaseModelConfiguration model::Terms::TermsConfiguration"
  },
  "api/model/terms/terms-configuration/get/terms.html": {
    "href": "api/model/terms/terms-configuration/get/terms.html",
    "title": "Struct model::Terms::TermsConfiguration::Get_Terms | qiotoolkit",
    "keywords": "Struct model::Terms::TermsConfiguration::Get_Terms Methods get() Declaration static std::vector<Term>&model::Terms::TermsConfiguration::Get_Terms::get(TermsConfiguration&config) get_key() Declaration static std::string model::Terms::TermsConfiguration::Get_Terms::get_key()"
  },
  "api/model/terms/variable.html": {
    "href": "api/model/terms/variable.html",
    "title": "Class model::Terms::Variable | qiotoolkit",
    "keywords": "Class model::Terms::Variable Inheritance utils::Component model::Terms::Variable Inherited Members configure ~Component Component get_status param Methods <() Declaration bool model::Terms::Variable::operator<(const Variable&other) const <() Declaration bool model::Terms::Variable::operator<(const Variable&other) const render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure model::Terms::Variable::render() const override get_class_name() get_class_name shows an identifier of the (derived) class name Its primary use is for type identification during stream output and logging, where a component is rendered as <ClassName: status>. Example: {c++} MyClass : public Component {} // This will render as '<MyClass>' MyClass my_object; std::cout << my_object << std::endl; Note You do not need to overload this method unless you want to change the output of the default implementation. Declaration std::string model::Terms::Variable::get_class_name() const override"
  },
  "api/model/tsp-configuration.html": {
    "href": "api/model/tsp-configuration.html",
    "title": "Class model::TspConfiguration | qiotoolkit",
    "keywords": "Class model::TspConfiguration Inheritance model::BaseModelConfiguration model::TspConfiguration"
  },
  "api/model/tsp-configuration/get/dist.html": {
    "href": "api/model/tsp-configuration/get/dist.html",
    "title": "Struct model::TspConfiguration::Get_Dist | qiotoolkit",
    "keywords": "Struct model::TspConfiguration::Get_Dist Methods get() Declaration static std::vector<std::vector<double>>&model::TspConfiguration::Get_Dist::get(TspConfiguration&config) get_key() Declaration static std::string model::TspConfiguration::Get_Dist::get_key()"
  },
  "api/model/tsp-configuration/get/model.html": {
    "href": "api/model/tsp-configuration/get/model.html",
    "title": "Struct model::TspConfiguration::Get_Model | qiotoolkit",
    "keywords": "Struct model::TspConfiguration::Get_Model Methods get() Declaration static TspConfiguration&model::TspConfiguration::Get_Model::get(TspConfiguration&config) get_key() Declaration static std::string model::TspConfiguration::Get_Model::get_key()"
  },
  "api/model/tsp-transition.html": {
    "href": "api/model/tsp-transition.html",
    "title": "Class model::TspTransition | qiotoolkit",
    "keywords": "Class model::TspTransition Inheritance markov::Transition model::TspTransition Inherited Members configure render ~Component Component get_status param Constructors TspTransition() Declaration model::TspTransition::TspTransition() TspTransition() Declaration model::TspTransition::TspTransition(Type type, size_t a, size_t b) Methods apply() Declaration void model::TspTransition::apply(Permutation&permutation) const get_class_name() get_class_name shows an identifier of the (derived) class name Its primary use is for type identification during stream output and logging, where a component is rendered as <ClassName: status>. Example: {c++} MyClass : public Component {} // This will render as '<MyClass>' MyClass my_object; std::cout << my_object << std::endl; Note You do not need to overload this method unless you want to change the output of the default implementation. Declaration std::string model::TspTransition::get_class_name() const override type() Declaration Type model::TspTransition::type() const a() Declaration size_t model::TspTransition::a() const b() Declaration size_t model::TspTransition::b() const operator==() Declaration bool model::TspTransition::operator==(const TspTransition&trans) const random() Declaration TspTransition model::TspTransition::random(size_t N, utils::RandomGenerator&rng)"
  },
  "api/model/tsp.html": {
    "href": "api/model/tsp.html",
    "title": "Class model::Tsp | qiotoolkit",
    "keywords": "Class model::Tsp Inheritance markov::Model model::Tsp Inherited Members get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model state_only_memory_estimate get_term_count get_sweep_size init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate rescale render calculate_cost_difference get_random_transition get_const_cost configure match_version ~BaseModel configure BaseModel render ~Component Component get_status param Constructors Tsp() Declaration model::Tsp::Tsp() Methods get_identifier() Returns the identifier string of the model type (e.g., \"ising\"). Declaration std::string model::Tsp::get_identifier() const override get_version() Returns the version of the input format this implementation expects. Declaration std::string model::Tsp::get_version() const override calculate_cost() Declaration double model::Tsp::calculate_cost(const Permutation&p) const override calculate_cost_difference() Declaration double model::Tsp::calculate_cost_difference(const Permutation&p, const TspTransition&t) const override get_random_state() Return a valid random state for the model. The various algorithms don't know how to initialize a valid state; this allows them to start with a random one. NOTE: The rng being passed should be used for randomness (as opposed to creating one for the model), as multiple threads can potentially use the same underlying model (albeit with separate rngs). Declaration Permutation model::Tsp::get_random_state(utils::RandomGenerator&rng) const override get_random_transition() Declaration TspTransition model::Tsp::get_random_transition(const Permutation&p, utils::RandomGenerator&rng) const override apply_transition() Declaration void model::Tsp::apply_transition(const TspTransition&transition, Permutation&p) const override configure() Populates model internals from the input json. Declaration void model::Tsp::configure(const utils::Json&json) override configure() Declaration void model::Tsp::configure(Configuration_T&config) get_class_name() get_class_name shows an identifier of the (derived) class name Its primary use is for type identification during stream output and logging, where a component is rendered as <ClassName: status>. Example: {c++} MyClass : public Component {} // This will render as '<MyClass>' MyClass my_object; std::cout << my_object << std::endl; Note You do not need to overload this method unless you want to change the output of the default implementation. Declaration std::string model::Tsp::get_class_name() const override get_const_cost() Declaration double model::Tsp::get_const_cost() const override is_empty() Declaration bool model::Tsp::is_empty() const override state_memory_estimate() Declaration size_t model::Tsp::state_memory_estimate() const override state_only_memory_estimate() Declaration size_t model::Tsp::state_only_memory_estimate() const override"
  },
  "api/observe/average-and-count.html": {
    "href": "api/observe/average-and-count.html",
    "title": "Class observe::AverageAndCount | qiotoolkit",
    "keywords": "Class observe::AverageAndCount Inheritance observe::Average observe::AverageAndCount observe::MinMax Inherited Members record reset record record record record Average record ~Observable record record configure ~Component Component get_status param get_class_name Methods render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::AverageAndCount::render() const override"
  },
  "api/observe/average.html": {
    "href": "api/observe/average.html",
    "title": "Class observe::Average | qiotoolkit",
    "keywords": "Class observe::Average Average observable. Keeps track of the sum of values and total weight in order to produce the weighted average of recorded data points since the last reset. Inheritance observe::Observable observe::Average observe::AverageAndCount Inherited Members record ~Observable record record configure ~Component Component get_status param get_class_name Constructors Average() Declaration observe::Average::Average() Methods reset() Declaration void observe::Average::reset() override record() Declaration void observe::Average::record(double value, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::Average::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/average2.html": {
    "href": "api/observe/average2.html",
    "title": "Class observe::Average2 | qiotoolkit",
    "keywords": "Class observe::Average2 Average2 observable. Additionally keeps track of the sum of squares to produce the standard deviation of recorded data points. Inheritance observe::MinMax observe::Average2 observe::BinWidthDistribution observe::RangedDistribution Inherited Members Average MinMax record ~Observable record record configure ~Component Component get_status param get_class_name Constructors Average2() Declaration observe::Average2::Average2() Methods reset() Declaration void observe::Average2::reset() override record() Declaration void observe::Average2::record(double value, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::Average2::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/batched.html": {
    "href": "api/observe/batched.html",
    "title": "Class observe::Batched | qiotoolkit",
    "keywords": "Class observe::Batched Batched observable. This creates a \"batched\" version of any observable which records a timeline with batches of size batch_size. I.e., after a total weight of batch_size has been recorded, the observable is rendered to the timeline of batches and the observable itself is reset. Example: Batched(BinWidthDistribution(0.2), 1000) records the evolution of a histogram: 1000 values are recorded into each histogram the histogram is grouped into bins of width 0.2 after each 1000, the histogram is stored and a new one started. Inheritance observe::Observable observe::Batched Inherited Members record ~Observable record record configure ~Component Component get_status param get_class_name Constructors Batched() Declaration observe::Batched::Batched(std::unique_ptr<Observable>&&observable, double batch_size) Batched() Declaration observe::Batched::Batched(Observable *observable, double batch_size) Batched() Declaration observe::Batched::Batched(const Batched&)=delete Methods operator=() Declaration Batched&observe::Batched::operator=(const Batched&)=delete reset() Declaration void observe::Batched::reset() override record() Declaration void observe::Batched::record(double value, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::Batched::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/bin-width-distribution.html": {
    "href": "api/observe/bin-width-distribution.html",
    "title": "Class observe::BinWidthDistribution | qiotoolkit",
    "keywords": "Class observe::BinWidthDistribution BinWidthDistribution observable. In addition to the mean and standard deviation, keeps track of the full histogram of recorded values in bins of the specified bin_width. This histogram can become quite large if you specify a small bin width in relation to the range of recorded values. Note There is a bin centered around 0 and bins are directly adjacent to each other. Inheritance observe::Average2 observe::BinWidthDistribution Inherited Members Average2 Average MinMax record ~Observable record record configure ~Component Component get_status param get_class_name Constructors BinWidthDistribution() Declaration observe::BinWidthDistribution::BinWidthDistribution(double bin_width) Methods reset() Declaration void observe::BinWidthDistribution::reset() override record() Declaration void observe::BinWidthDistribution::record(double value, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::BinWidthDistribution::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/constant.html": {
    "href": "api/observe/constant.html",
    "title": "Class observe::Constant | qiotoolkit",
    "keywords": "Class observe::Constant Constant observable. Keeps track of the last value (the assumption is that this observable is used to indicate a constant associated with other observables, hence the name). It does not keep track of previously recorded values. Inheritance observe::Observable observe::Constant Inherited Members record ~Observable record record configure ~Component Component get_status param get_class_name Constructors Constant() Declaration observe::Constant::Constant() Methods reset() Declaration void observe::Constant::reset() override record() Declaration void observe::Constant::record(double value, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::Constant::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/index.html": {
    "href": "api/observe/index.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/observe/log-batched.html": {
    "href": "api/observe/log-batched.html",
    "title": "Class observe::LogBatched | qiotoolkit",
    "keywords": "Class observe::LogBatched LograithmicBatched. This creates a \"logarithmically batched\" version of any observable which records a timeline with batches doubling in size over time. I.e., after a total weight of 32 has been recorded, the observable is rendered to the timeline of batches and the observable itself is reset with a new batch size of 64. Example: LogBatched(BinWidthDistribution(0.2), 16) records the evolution of a histogram: 16 values are recorded into the first batch 32 values will be recorded into the next batch the histogram is grouped into bins of width 0.2 Inheritance observe::Observable observe::LogBatched Inherited Members record ~Observable record record configure ~Component Component get_status param get_class_name Constructors LogBatched() Declaration observe::LogBatched::LogBatched(std::unique_ptr<Observable>&&observable, double initial_batch_size=1) LogBatched() Declaration observe::LogBatched::LogBatched(const LogBatched&)=delete Methods operator=() Declaration LogBatched&observe::LogBatched::operator=(const LogBatched&)=delete reset() Declaration void observe::LogBatched::reset() override record() Declaration void observe::LogBatched::record(double value, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::LogBatched::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/milestone.html": {
    "href": "api/observe/milestone.html",
    "title": "Class observe::Milestone | qiotoolkit",
    "keywords": "Class observe::Milestone Record when new values are reached. This records a timeline of when a new value is recorded. If the recorded values are monotonically decreasing (or increasing), this translates to a timeline of when a new lowest (or highest) value is found. Inheritance observe::Observable observe::Milestone Inherited Members record ~Observable record record configure ~Component Component get_status param get_class_name Constructors Milestone() Declaration observe::Milestone::Milestone(const std::string&key_label=\"key\", const std::string&value_label=\"value\") Methods reset() Declaration void observe::Milestone::reset() override record() Declaration void observe::Milestone::record(double value_label, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::Milestone::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/min-max.html": {
    "href": "api/observe/min-max.html",
    "title": "Class observe::MinMax | qiotoolkit",
    "keywords": "Class observe::MinMax MinMax observable. In addition to the average value, keeps track of the smallest and largest value recorded. Inheritance observe::AverageAndCount observe::MinMax observe::Average2 Inherited Members Average record ~Observable record record configure ~Component Component get_status param get_class_name Constructors MinMax() Declaration observe::MinMax::MinMax() Methods reset() Declaration void observe::MinMax::reset() override record() Declaration void observe::MinMax::record(double value, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::MinMax::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/observable.html": {
    "href": "api/observe/observable.html",
    "title": "Class observe::Observable | qiotoolkit",
    "keywords": "Class observe::Observable Observable Interface. An observable can be reset and allows recoding of values with individual weights (the default weight being 1.0). Depending on the type of the observable, it will keep track of the last recorded value, the mean since the last reset or a full histogram of values (see implementations). Inheritance utils::Component observe::Observable observe::Average observe::Batched observe::Constant observe::LogBatched observe::Milestone observe::Windowed Inherited Members configure render ~Component Component get_status param get_class_name Methods ~Observable() Declaration virtual observe::Observable::~Observable() reset() Declaration virtual void observe::Observable::reset()=0 record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/observer.html": {
    "href": "api/observe/observer.html",
    "title": "Class observe::Observer | qiotoolkit",
    "keywords": "Class observe::Observer Inheritance utils::ComponentWithOutput observe::Observer solver::Solver Inherited Members set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors Observer() Declaration observe::Observer::Observer() Methods restart() Declaration void observe::Observer::restart(size_t step) is_watching() Declaration bool observe::Observer::is_watching(const std::string&identifier) const set_observable_label() Declaration void observe::Observer::set_observable_label(const std::string&identifier, const T&value) clear_observable_label() Declaration void observe::Observer::clear_observable_label(const std::string&identifier) scoped_observable_label() Declaration ScopedLabel observe::Observer::scoped_observable_label(const std::string&label, T value) observe() Declaration void observe::Observer::observe(const std::string&identifier, double value, double weight=1.0) configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void observe::Observer::configure(const utils::Json&json) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure observe::Observer::render() const override"
  },
  "api/observe/observer/factory.html": {
    "href": "api/observe/observer/factory.html",
    "title": "Class observe::Observer::Factory | qiotoolkit",
    "keywords": "Class observe::Observer::Factory Inheritance observe::Observer::Factory Inherited Members ~ObservableFactory Methods create_observable() Declaration virtual std::unique_ptr<Observable>observe::Observer::Factory<T>::create_observable() const"
  },
  "api/observe/observer/group.html": {
    "href": "api/observe/observer/group.html",
    "title": "Class observe::Observer::Group | qiotoolkit",
    "keywords": "Class observe::Observer::Group Inheritance utils::Component observe::Observer::Group Inherited Members configure ~Component Component get_status param get_class_name Methods select_subgroup() Declaration Observer::Group * observe::Observer::Group::select_subgroup(const Label&label) has_observable() Declaration bool observe::Observer::Group::has_observable(const std::string&identifier) add_observable() Declaration Observable&observe::Observer::Group::add_observable(const std::string&identifier, std::unique_ptr<Observable>&&observable) select_observable() Declaration Observable&observe::Observer::Group::select_observable(const std::string&identifier) render() Declaration utils::Structure observe::Observer::Group::render() const override"
  },
  "api/observe/observer/observable-factory.html": {
    "href": "api/observe/observer/observable-factory.html",
    "title": "Class observe::Observer::ObservableFactory | qiotoolkit",
    "keywords": "Class observe::Observer::ObservableFactory Inheritance observe::Observer::ObservableFactory Methods ~ObservableFactory() Declaration virtual observe::Observer::ObservableFactory::~ObservableFactory() create_observable() Declaration virtual std::unique_ptr<Observable>observe::Observer::ObservableFactory::create_observable() const =0"
  },
  "api/observe/observer/protocol.html": {
    "href": "api/observe/observer/protocol.html",
    "title": "Class observe::Observer::Protocol | qiotoolkit",
    "keywords": "Class observe::Observer::Protocol Inheritance utils::Component observe::Observer::Protocol Inherited Members render ~Component Component get_status param get_class_name Methods configure() Declaration void observe::Observer::Protocol::configure(const utils::Json&json) override is_watching() Declaration bool observe::Observer::Protocol::is_watching(const std::string&name) const select_group() Declaration Observer::Group * observe::Observer::Protocol::select_group(Group&groups, size_t restart_step, std::map<std::string, utils::Structure>labels_) const create_observable() Declaration std::unique_ptr<Observable>observe::Observer::Protocol::create_observable(const std::string&name) const"
  },
  "api/observe/observer/scoped-label.html": {
    "href": "api/observe/observer/scoped-label.html",
    "title": "Class observe::Observer::ScopedLabel | qiotoolkit",
    "keywords": "Class observe::Observer::ScopedLabel Inheritance observe::Observer::ScopedLabel Constructors ScopedLabel() Declaration observe::Observer::ScopedLabel::ScopedLabel(Observer *observer, const std::string&label) ScopedLabel() Declaration observe::Observer::ScopedLabel::ScopedLabel(const ScopedLabel&)=default Methods operator=() Declaration ScopedLabel&observe::Observer::ScopedLabel::operator=(const ScopedLabel&copy)=default ~ScopedLabel() Declaration virtual observe::Observer::ScopedLabel::~ScopedLabel()"
  },
  "api/observe/ranged-distribution.html": {
    "href": "api/observe/ranged-distribution.html",
    "title": "Class observe::RangedDistribution | qiotoolkit",
    "keywords": "Class observe::RangedDistribution RangedDistribution observable. In addition to the mean and standard deviation, keeps track of binned counts of the values within low and high (divided into bins bins). Values outside the specified range affect the mean and stddev, but are not recorded in the histogram. This observable is useful if you're only interested in the counts for a specific (known) value range. Inheritance observe::Average2 observe::RangedDistribution Inherited Members Average2 Average MinMax record ~Observable record record configure ~Component Component get_status param get_class_name Constructors RangedDistribution() Declaration observe::RangedDistribution::RangedDistribution(double low, double high, int bins) Methods reset() Declaration void observe::RangedDistribution::reset() override record() Declaration void observe::RangedDistribution::record(double value, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::RangedDistribution::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/windowed.html": {
    "href": "api/observe/windowed.html",
    "title": "Class observe::Windowed | qiotoolkit",
    "keywords": "Class observe::Windowed Windowed observable. This creates a \"windowed\" version of any observable such that the current state of the contained observable always reflects the last window_size values recorded. This is implemented by keeping track of the recorded values in the queue and removing them when the window_size is exceeded. Note The window_size is implemented as a weight; if all your recorded values have a weight of 1, this translates to window_size observables being tracked; if they differ it can be more or less, accordingly. Inheritance observe::Observable observe::Windowed Inherited Members record ~Observable record record configure ~Component Component get_status param get_class_name Constructors Windowed() Declaration observe::Windowed::Windowed(std::unique_ptr<Observable>&&observable, double window_size) Windowed() Declaration observe::Windowed::Windowed(const Windowed&)=delete Methods operator=() Declaration Windowed&observe::Windowed::operator=(const Windowed&)=delete reset() Declaration void observe::Windowed::reset() override record() Declaration void observe::Windowed::record(double value, double weight) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure observe::Windowed::render() const override record() Declaration void observe::Observable::record(double value) record() Declaration void observe::Observable::record(int value) record() Declaration void observe::Observable::record(size_t value) record() Declaration virtual void observe::Observable::record(double value, double weight)=0"
  },
  "api/observe/windowed/value-and-weight.html": {
    "href": "api/observe/windowed/value-and-weight.html",
    "title": "Class observe::Windowed::ValueAndWeight | qiotoolkit",
    "keywords": "Class observe::Windowed::ValueAndWeight Inheritance utils::Component observe::Windowed::ValueAndWeight Inherited Members configure ~Component Component get_status param get_class_name Constructors ValueAndWeight() Declaration observe::Windowed::ValueAndWeight::ValueAndWeight() ValueAndWeight() Declaration observe::Windowed::ValueAndWeight::ValueAndWeight(double value_, double weight_) Methods render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure observe::Windowed::ValueAndWeight::render() const override"
  },
  "api/optimal/learning/bounds-exception.html": {
    "href": "api/optimal/learning/bounds-exception.html",
    "title": "Class optimal_learning::BoundsException | qiotoolkit",
    "keywords": "Class optimal_learning::BoundsException \\rst Overview** Exception to capture value < min_value OR value > max_value. Stores value, min, and max for debugging/logging/reacting purposes. Message Format** The what() message is formatted in the class ctor (capitals indicate variable information):: R\"%%( BoundsException: VALUE is not in range [MIN, MAX]. CUSTOM_MESSAGE FUNCTION_NAME FILE_LINE_INFO )%%\" \\endrst Inheritance optimal_learning::OptimalLearningException optimal_learning::BoundsException optimal_learning::LowerBoundException optimal_learning::UpperBoundException Inherited Members OptimalLearningException OptimalLearningException what AppendCustomMessageAndDebugInfo OptimalLearningException Constructors BoundsException() \\rst Constructs a BoundsException object with extra fields to flesh out the what() message. :line_info[] ptr to char array containing FILE and LINE info; e.g., from OL_STRINGIFY_FILE_AND_LINE :func_info[]: optional ptr to char array from OL_CURRENT_FUNCTION_NAME or similar :custom_message[]: optional ptr to char array with any additional text/info to print/log :value: the value that violates its min or max bound :min: the minimum bound for value :max: the maximum bound for value \\endrst Declaration optimal_learning::BoundsException<ValueType>::BoundsException(char const *line_info, char const *func_info, char const *custom_message, ValueType value_in, ValueType min_in, ValueType max_in) BoundsException() Declaration optimal_learning::BoundsException<ValueType>::BoundsException(char const *name_in, char const *line_info, char const *func_info, char const *custom_message, ValueType value_in, ValueType min_in, ValueType max_in) Methods value() Declaration ValueType optimal_learning::BoundsException<ValueType>::value() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT max() Declaration ValueType optimal_learning::BoundsException<ValueType>::max() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT min() Declaration ValueType optimal_learning::BoundsException<ValueType>::min() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT OL_DISALLOW_DEFAULT_AND_ASSIGN() Declaration optimal_learning::BoundsException<ValueType>::OL_DISALLOW_DEFAULT_AND_ASSIGN(BoundsException)"
  },
  "api/optimal/learning/closed-interval.html": {
    "href": "api/optimal/learning/closed-interval.html",
    "title": "Struct optimal_learning::ClosedInterval | qiotoolkit",
    "keywords": "Struct optimal_learning::ClosedInterval \\rst Container to represent the mathematical notion of a closed interval, commonly written \\ms [a,b]\\me. The closed interval \\ms [a,b]\\me is the set of all numbers \\ms x \\in \\mathbb{R}\\me such that \\ms a \\leq x \\leq b\\me. Note that \"closed\" here indicates the interval includes both endpoints. An interval with \\ms a > b\\me is considered empty. This struct is \"trivial\" and \"standard layout\" and thus \"POD\" (in the C++11 sense). http://en.cppreference.com/w/cpp/types/is_pod http://stackoverflow.com/questions/4178175/what-are-aggregates-and-pods-and-how-why-are-they-special/7189821#7189821 This struct is not an aggregate; list (aka brace) initialization and a 2-argument constructor are both available:: ClosedInterval tmp(1.0, 2.0); // this ctor makes it non-aggregate ClosedInterval tmp{1.0, 2.0}; // and brace-style (aka initializer list) inits also work \\endrst Constructors ClosedInterval() \\rst Explicitly defaulted default constructor. Defining a custom ctor (below) disables the default ctor, so we explicitly default it. This is needed to maintain POD-ness. \\endrst Declaration optimal_learning::ClosedInterval::ClosedInterval()=default ClosedInterval() \\rst Constructs a ClosedInterval object with specified min, max. The presence of this ctor makes this object a non-aggregate, so brace-initialization follow list initialization rules (not aggregate initialization): http://en.cppreference.com/w/cpp/language/list_initialization :min_in left bound of the interval :max_in: right bound of the interval \\endrst Declaration optimal_learning::ClosedInterval::ClosedInterval(double min_in, double max_in) Methods IsInside() \\rst Check if a value is inside this ClosedInterval. :value the value to check true if min \\ms\\leq\\me value \\ms\\leq\\me max \\endrst Declaration bool optimal_learning::ClosedInterval::IsInside(double value) const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT Length() \\rst Compute the length of this ClosedInterval; result can be negative (i.e., an empty interval). length of the interval \\endrst Declaration double optimal_learning::ClosedInterval::Length() const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT IsEmpty() \\rst Checks whether the interval is \\ms\\emptyset\\me (i.e., max < min). Equivalent to Length() \\ms\\geq\\me 0.0. true if the interval is non-empty: max \\ms\\geq\\me min \\endrst Declaration bool optimal_learning::ClosedInterval::IsEmpty() const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT"
  },
  "api/optimal/learning/covariance-interface.html": {
    "href": "api/optimal/learning/covariance-interface.html",
    "title": "Class optimal_learning::CovarianceInterface | qiotoolkit",
    "keywords": "Class optimal_learning::CovarianceInterface \\rst Abstract class to enable evaluation of covariance functionssupports the evaluation of the covariance between two points, as well as the gradient with respect to those coordinates and gradient/hessian with respect to the hyperparameters of the covariance function. Covariance operaters, cov(x_1, x_2) are SPD. Due to the symmetry, there is no need to differentiate wrt x_1 and x_2; hence the gradient operation should only take gradients wrt dim variables, where dim = |x_1| Hyperparameters (denoted \\theta_j) are stored as class member data by subclasses. This class has only pure virtual functions, making it abstract. Users cannot instantiate this class directly. \\endrst Inheritance optimal_learning::CovarianceInterface optimal_learning::MaternNu2p5 optimal_learning::SquareExponential Methods ~CovarianceInterface() Declaration virtual optimal_learning::CovarianceInterface::~CovarianceInterface()=default Covariance() \\rst Computes the covariance function of the function values and their gradients of two points, cov(point_one, point_two). Points must be arrays with length dim. The covariance function is guaranteed to be symmetric by definition: Covariance(x, y) = Covariance(y, x). This function is also positive definite by definition. :point_one[dim] first spatial coordinate :derivatives_one[dim]: which derivatives of point_one are available :num_derivatives_one: int, the number of derivatives of point one :point_two[dim]: second spatial coordinate :derivatives_two[dim]: which derivatives of point_two are available :num_derivatives_two: int, the number of derivatives of point two cov[1+num_derivatives_one][1+num_derivatives_two]: value of covariance between the function values and their gradients of the input points \\endrst Declaration virtual void optimal_learning::CovarianceInterface::Covariance(double const *restrict point_one, int const *restrict derivatives_one, int num_derivatives_one, double const *restrict point_two, int const *restrict derivatives_two, int num_derivatives_two, double *restrict cov) const noexcept=0 GradCovariance() \\rst Computes the gradient of this.Covariance(point_one, point_two) with respect to the FIRST argument, point_one. This distinction is important for maintaining the desired symmetry. Cov(x, y) = Cov(y, x). Additionally, \\pderiv{Cov(x, y)}{x} = \\pderiv{Cov(y, x)}{x}. However, in general, \\pderiv{Cov(x, y)}{x} != \\pderiv{Cov(y, x)}{y} (NOT equal! These may differ by a negative sign) Hence to avoid separate implementations for differentiating against first vs second argument, this function only handles differentiation against the first argument. If you need \\pderiv{Cov(y, x)}{x}, just swap points x and y. :point_one[dim] first spatial coordinate :derivatives_one[dim]: which derivatives of point_one are available :num_derivatives_one: int, the number of derivatives of point one :point_two[dim]: second spatial coordinate :derivatives_two[dim]: which derivatives of point_two are available :num_derivatives_two: int, the number of derivatives of point two \\output grad_cov[dim][1+num_derivatives_one][1+num_derivatives_two]: (i, j, k)-th entry is \\pderiv{cov(x_1, x_2)(j, k))}{x1_i} \\endrst Declaration virtual void optimal_learning::CovarianceInterface::GradCovariance(double const *restrict point_one, int const *restrict derivatives_one, int num_derivatives_one, double const *restrict point_two, int const *restrict derivatives_two, int num_derivatives_two, double *restrict grad_cov) const noexcept OL_NONNULL_POINTERS=0 GetNumberOfHyperparameters() \\rst Returns the number of hyperparameters. This base class only allows for a maximum of dim + 1 hyperparameters but subclasses may implement additional ones. The number of hyperparameters. Return 0 to disable hyperparameter-related gradients, optimizations. \\endrst Declaration virtual int optimal_learning::CovarianceInterface::GetNumberOfHyperparameters() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT=0 HyperparameterGradCovariance() \\rst Similar to GradCovariance(), except gradients are computed w.r.t. the hyperparameters. Unlike GradCovariance(), the order of point_one and point_two is irrelevant here (since we are not differentiating against either of them). Thus the matrix of grad covariances (wrt hyperparameters) is symmetric. :point_one[dim] first spatial coordinate :derivatives_one[dim]: which derivatives of point_one are available :num_derivatives_one: int, the number of derivatives of point one :point_two[dim]: second spatial coordinate :derivatives_two[dim]: which derivatives of point_two are available :num_derivatives_two: int, the number of derivatives of point two \\output :grad_hyperparameter_cov[this.GetNumberOfHyperparameters()][1+num_derivatives_one][1+num_derivatives_two]: (i, j, k)-th entry is \\pderiv{cov(x_1, x_2)(j, k)}{\\theta_i} \\endrst Declaration virtual void optimal_learning::CovarianceInterface::HyperparameterGradCovariance(double const *restrict point_one, int const *restrict derivatives_one, int num_derivatives_one, double const *restrict point_two, int const *restrict derivatives_two, int num_derivatives_two, double *restrict grad_hyperparameter_cov) const noexcept OL_NONNULL_POINTERS=0 SetHyperparameters() \\rst Sets the hyperparameters. Hyperparameter ordering is defined implicitly by GetHyperparameters: [alpha=\\sigma_f^2, length_0, ..., length_{n-1}] :hyperparameters[this.GetNumberOfHyperparameters()] hyperparameters to set \\endrst Declaration virtual void optimal_learning::CovarianceInterface::SetHyperparameters(double const *restrict hyperparameters) noexcept OL_NONNULL_POINTERS=0 GetHyperparameters() \\rst Gets the hyperparameters. Ordering is [alpha=\\sigma_f^2, length_0, ..., length_{n-1}] \\output :hyperparameters[this.GetNumberOfHyperparameters()]: values of current hyperparameters \\endrst Declaration virtual void optimal_learning::CovarianceInterface::GetHyperparameters(double *restrict hyperparameters) const noexcept OL_NONNULL_POINTERS=0 Clone() \\rst For implementing the virtual (copy) constructor idiom. :Pointer to a constructed object that is a subclass of CovarianceInterface \\endrst Declaration virtual CovarianceInterface* optimal_learning::CovarianceInterface::Clone() const OL_WARN_UNUSED_RESULT=0"
  },
  "api/optimal/learning/dummy-domain.html": {
    "href": "api/optimal/learning/dummy-domain.html",
    "title": "Class optimal_learning::DummyDomain | qiotoolkit",
    "keywords": "Class optimal_learning::DummyDomain \\rst A dummy domain; commonly paired with the NullOptimizer. Use when domain is irrelevant. It does not track any member data and claims all points are inside. \\endrst Inheritance optimal_learning::DummyDomain Methods CheckPointInside() \\rst Always returns true: DummyDomain contains all points. :point[dim] point to check true if point is inside the domain or on its boundary, false otherwise \\endrst Declaration bool optimal_learning::DummyDomain::CheckPointInside(double const *restrict OL_UNUSED(point)) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT"
  },
  "api/optimal/learning/expected-improvement-evaluator.html": {
    "href": "api/optimal/learning/expected-improvement-evaluator.html",
    "title": "Class optimal_learning::ExpectedImprovementEvaluator | qiotoolkit",
    "keywords": "Class optimal_learning::ExpectedImprovementEvaluator \\rst A class to encapsulate the computation of expected improvement and its spatial gradient. This class handles the general EI computation case using monte carlo integration; it can support q,p-EI optimization. It is designed to work with any GaussianProcess. Additionally, this class has no state and within the context of EI optimization, it is meant to be accessed by const reference only. The random numbers needed for EI computation will be passed as parameters instead of contained as members to make multithreading more straightforward. \\endrst Inheritance optimal_learning::ExpectedImprovementEvaluator Constructors ExpectedImprovementEvaluator() \\rst Constructs a ExpectedImprovementEvaluator object. All inputs are required; no default constructor nor copy/assignment are allowed. :gaussian_process GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the underlying GP :num_mc_iterations: number of monte carlo iterations :best_so_far: best (minimum) objective function value (in points_sampled_value) \\endrst Declaration optimal_learning::ExpectedImprovementEvaluator::ExpectedImprovementEvaluator(const GaussianProcess&gaussian_process_in, int num_mc_iterations, double best_so_far) ExpectedImprovementEvaluator() Declaration optimal_learning::ExpectedImprovementEvaluator::ExpectedImprovementEvaluator(ExpectedImprovementEvaluator&&other) Methods dim() Declaration int optimal_learning::ExpectedImprovementEvaluator::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_mc_iterations() Declaration int optimal_learning::ExpectedImprovementEvaluator::num_mc_iterations() noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT best_so_far() Declaration double optimal_learning::ExpectedImprovementEvaluator::best_so_far() noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT gaussian_process() Declaration const GaussianProcess* optimal_learning::ExpectedImprovementEvaluator::gaussian_process() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT ComputeObjectiveFunction() \\rst Wrapper for ComputeExpectedImprovement(); see that function for details. \\endrst Declaration double optimal_learning::ExpectedImprovementEvaluator::ComputeObjectiveFunction(StateType *ei_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradObjectiveFunction() \\rst Wrapper for ComputeGradExpectedImprovement(); see that function for details. \\endrst Declaration void optimal_learning::ExpectedImprovementEvaluator::ComputeGradObjectiveFunction(StateType *ei_state, double *restrict grad_EI) const OL_NONNULL_POINTERS ComputeExpectedImprovement() \\rst Computes the expected improvement EI(Xs) = E_n[[f^*_n(X) - min(f(Xs_1),...,f(Xs_m))]^+], where Xs are potential points to sample (union of points_to_sample and points_being_sampled) and X are already sampled points. The ^+ indicates that the expression in the expectation evaluates to 0 if it is negative. f^*(X) is the MINIMUM over all known function evaluations (points_sampled_value), whereas f(Xs) are GP-predicted function evaluations. points_to_sample is the \"q\" and points_being_sampled is the \"p\" in q,p-EI. In words, we are computing the expected improvement (over the current best_so_far, best known objective function value) that would result from sampling (aka running new experiments) at points_to_sample with points_being_sampled concurrent/ongoing experiments. In general, the EI expression is complex and difficult to evaluate; hence we use Monte-Carlo simulation to approximate it. When faster (e.g., analytic) techniques are available, we will prefer them. The idea of the MC approach is to repeatedly sample at the union of points_to_sample and points_being_sampled. This is analogous to gaussian_process_interface.sample_point_from_gp, but we sample num_union points at once: y = \\mu + Lw where \\mu is the GP-mean, L is the chol_factor(GP-variance) and w is a vector of num_union draws from N(0, 1). Then: improvement_per_step = max(max(best_so_far - y), 0.0) Observe that the inner max means only the smallest component of y contributes in each iteration. We compute the improvement over many random draws and average. .. Note:: These comments were copied into ExpectedImprovementInterface.compute_expected_improvement() in interfaces/expected_improvement_interface.py. :ei_state[1] properly configured state object \\output :ei_state[1]: state with temporary storage modified; normal_rng modified the expected improvement from sampling points_to_sample with points_being_sampled concurrent experiments \\endrst \\rst Let Ls * Ls^T = Vars and w = vector of IID normal(0,1) variables Then: y = mus + Ls * w (Equation 4, from file docs) simulates drawing from our GP with mean mus and variance Vars. Then as given in the file docs, we compute the improvement: Then the improvement for this single sample is:: I = { best_known - min(y) if (best_known - min(y) > 0) (Equation 5 from file docs) { 0 else This is implemented as max_{y} (best_known - y). Notice that improvement takes the value 0 if it would be negative. Since we cannot compute min(y) directly, we do so via monte-carlo (MC) integration. That is, we draw from the GP repeatedly, computing improvement during each iteration, and averaging the result. See Scott's PhD thesis, sec 6.2. .. Note:: comments here are copied to _compute_expected_improvement_monte_carlo() in python_version/expected_improvement.py \\endrst Declaration double optimal_learning::ExpectedImprovementEvaluator::ComputeExpectedImprovement(StateType *ei_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradExpectedImprovement() \\rst Computes the (partial) derivatives of the expected improvement with respect to each point of points_to_sample. As with ComputeExpectedImprovement(), this computation accounts for the effect of points_being_sampled concurrent experiments. points_to_sample is the \"q\" and points_being_sampled is the \"p\" in q,p-EI.. In general, the expressions for gradients of EI are complex and difficult to evaluate; hence we use Monte-Carlo simulation to approximate it. When faster (e.g., analytic) techniques are available, we will prefer them. The MC computation of grad EI is similar to the computation of EI (decsribed in compute_expected_improvement). We differentiate y = \\mu + Lw wrt points_to_sample; only terms from the gradient of \\mu and L contribute. In EI, we computed: improvement_per_step = max(max(best_so_far - y), 0.0) and noted that only the smallest component of y may contribute (if it is > 0.0). Call this index winner. Thus in computing grad EI, we only add gradient terms that are attributable to the winner-th component of y. .. Note:: These comments were copied into ExpectedImprovementInterface.compute_expected_improvement() in interfaces/expected_improvement_interface.py. :ei_state[1] properly configured state object \\output :ei_state[1]: state with temporary storage modified; normal_rng modified :grad_EI[dim][num_to_sample]: gradient of EI, \\pderiv{EI(Xq \\cup Xp)}{Xq_{d,i}} where Xq is points_to_sample and Xp is points_being_sampled (grad EI from sampling points_to_sample with points_being_sampled concurrent experiments wrt each dimension of the points in points_to_sample) \\endrst \\rst Computes gradient of EI (see ExpectedImprovementEvaluator::ComputeGradExpectedImprovement) wrt points_to_sample (stored in union_of_points[0:num_to_sample]). Mechanism is similar to the computation of EI, where points' contributions to the gradient are thrown out of their corresponding improvement <= 0.0. Thus \\nabla(\\mu) only contributes when the winner (point w/best improvement this iteration) is the current point. That is, the gradient of \\mu at x_i wrt x_j is 0 unless i == j (and only this result is stored in ei_state->grad_mu). The interaction with ei_state->grad_chol_decomp is harder to know a priori (like with grad_mu) and has a more complex structure (rank 3 tensor), so the derivative wrt x_j is computed fully, and the relevant submatrix (indexed by the current winner) is accessed each iteration. .. Note:: comments here are copied to _compute_grad_expected_improvement_monte_carlo() in python_version/expected_improvement.py \\endrst Declaration void optimal_learning::ExpectedImprovementEvaluator::ComputeGradExpectedImprovement(StateType *ei_state, double *restrict grad_EI) const OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::ExpectedImprovementEvaluator::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(ExpectedImprovementEvaluator)"
  },
  "api/optimal/learning/expected-improvement-mcmcevaluator.html": {
    "href": "api/optimal/learning/expected-improvement-mcmcevaluator.html",
    "title": "Class optimal_learning::ExpectedImprovementMCMCEvaluator | qiotoolkit",
    "keywords": "Class optimal_learning::ExpectedImprovementMCMCEvaluator \\rst A class to encapsulate the computation of knowledge gradient and its spatial gradient. This class handles the general KG computation case using monte carlo integration; it can support q,p-KG optimization. It is designed to work with any GaussianProcess. Additionally, this class has no state and within the context of KG optimization, it is meant to be accessed by const reference only. The random numbers needed for KG computation will be passed as parameters instead of contained as members to make multithreading more straightforward. \\endrst Inheritance optimal_learning::ExpectedImprovementMCMCEvaluator Constructors ExpectedImprovementMCMCEvaluator() \\rst Constructs a KnowledgeGradientEvaluator object. All inputs are required; no default constructor nor copy/assignment are allowed. :gaussian_process GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the underlying GP :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts :num_mc_iterations: number of monte carlo iterations :best_so_far: best (minimum) objective function value (in points_sampled_value) \\endrst Declaration optimal_learning::ExpectedImprovementMCMCEvaluator::ExpectedImprovementMCMCEvaluator(const GaussianProcessMCMC&gaussian_process_mcmc, int num_mc_iterations, double const *best_so_far, std::vector<typename ExpectedImprovementState::EvaluatorType>*evaluator_vector) Methods dim() Declaration int optimal_learning::ExpectedImprovementMCMCEvaluator::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_mcmc() Declaration int optimal_learning::ExpectedImprovementMCMCEvaluator::num_mcmc() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT expected_improvement_evaluator_list() Declaration std::vector<ExpectedImprovementEvaluator>* optimal_learning::ExpectedImprovementMCMCEvaluator::expected_improvement_evaluator_list() const noexcept OL_WARN_UNUSED_RESULT best_so_far_list() Declaration std::vector<double>optimal_learning::ExpectedImprovementMCMCEvaluator::best_so_far_list(double const *best_so_far) const noexcept OL_WARN_UNUSED_RESULT ComputeObjectiveFunction() \\rst Wrapper for ComputeKnowledgeGradient(); see that function for details. \\endrst Declaration double optimal_learning::ExpectedImprovementMCMCEvaluator::ComputeObjectiveFunction(StateType *ei_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradObjectiveFunction() \\rst Wrapper for ComputeGradKnowledgeGradient(); see that function for details. \\endrst Declaration void optimal_learning::ExpectedImprovementMCMCEvaluator::ComputeGradObjectiveFunction(StateType *ei_state, double *restrict grad_EI) const OL_NONNULL_POINTERS ComputeExpectedImprovement() \\rst Computes the knowledge gradient :kg_state[1] properly configured state object \\output :kg_state[1]: state with temporary storage modified; normal_rng modified the knowledge gradient from sampling points_to_sample with points_being_sampled concurrent experiments \\endrst \\rst Compute Knowledge Gradient This version requires the discretization of A (the feasibe domain). The discretization usually is: some set + points previous sampled + points being sampled + points to sample \\endrst Declaration double optimal_learning::ExpectedImprovementMCMCEvaluator::ComputeExpectedImprovement(StateType *ei_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradExpectedImprovement() \\rst Computes the (partial) derivatives of the knowledge gradient with respect to each point of points_to_sample. As with ComputeKnowledgeGradient(), this computation accounts for the effect of points_being_sampled concurrent experiments. points_to_sample is the \"q\" and points_being_sampled is the \"p\" in q,p-KG. :kg_state[1] properly configured state object \\output :kg_state[1]: state with temporary storage modified; normal_rng modified :grad_KG[dim][num_to_sample]: gradient of KG, \\pderiv{KG(Xq \\cup Xp)}{Xq_{d,i}} where Xq is points_to_sample and Xp is points_being_sampled (grad KG from sampling points_to_sample with points_being_sampled concurrent experiments wrt each dimension of the points in points_to_sample) \\endrst \\rst Computes gradient of KG (see KnowledgeGradientEvaluator::ComputeGradKnowledgeGradient) wrt points_to_sample (stored in union_of_points[0:num_to_sample]). Mechanism is similar to the computation of KG, where points' contributions to the gradient are thrown out of their corresponding improvement <= 0.0. Thus \\nabla(\\mu) only contributes when the winner (point w/best improvement this iteration) is the current point. That is, the gradient of \\mu at x_i wrt x_j is 0 unless i == j (and only this result is stored in kg_state->grad_mu). The interaction with kg_state->grad_chol_decomp is harder to know a priori (like with grad_mu) and has a more complex structure (rank 3 tensor), so the derivative wrt x_j is computed fully, and the relevant submatrix (indexed by the current winner) is accessed each iteration. .. Note:: comments here are copied to _compute_grad_knowledge_gradient_monte_carlo() in python_version/knowledge_gradient.py \\endrst Declaration void optimal_learning::ExpectedImprovementMCMCEvaluator::ComputeGradExpectedImprovement(StateType *ei_state, double *restrict grad_EI) const OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::ExpectedImprovementMCMCEvaluator::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(ExpectedImprovementMCMCEvaluator)"
  },
  "api/optimal/learning/expected-improvement-mcmcstate.html": {
    "href": "api/optimal/learning/expected-improvement-mcmcstate.html",
    "title": "Struct optimal_learning::ExpectedImprovementMCMCState | qiotoolkit",
    "keywords": "Struct optimal_learning::ExpectedImprovementMCMCState \\rst State object for KnowledgeGradientEvaluator. This tracks the points being sampled in concurrent experiments (points_being_sampled) ALONG with the points currently being evaluated via knowledge gradient for future experiments (called points_to_sample); these are the p and q of q,p-KG, respectively. points_to_sample joined with points_being_sampled is stored in union_of_points in that order. This struct also tracks the state of the GaussianProcess that underlies the knowledge gradient computation: the GP state is built to handle the initial union_of_points, and subsequent updates to points_to_sample in this object also update the GP state. This struct also holds a pointer to a random number generator needed for Monte Carlo integrated KG computations. .. WARNING:: Users MUST guarantee that multiple state objects DO NOT point to the same RNG (in a multithreaded env). See general comments on State structs in gpp_common.hpp's header docs. \\endrst Constructors ExpectedImprovementMCMCState() \\rst Constructs an KnowledgeGradientMCMCState object with a specified source of randomness for the purpose of computing KG (and its gradient) over the specified set of points to sample. This establishes properly sized/initialized temporaries for KG computation, including dependent state from the associated Gaussian Process (which arrives as part of the kg_evaluator). .. WARNING:: This object is invalidated if the associated kg_evaluator is mutated. SetupState() should be called to reset. .. WARNING:: Using this object to compute gradients when configure_for_gradients := false results in UNDEFINED BEHAVIOR. :kg_evaluator knowledge gradient evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: points at which to evaluate KG and/or its gradient to check their value in future experiments (i.e., test points for GP predictions) :points_being_sampled[dim][num_being_sampled]: points being sampled in concurrent experiments :num_to_sample: number of potential future samples; gradients are evaluated wrt these points (i.e., the \"q\" in q,p-KG) :num_being_sampled: number of points being sampled in concurrent experiments (i.e., the \"p\" in q,p-KG) :configure_for_gradients: true if this object will be used to compute gradients, false otherwise :normal_rng[1]: pointer to a properly initialized* NormalRNG object .. NOTE:: The NormalRNG object must already be seeded. If multithreaded computation is used for KG, then every state object must have a different NormalRNG (different seeds, not just different objects). \\endrst Declaration optimal_learning::ExpectedImprovementMCMCState::ExpectedImprovementMCMCState(const EvaluatorType&ei_evaluator, double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample_in, int num_being_sampled_in, int const *restrict gradients_in, int num_gradients_in, bool configure_for_gradients, NormalRNGInterface *normal_rng_in, std::vector<typename ExpectedImprovementEvaluator::StateType>*ei_state_vector) ExpectedImprovementMCMCState() Declaration optimal_learning::ExpectedImprovementMCMCState::ExpectedImprovementMCMCState(ExpectedImprovementMCMCState&&other) Methods GetProblemSize() Declaration int optimal_learning::ExpectedImprovementMCMCState::GetProblemSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetCurrentPoint() \\rst Get the points_to_sample: potential future samples whose KG (and/or gradients) are being evaluated \\output :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::ExpectedImprovementMCMCState::GetCurrentPoint(double *restrict points_to_sample) const noexcept OL_NONNULL_POINTERS SetCurrentPoint() \\rst Change the potential samples whose KG (and/or gradient) are being evaluated. Update the state's derived quantities to be consistent with the new points. :kg_evaluator expected improvement evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::ExpectedImprovementMCMCState::SetCurrentPoint(const EvaluatorType&kg_evaluator, double const *restrict points_to_sample_in) OL_NONNULL_POINTERS SetupState() \\rst Configures this state object with new points_to_sample, the location of the potential samples whose KG is to be evaluated. Ensures all state variables & temporaries are properly sized. Properly sets all dependent state variables (e.g., GaussianProcess's state) for KG evaluation. .. WARNING:: This object's state is INVALIDATED if the kg_evaluator (including the GaussianProcess it depends on) used in SetupState is mutated! SetupState() should be called again in such a situation. :kg_evaluator knowledge gradient evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::ExpectedImprovementMCMCState::SetupState(const EvaluatorType&ei_evaluator, double const *restrict points_to_sample) OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::ExpectedImprovementMCMCState::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(ExpectedImprovementMCMCState) BuildUnionOfPoints() \\rst Create a vector with the union of points_to_sample and points_being_sampled (the latter is appended to the former). Note the l-value return. Assigning the return to a std::vector or passing it as an argument to the ctor will result in copy-elision or move semantics; no copying/performance loss. :points_to_sample[dim][num_to_sample]: points at which to evaluate KG and/or its gradient to check their value in future experiments (i.e., test points for GP predictions) :points_being_sampled[dim][num_being_sampled]: points being sampled in concurrent experiments :num_to_sample: number of potential future samples; gradients are evaluated wrt these points (i.e., the \"q\" in q,p-KG) :num_being_sampled: number of points being sampled in concurrent experiments (i.e., the \"p\" in q,p-KG) :dim: the number of spatial dimensions of each point array std::vector with the union of the input arrays: points_being_sampled is appended to points_to_sample \\endrst Declaration static std::vector<double>optimal_learning::ExpectedImprovementMCMCState::BuildUnionOfPoints(double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, int dim) noexcept OL_WARN_UNUSED_RESULT"
  },
  "api/optimal/learning/expected-improvement-state.html": {
    "href": "api/optimal/learning/expected-improvement-state.html",
    "title": "Struct optimal_learning::ExpectedImprovementState | qiotoolkit",
    "keywords": "Struct optimal_learning::ExpectedImprovementState \\rst State object for ExpectedImprovementEvaluator. This tracks the points being sampled in concurrent experiments (points_being_sampled) ALONG with the points currently being evaluated via expected improvement for future experiments (called points_to_sample); these are the p and q of q,p-EI, respectively. points_to_sample joined with points_being_sampled is stored in union_of_points in that order. This struct also tracks the state of the GaussianProcess that underlies the expected improvement computation: the GP state is built to handle the initial union_of_points, and subsequent updates to points_to_sample in this object also update the GP state. This struct also holds a pointer to a random number generator needed for Monte Carlo integrated EI computations. .. WARNING:: Users MUST guarantee that multiple state objects DO NOT point to the same RNG (in a multithreaded env). See general comments on State structs in gpp_common.hpp's header docs. \\endrst Constructors ExpectedImprovementState() \\rst Constructs an ExpectedImprovementState object with a specified source of randomness for the purpose of computing EI (and its gradient) over the specified set of points to sample. This establishes properly sized/initialized temporaries for EI computation, including dependent state from the associated Gaussian Process (which arrives as part of the ei_evaluator). .. WARNING:: This object is invalidated if the associated ei_evaluator is mutated. SetupState() should be called to reset. .. WARNING:: Using this object to compute gradients when configure_for_gradients := false results in UNDEFINED BEHAVIOR. :ei_evaluator expected improvement evaluator object that specifies the parameters & GP for EI evaluation :points_to_sample[dim][num_to_sample]: points at which to evaluate EI and/or its gradient to check their value in future experiments (i.e., test points for GP predictions) :points_being_sampled[dim][num_being_sampled]: points being sampled in concurrent experiments :num_to_sample: number of potential future samples; gradients are evaluated wrt these points (i.e., the \"q\" in q,p-EI) :num_being_sampled: number of points being sampled in concurrent experiments (i.e., the \"p\" in q,p-EI) :configure_for_gradients: true if this object will be used to compute gradients, false otherwise :normal_rng[1]: pointer to a properly initialized* NormalRNG object .. NOTE:: The NormalRNG object must already be seeded. If multithreaded computation is used for EI, then every state object must have a different NormalRNG (different seeds, not just different objects). \\endrst Declaration optimal_learning::ExpectedImprovementState::ExpectedImprovementState(const EvaluatorType&ei_evaluator, double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample_in, int num_being_sampled_in, bool configure_for_gradients, NormalRNGInterface *normal_rng_in) ExpectedImprovementState() Declaration optimal_learning::ExpectedImprovementState::ExpectedImprovementState(ExpectedImprovementState&&other) Methods GetProblemSize() Declaration int optimal_learning::ExpectedImprovementState::GetProblemSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetCurrentPoint() \\rst Get the points_to_sample: potential future samples whose EI (and/or gradients) are being evaluated \\output :points_to_sample[dim][num_to_sample]: potential future samples whose EI (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::ExpectedImprovementState::GetCurrentPoint(double *restrict points_to_sample) const noexcept OL_NONNULL_POINTERS SetCurrentPoint() \\rst Change the potential samples whose EI (and/or gradient) are being evaluated. Update the state's derived quantities to be consistent with the new points. :ei_evaluator expected improvement evaluator object that specifies the parameters & GP for EI evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose EI (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::ExpectedImprovementState::SetCurrentPoint(const EvaluatorType&ei_evaluator, double const *restrict points_to_sample) OL_NONNULL_POINTERS SetupState() \\rst Configures this state object with new points_to_sample, the location of the potential samples whose EI is to be evaluated. Ensures all state variables & temporaries are properly sized. Properly sets all dependent state variables (e.g., GaussianProcess's state) for EI evaluation. .. WARNING:: This object's state is INVALIDATED if the ei_evaluator (including the GaussianProcess it depends on) used in SetupState is mutated! SetupState() should be called again in such a situation. :ei_evaluator expected improvement evaluator object that specifies the parameters & GP for EI evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose EI (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::ExpectedImprovementState::SetupState(const EvaluatorType&ei_evaluator, double const *restrict points_to_sample) OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::ExpectedImprovementState::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(ExpectedImprovementState) BuildUnionOfPoints() \\rst Create a vector with the union of points_to_sample and points_being_sampled (the latter is appended to the former). Note the l-value return. Assigning the return to a std::vector or passing it as an argument to the ctor will result in copy-elision or move semantics; no copying/performance loss. :points_to_sample[dim][num_to_sample]: points at which to evaluate EI and/or its gradient to check their value in future experiments (i.e., test points for GP predictions) :points_being_sampled[dim][num_being_sampled]: points being sampled in concurrent experiments :num_to_sample: number of potential future samples; gradients are evaluated wrt these points (i.e., the \"q\" in q,p-EI) :num_being_sampled: number of points being sampled in concurrent experiments (i.e., the \"p\" in q,p-EI) :dim: the number of spatial dimensions of each point array std::vector with the union of the input arrays: points_being_sampled is appended to points_to_sample \\endrst Declaration static std::vector<double>optimal_learning::ExpectedImprovementState::BuildUnionOfPoints(double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, int dim) noexcept OL_WARN_UNUSED_RESULT"
  },
  "api/optimal/learning/gaussian-process-mcmc.html": {
    "href": "api/optimal/learning/gaussian-process-mcmc.html",
    "title": "Struct optimal_learning::GaussianProcessMCMC | qiotoolkit",
    "keywords": "Struct optimal_learning::GaussianProcessMCMC Constructors GaussianProcessMCMC() Declaration optimal_learning::GaussianProcessMCMC::GaussianProcessMCMC(double const *restrict hypers_mcmc, double const *restrict noises_mcmc, int num_mcmc, double const *restrict points_sampled_in, double const *restrict points_sampled_value_in, int const *restrict derivatives_in, int num_derivatives_in, int dim_in, int num_sampled_in) OL_NONNULL_POINTERS Methods num_mcmc() Declaration int optimal_learning::GaussianProcessMCMC::num_mcmc() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT dim() Declaration int optimal_learning::GaussianProcessMCMC::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_sampled() Declaration int optimal_learning::GaussianProcessMCMC::num_sampled() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_derivatives() Declaration int optimal_learning::GaussianProcessMCMC::num_derivatives() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT points_sampled() Declaration const std::vector<double>&optimal_learning::GaussianProcessMCMC::points_sampled() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT points_sampled_value() Declaration const std::vector<double>&optimal_learning::GaussianProcessMCMC::points_sampled_value() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT derivatives() Declaration const std::vector<int>&optimal_learning::GaussianProcessMCMC::derivatives() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT"
  },
  "api/optimal/learning/gaussian-process.html": {
    "href": "api/optimal/learning/gaussian-process.html",
    "title": "Class optimal_learning::GaussianProcess | qiotoolkit",
    "keywords": "Class optimal_learning::GaussianProcess \\rst Object that encapsulates Gaussian Process Priors (GPPs). A GPP is defined by a set of (sample point, function value, noise variance) triples along with a covariance function that relates the points. Each point has dimension dim. These are the training data; for example, each sample point might specify an experimental cohort and the corresponding function value is the objective measured for that experiment. There is one noise variance value per function value; this is the measurement error and is treated as N(0, noise_variance) Gaussian noise. GPPs estimate a real process \\ms f(x) = GP(m(x), k(x,x'))\\me (see file docs). This class deals with building an estimator to the actual process using measurements taken from the actual processthe (sample point, function val, noise) triple. Then predictions about unknown points can be made by sampling from the GPPin particular, finding the (predicted) mean and variance. These functions (and their gradients) are provided in ComputeMeanOfPoints, ComputeVarianceOfPoints, etc. Further mathematical details are given in the implementation comments, but we are essentially computing: | ComputeMeanOfPoints : K(Xs, X) * [K(X,X) + \\sigma_n^2 I]^{-1} * y | ComputeVarianceOfPoints: K(Xs, Xs) - K(Xs,X) * [K(X,X) + \\sigma_n^2 I]^{-1} * K(X,Xs) This (estimated) mean and variance characterize the predicted distributions of the actual \\ms m(x), k(x,x')\\me functions that underly our GP. .. Note:: the preceding comments are copied in Python: interfaces/gaussian_process_interface.py For testing and experimental purposes, this class provides a framework for sampling points from the GP (i.e., given a point to sample and predicted measurement noise) as well as adding additional points to an already-formed GP. Sampling points requires drawing from \\ms N(0,1)\\me so this class also holds PRNG state to do so via the NormalRNG object from gpp_random. .. NOTE:: Functions that manipulate the PRNG directly or indirectly (changing state, generating points) are NOT THREAD-SAFE. All thread-safe functions are marked const. These mean/variance methods require some external state: namely, the set of potential points to sample. Additionally, temporaries and derived quantities depending on these \"points to sample\" eliminate redundant computation. This external state is handled through PointsToSampleState objects, which are constructed separately and filled through PointsToSampleState::SetupState() which interacts with functions in this class. \\endrst Inheritance optimal_learning::GaussianProcess Constructors GaussianProcess() \\rst Constructs a GaussianProcess object. All inputs are required; no default constructor nor copy/assignment are allowed. .. Warning:: points_sampled is not allowed to contain duplicate points; doing so results in singular covariance matrices. :covariance the CovarianceFunction object encoding assumptions about the GP's behavior on our data :points_sampled[dim][num_sampled]: points that have already been sampled :points_sampled_value[num_sampled*(num_derivatives+1)]: values and derivatives of the already-sampled points, if only values are available, they should be used by setting points_sampled_value[i*(num_derivatives+1)] = value[i], when derivatives componens of points_sampled_value could be set to 0. :noise_variance[num_derivatives + 1]: the \\sigma_n^2 (noise variance) associated w/observation, points_sampled_value :dim: the spatial dimension of a point (i.e., number of independent params in experiment) :num_sampled: number of already-sampled points \\endrst Declaration optimal_learning::GaussianProcess::GaussianProcess(const CovarianceInterface&covariance_in, double const *restrict points_sampled_in, double const *restrict points_sampled_value_in, double const *restrict noise_variance_in, int const *restrict derivatives_in, int num_derivatives_in, int dim_in, int num_sampled_in) OL_NONNULL_POINTERS GaussianProcess() Declaration optimal_learning::GaussianProcess::GaussianProcess(const GaussianProcess&source) Methods dim() Declaration int optimal_learning::GaussianProcess::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_sampled() Declaration int optimal_learning::GaussianProcess::num_sampled() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_derivatives() Declaration int optimal_learning::GaussianProcess::num_derivatives() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT points_sampled() Declaration const std::vector<double>&optimal_learning::GaussianProcess::points_sampled() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT points_sampled_value() Declaration const std::vector<double>&optimal_learning::GaussianProcess::points_sampled_value() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT noise_variance() Declaration const std::vector<double>&optimal_learning::GaussianProcess::noise_variance() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT derivatives() Declaration const std::vector<int>&optimal_learning::GaussianProcess::derivatives() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT get_mean() Declaration double optimal_learning::GaussianProcess::get_mean() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT get_K_inv_y() Declaration const std::vector<double>&optimal_learning::GaussianProcess::get_K_inv_y() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT SetCovarianceHyperparameters() \\rst Change the hyperparameters of this GP's covariance function. Also forces recomputation of all derived quantities for GP to remain consistent. .. WARNING:: Using this function invalidates any PointsToSampleState objects created with \"this\" object. For any such objects \"state\", call state.SetupState(...) to restore them. :hyperparameters_new[covariance_ptr->GetNumberOfHyperparameters] new hyperparameter array \\endrst Declaration void optimal_learning::GaussianProcess::SetCovarianceHyperparameters(double const *restrict hyperparameters_new) OL_NONNULL_POINTERS FillPointsToSampleState() \\rst Sets up the PointsToSampleState object so that it can be used to compute GP mean, variance, and gradients thereof. ASSUMES all needed space is ALREADY ALLOCATED. This function should not be called directly; instead use PointsToSampleState::SetupState(). :points_to_sample_state[1] pointer to a PointsToSampleState object where all space has been properly allocated \\output :points_to_sample_state[1]: pointer to a fully configured PointsToSampleState object. overwrites input \\endrst \\rst Sets up precomputed quantities needed for mean, variance, and gradients thereof. These quantities are: Ks := Ks_{k,i} = cov(X_k, Xs_i) (used by mean, variance) Then if we need gradients: | K^-1 * Ks := solution X of K_{k,l} * X_{l,i} = Ks{k,i} (used by variance, grad variance) | gradient of Ks := C_{d,k,i} = \\pderiv{Ks_{k,i}}{Xs_{d,i}} (used by grad mean, grad variance) \\endrst Declaration void optimal_learning::GaussianProcess::FillPointsToSampleState(StateType *points_to_sample_state) const OL_NONNULL_POINTERS AddPointsToGP() \\rst Add the specified (point, fcn value, noise variance) historical data to this GP. Forces recomputation of all derived quantities for GP to remain consistent. :new_points[dim][num_new_points] coordinates of each new point to add :new_points_value[num_new_points]: function value at each new point :new_points_noise_variance[num_new_points]: \\sigma_n^2 corresponding to the signal noise in measuring new_points_value :num_new_points: number of new points to add to the GP \\endrst Declaration void optimal_learning::GaussianProcess::AddPointsToGP(double const *restrict new_points, double const *restrict new_points_value, int num_new_points, bool mean_change=true) AddSampledPointsToGP() Declaration void optimal_learning::GaussianProcess::AddSampledPointsToGP(double const *restrict new_points, double const *restrict new_points_value, int num_new_points) NewSampledValue() Declaration void optimal_learning::GaussianProcess::NewSampledValue(double const *restrict new_points_value, int num_new_points, int sampling_point_index, bool mean_change) SamplePointFromGP() \\rst Sample a function value from a Gaussian Process prior, provided a point at which to sample. Uses the formula function_value = gpp_mean + sqrt(gpp_variance) * w1 + sqrt(noise_variance) * w2, where w1, w2 are draws from \\ms N(0,1)\\me. .. NOTE:: Set noise_variance to 0 if you want \"accurate\" draws from the GP. BUT if the drawn (point, value) pair is meant to be added back into the GP (e.g., for testing), then this point MUST be drawn with noise_variance equal to the noise associated with \"point\" as a member of \"points_sampled\" :point_to_sample[dim] coordinates of the point at which to generate a function value (from GP) :noise_variance_this_point: if this point is to be added into the GP, it needs to be generated with its associated noise var function value drawn from this GP \\endrst \\rst Samples function values from a GPP given a list of points. Samples by: function_value = gpp_mean + gpp_variance * w, where w is a single draw from N(0,1). We only draw one point at a time (i.e., num_to_sample fixed at 1). We want multiple draws from the same GPP; drawing many points per step would be akin to sampling multiple GPPs. Thus gpp_mean, gpp_variance, and w all have size 1. If the GPP does not receive any data, then on the first step, gpp_mean = 0 and gpp_variance is just the \"covariance\" of a single point. Then we iterate through the remaining points in points_sampled, generating gpp_mean, gpp_variance, and a sample function value. \\endrst Declaration void optimal_learning::GaussianProcess::SamplePointFromGP(double const *restrict point_to_sample, double *results) noexcept OL_NONNULL_POINTERS SamplePointsFromGP() \\rst Sample only function values for a list of points \\endrst Declaration int optimal_learning::GaussianProcess::SamplePointsFromGP(double const *restrict points_to_sample, const int num_sample, double *results) noexcept OL_NONNULL_POINTERS SampleGlobalOptimaFromGP() \\rst Approximate the global optima of the GP. \\endrst Declaration void optimal_learning::GaussianProcess::SampleGlobalOptimaFromGP(int const num_optima, int const inner_number, const TensorProductDomain&domain, double *points_optima) noexcept OL_NONNULL_POINTERS ComputeMeanOfPoints() \\rst Computes the mean of this GP at each of Xs (points_to_sample). .. Note:: points_to_sample should not contain duplicate points. .. Note:: comments are copied in Python: interfaces/gaussian_process_interface.py :points_to_sample_state a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) \\output :mean_of_points[num_to_sample]: mean of GP, one per GP dimension \\endrst \\rst Calculates the mean (from the GPP) of a set of points: mus = Ks^T * K^-1 * y See Rasmussen and Willians page 19 alg 2.1 \\endrst Declaration void optimal_learning::GaussianProcess::ComputeMeanOfPoints(const StateType&points_to_sample_state, double *restrict mean_of_points) const noexcept OL_NONNULL_POINTERS ComputeMeanOfAdditionalPoints() \\rst :discrete_pts[dim][num_pts] the set of points to approximate the KG factor :num_pts: number of points in discrete_pts \\output :mean_of_points[num_pts]: mean of GP, one per GP dimension \\endrst \\rst Calculates the mean (from the GPP) of a set of points: mus = Ks^T * K^-1 * y See Rasmussen and Willians page 19 alg 2.1 \\endrst Declaration void optimal_learning::GaussianProcess::ComputeMeanOfAdditionalPoints(double const *discrete_pts, int num_pts, int const *gradients_discrete_pts, int num_gradients_discrete_pts, double *restrict mean_of_points) const noexcept ComputeGradMeanOfPoints() \\rst Computes the gradient of the mean of this GP at each of Xs (points_to_sample) wrt Xs. .. Note:: points_to_sample should not contain duplicate points. Note that grad_mu is nominally sized: grad_mu[dim][num_to_sample][num_to_sample]. However, for 0 <= i,j < num_to_sample, i != j, grad_mu[d][i][j] = 0. (See references or implementation for further details.) Thus, grad_mu is stored in a reduced form which only tracks the nonzero entries. .. Note:: comments are copied in Python: interfaces/gaussian_process_interface.py :points_to_sample_state a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) \\output :grad_mu[dim][state.num_derivatives]: gradient of the mean of the GP. grad_mu[d][i] is actually the gradient of \\mu_i with respect to x_{d,i}, the d-th dimension of the i-th entry of points_to_sample. \\endrst \\rst Gradient of the mean of a GP. Note that the output storage skips known zeros (see declaration docs for details). See Scott Clark's PhD thesis for more spelled out mathematical details, but this is a reasonably straightforward differentiation of: mus = Ks^T * K^-1 * y wrt Xs (so only Ks contributes derivative terms) \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradMeanOfPoints(const StateType&points_to_sample_state, double *restrict grad_mu) const noexcept OL_NONNULL_POINTERS ComputeGradMeanOfAdditionalPoints() Declaration void optimal_learning::GaussianProcess::ComputeGradMeanOfAdditionalPoints(double const *discrete_pts, int num_pts, int const *gradients_discrete_pts, int num_gradients_discrete_pts, double *restrict grad_mu) const noexcept ComputeVarianceOfPoints() \\rst Computes the variance (matrix) of this GP at each point of Xs (points_to_sample). The variance matrix is symmetric (in fact, SPD) and is stored in the LOWER TRIANGLE. .. Note:: points_to_sample should not contain duplicate points. .. Note:: comments are copied in Python: interfaces/gaussian_process_interface.py :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :var_star[num_to_sample][num_to_sample]: variance of GP evaluated at points_to_sample, LOWER TRIANGLE \\endrst \\rst Mathematically, we are computing Vars (Var_star), the GP variance. Vars is defined at the top of this file (Equation 3) and in Rasmussen & Williams, Equation 2.19: | L * L^T = K | V = L^-1 * Ks | Vars = Kss - (V^T * V) This quantity is: Kss: the covariance between test points based on the prior distribution minus V^T * V: the information observations give us about the objective function Notice that Vars is clearly symmetric. Kss is SPD. And V^T * V = (V^T * V)^T is symmetric (and is in fact SPD). V^T * V = Ks^T * K^-1 * K_s is SPD because: X^T * A * X is SPD when A is SPD AND X has full rank (X need not be square) Ks has full rank as long as K & Kss are SPD; K^-1 is SPD because K is SPD. It turns out that Vars is SPD. In Equation 1 (Rasmussen & Williams 2.18), it is clear that the combined covariance matrix is SPD (as long as no duplicate points and the covariance function is valid). A matrix of the form:: [ A B ] [ B^T C ] is SPD if and only if A is SPD AND (C - B^T * A^-1 * B) is SPD. Here, A = K, B = Ks, C = Kss. This (aka Schur Complement) can be shown readily:: [ A B ] = [ I 0 ] * [ A 0 ] * [ I A^-1 * B ] [ B^T C ] [ (A^-1 * B)^T I ] * [ 0 (C - B^T * A^-1 * B)] [ 0 I ] This factorization is valid because A is SPD (and thus invertible). Then by the X^T * A * X rule for SPD-ness, we know the block-diagonal matrix in the center is SPD. Hence the SPD-ness of V^T * V follows readily. For more information, see: http://en.wikipedia.org/wiki/Schur_complement [num_to_sample * num_gradients_to_sample] [num_to_sample*(the number of gradients in the bracket below)] \\endrst Declaration void optimal_learning::GaussianProcess::ComputeVarianceOfPoints(StateType *points_to_sample_state, int const *restrict gradients_to_sample_part2, int num_gradients_to_sample_part2, double *restrict var_star) const noexcept ComputeCovarianceOfPoints() \\rst Computes the covariance (matrix) of this GP at each point of Xs (points_to_sample) and each point of discrete points. .. Note:: points_to_sample should not contain duplicate points. .. Note:: comments are copied in Python: interfaces/gaussian_process_interface.py :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :var_star[num_to_sample][num_pts]: covariance of GP evaluated at points_to_sample and discrete_pts \\endrst \\rst Mathematically, we are computing Covars (Covar_star), the GP covariance. Vars is defined at the top of this file (Equation 3) and in Rasmussen & Williams, Equation 2.19: | L * L^T = K | V = L^-1 * Ks | W = L^-1 * Kt | Vars = Kst - (V^T * W) This quantity is: Kst: the covariance between two sets of test points based on the prior distribution minus V^T * W: the information observations give us about the objective function For more information, see: http://en.wikipedia.org/wiki/Schur_complement :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :var_star[num_to_sample][num_pts]: covariance of GP evaluated at points_to_sample and discrete_pts \\endrst Declaration void optimal_learning::GaussianProcess::ComputeCovarianceOfPoints(StateType *points_to_sample_state, double const *restrict discrete_pts, int num_pts, int const *restrict gradients_discrete_pts, int num_gradients_discrete_pts, bool precomputed, double const *ktd, double *restrict var_star) const noexcept ComputeTrain() \\rst Computes the covariance (matrix) of this GP at each point of Xs (points_to_sample) and each point of discrete points. .. Note:: points_to_sample should not contain duplicate points. .. Note:: comments are copied in Python: interfaces/gaussian_process_interface.py :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :var_star[num_to_sample][num_pts]: covariance of GP evaluated at points_to_sample and discrete_pts \\endrst \\rst Mathematically, we are computing Covars (Covar_star), the GP covariance. Vars is defined at the top of this file (Equation 3) and in Rasmussen & Williams, Equation 2.19: | L * L^T = K | V = L^-1 * Ks | W = L^-1 * Kt | Vars = Kst - (V^T * W) This quantity is: Kst: the covariance between two sets of test points based on the prior distribution minus V^T * W: the information observations give us about the objective function For more information, see: http://en.wikipedia.org/wiki/Schur_complement :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :var_star[num_to_sample][num_pts]: covariance of GP evaluated at points_to_sample and discrete_pts \\endrst Declaration void optimal_learning::GaussianProcess::ComputeTrain(double const *restrict discrete_pts, int num_pts, int const *restrict gradients_discrete_pts, int num_gradients_discrete_pts, double *restrict var_star) const noexcept OL_NONNULL_POINTERS ComputeGradVarianceOfPoints() \\rst Similar to ComputeGradCholeskyVarianceOfPoints() except this does not include the gradient terms from the cholesky factorization. Description will not be duplicated here. \\endrst \\rst This is just a thin wrapper that calls ComputeGradVarianceOfPointsPerPoint() in a loop num_derivatives times. See ComputeGradVarianceOfPointsPerPoint()'s function comments and implementation for more mathematical details on the derivation, algorithm, optimizations, etc. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradVarianceOfPoints(StateType *points_to_sample_state, double *restrict grad_var) const noexcept OL_NONNULL_POINTERS ComputeGradCovarianceOfPoints() \\rst :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :grad_var[dim][num_to_sample][num_pts][state->num_derivatives]: gradient of the variance of the GP. grad_var[d][i][j][k] is actually the gradients of var_{i,j} with respect to x_{d,k}, the d-th dimension of the k-th entry of points_to_sample \\endrst \\rst This is just a thin wrapper that calls ComputeGradCovarianceOfPointsPerPoint() in a loop num_derivatives times. See ComputeGradVarianceOfPointsPerPoint()'s function comments and implementation for more mathematical details on the derivation, algorithm, optimizations, etc. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradCovarianceOfPoints(StateType *points_to_sample_state, double const *restrict discrete_pts, int num_pts, int const *restrict gradients_discrete_pts, int num_gradients_discrete_pts, bool precomputed, double const *ktd, double *restrict grad_var) const noexcept ComputeGradCholeskyVarianceOfPoints() \\rst Computes the gradient of the cholesky factorization of the variance of this GP with respect to points_to_sample. This function accounts for the effect on the gradient resulting from cholesky-factoring the variance matrix. See Smith 1995 for algorithm details. points_to_sample is not allowed to contain duplicate points. Violating this results in a singular variance matrix. Note that grad_chol is nominally sized: grad_chol[dim][num_to_sample][num_to_sample][num_to_sample]. Let this be indexed grad_chol[d][i][j][k], which is read the derivative of var[i][j] with respect to x_{d,k} (x = points_to_sample) .. Note:: comments are copied in Python: interfaces/gaussian_process_interface.py :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :chol_var[num_to_sample][num_to_sample]: the variance (matrix) of this GP at each point of Xs (points_to_sample) e.g., from the cholesky factorization of ComputeVarianceOfPoints \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :grad_chol[dim][num_to_sample][num_to_sample][state->num_derivatives]: gradient of the cholesky-factored variance of the GP. grad_chol[d][i][j][k] is actually the gradients of var_{i,j} with respect to x_{d,k}, the d-th dimension of the k-th entry of points_to_sample store in UPPER triangle \\endrst \\rst This is just a thin wrapper that calls ComputeGradCholeskyVarianceOfPointsPerPoint() in a loop num_derivatives times. See ComputeGradCholeskyVarianceOfPointsPerPoint()'s function comments and implementation for more mathematical details on the algorithm. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradCholeskyVarianceOfPoints(StateType *points_to_sample_state, double const *restrict chol_var, double *restrict grad_chol) const noexcept OL_NONNULL_POINTERS ComputeGradInverseCholeskyVarianceOfPoints() \\rst Computes the gradient of the invers of the cholesky factorization of the variance of this GP with respect to points_to_sample. Note that grad_chol is nominally sized: grad_chol[dim][num_to_sample][num_to_sample][num_to_sample]. Let this be indexed grad_chol[d][i][j][k], which is read the derivative of var[i][j] with respect to x_{d,k} (x = points_to_sample) .. Note:: comments are copied in Python: interfaces/gaussian_process_interface.py :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :chol_var[num_to_sample][num_to_sample]: the variance (matrix) of this GP at each point of Xs (points_to_sample) e.g., from the cholesky factorization of ComputeVarianceOfPoints \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :grad_var[dim][num_to_sample][num_pts + num_to_sample][state->num_derivatives]: gradient of the invers of the cholesky-factored variance of the GP. grad_chol[d][i][j][k] is actually the gradients of var_{i,j} with respect to x_{d,k}, the d-th dimension of the k-th entry of points_to_sample \\endrst \\rst Compute the derivatives of the inverse of the cholesky factor wrt to the points to sample. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradInverseCholeskyVarianceOfPoints(StateType *points_to_sample_state, double const *restrict chol_var, double const *restrict var, double const *restrict cov, double const *restrict discrete_pts, int num_pts, bool precomputed, double const *ktd, double *restrict grad_chol) const noexcept ComputeGradInverseCholeskyCovarianceOfPoints() \\rst Computes the gradient of the invers of the cholesky factorization of the variance of this GP with respect to points_to_sample. Note that grad_chol is nominally sized: grad_chol[dim][num_to_sample][num_to_sample][num_to_sample]. Let this be indexed grad_chol[d][i][j][k], which is read the derivative of var[i][j] with respect to x_{d,k} (x = points_to_sample) .. Note:: comments are copied in Python: interfaces/gaussian_process_interface.py :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :chol_var[num_to_sample][num_to_sample]: the variance (matrix) of this GP at each point of Xs (points_to_sample) e.g., from the cholesky factorization of ComputeVarianceOfPoints \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :grad_var[dim][num_to_sample][num_pts][state->num_derivatives]: gradient of the invers of the cholesky-factored variance of the GP. grad_chol[d][i][j][k] is actually the gradients of var_{i,j} with respect to x_{d,k}, the d-th dimension of the k-th entry of points_to_sample \\endrst \\rst Compute the derivatives of the inverse of the cholesky factor wrt to the points to sample. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradInverseCholeskyCovarianceOfPoints(StateType *points_to_sample_state, double const *restrict chol_var, double const *restrict grad_chol, double const *restrict chol_inv_times_cov, double const *restrict discrete_pts, int num_pts, bool precomputed, double const *ktd, double *restrict grad_inverse_chol) const noexcept SetExplicitSeed() \\rst Seed the random number generator with the specified seed. See gpp_random, struct NormalRNG for details. :seed new seed to set \\endrst Declaration void optimal_learning::GaussianProcess::SetExplicitSeed(EngineType::result_type seed) noexcept SetRandomizedSeed() \\rst Seed the random number generator using a combination of the specified seed, current time, and potentially other factors. See gpp_random, struct NormalRNG for details. :seed base value for new seed \\endrst Declaration void optimal_learning::GaussianProcess::SetRandomizedSeed(EngineType::result_type seed) noexcept ResetToMostRecentSeed() \\rst Seeds the generator with its last used seed value. Useful for testinge.g., can conduct multiple runs with the same initial conditions \\endrst Declaration void optimal_learning::GaussianProcess::ResetToMostRecentSeed() noexcept Clone() \\rst Clones \"this\" GaussianProcess. Pointer to a constructed object that is a copy of \"this\" \\endrst Declaration GaussianProcess * optimal_learning::GaussianProcess::Clone() const OL_WARN_UNUSED_RESULT OL_DISALLOW_DEFAULT_AND_ASSIGN() Declaration optimal_learning::GaussianProcess::OL_DISALLOW_DEFAULT_AND_ASSIGN(GaussianProcess) BuildCovarianceMatrixWithNoiseVariance() Declaration void optimal_learning::GaussianProcess::BuildCovarianceMatrixWithNoiseVariance() noexcept BuildMixCovarianceMatrix() \\rst :cov_matrix[num_sampled][num_to_sample]: computed \"mix\" covariance matrix \\endrst Declaration void optimal_learning::GaussianProcess::BuildMixCovarianceMatrix(double const *restrict points_to_sample, int num_to_sample, int const *restrict derivatives_to_sample, int num_derivatives_to_sample, double *restrict cov_mat) const noexcept ComputeGradVarianceOfPointsPerPoint() \\rst Similar to ComputeGradCholeskyVarianceOfPointsPerPoint() except this does not include the gradient terms from the cholesky factorization. Description will not be duplicated here. \\endrst \\rst CORE IDEA** Similar to ComputeGradCholeskyVarianceOfPoints() below, except this function does not account for the cholesky decomposition. That is, it produces derivatives wrt Xs_{d,p} (points_to_sample) of: Vars = Kss - (V^T * V) = Kss - Ks^T * K^-1 * Ks (see ComputeVarianceOfPoints) .. NOTE:: normally Xs_p would be the p-th point of Xs (all dimensions); here Xs_{d,p} more explicitly refers to the d-th spatial dimension of the p-th point. This function only returns the derivative wrt a single choice of p, as specified by diff_index. Expanded index notation: Vars_{i,j} = Kss_{i,j} - Ks^T_{i,l} * K^-1_{l,k} * Ks_{k,j} Recall Ks_{k,i} = cov(X_k, Xs_i) = cov(Xs_i, X_k) where Xs is points_to_sample and X is points_sampled. (Note this is not equivalent to saying Ks = Ks^T, although this would be true if |Xs| == |X|.) As a result of this symmetry, \\pderiv{Ks_{k,i}}{Xs_{d,i}} = \\pderiv{Ks_{i,k}}{Xs_{d,i}} (that's d(cov(Xs_i, X_k))/d(Xs_i)) We are being more strict with index labels than is standard to clearly specify tensor dimensions. To be clear: i,j range over num_to_sample l,k are the only non-free indices; they range over num_sampled d,p describe the SPECIFIC point being differentiated against in Xs (points_to_sample): d over dimension, p* over num_to_sample NOTE: p is fixed! Unlike all other indices, p refers to a SPECIFIC point in the range [0, ..., num_to_sample-1]. Thus, \\pderiv{Ks_{k,i}}{Xs_{d,i}} is a 3-tensor (A_{d,k,i}) (repeated i is not summation since they denote components of a derivative) while \\pderiv{Ks_{i,l}}{Xs_{d,p}} is a 2-tensor (A_{d,l}) b/c only \\pderiv{Ks_{i=p,l}}{Xs_{d,p}} is nonzero, and {d,l} are the only remaining free indices. Then differentiating against Xs_{d,p} (recall that this is a specific point b/c p is fixed): | \\pderiv{Vars_{i,j}}{Xs_{d,p}} = \\pderiv{K_ss{i,j}}{Xs_{d,p}} - | (\\pderiv{Ks_{i,l}}{Xs_{d,p}} * K^-1_{l,k} * Ks_{k,j} + K_s{i,l} * K^-1_{l,k} * \\pderiv{Ks_{k,j}}{Xs_{d,p}}) Many of these terms are analytically known to be 0: \\pderiv{Ks_{i,l}}{Xs_{d,p}} = 0 when p != i (see NOTE above). A similar statement holds for the other gradient term. Observe that the second term in the parens, Ks_{i,l} * K^-1_{l,k} * \\pderiv{Ks_{k,j}}{Xs_{d,p}}, can be reordered to \"look\" like the first term. We use three symmetries: K^-1{l,k} = K^-1{k,l}, Ks_{i,l} = Ks_{l,i}, and \\pderiv{Ks_{k,j}}{Xs_{d,p}} = \\pderiv{Ks_{j,k}}{Xs_{d,p}} Then we can write: K_s{i,l} * K^-1_{l,k} * \\pderiv{Ks_{k,j}}{Xs_{d,p}} = \\pderiv{Ks_{j,k}}{Xs_{d,p}} * K^-1_{k,l} * K_s{l,i} Now left and right terms have the same index ordering (i,j match; k,l are not free and thus immaterial) The final result, accounting for analytic zeros is given here for convenience:: DVars_{d,i,j} \\equiv \\pderiv{Vars_{i,j}}{Xs_{d,p}} =`` { \\pderiv{K_ss{i,j}}{Xs_{d,p}} - 2\\pderiv{Ks_{i,l}}{Xs_{d,p}} * K^-1_{l,k} * Ks_{k,j} : WHEN p == i == j { \\pderiv{K_ss{i,j}}{Xs_{d,p}} - \\pderiv{Ks_{i,l}}{Xs_{d,p}} * K^-1_{l,k} * Ks_{k,j} : WHEN p == i != j { \\pderiv{K_ss{i,j}}{Xs_{d,p}} - \\pderiv{Ks_{j,k}}{Xs_{d,p}} * K^-1_{k,l} * Ks_{l,i} : WHEN p == j != i { 0 : otherwise The first item has a factor of 2 b/c it gets a contribution from both parts of the sum since p == i and p == j. The ordering DVars_{d,i,j} is significant: this is the ordering (d changes the fastest) in storage. OPTIMIZATIONS** Implementing this formula naively results in a large amount of redundant computation, so we now describe the optimizations present in our implementation. The first thing to notice is that the result, \\pderiv{Vars_{i,j}}{Xs_{d,p}}, has a lot of 0s. In particular, only the p-th block row and p-th block column have nonzero entries (blocks are size dim, indexed d). Currently, we will not be taking advantage of this sparsity because the consumer of DVars, ComputeGradCholeskyVarianceOfPoints(), is not implemented with sparsity in mind. Similarly, the next thing to notice is that if we ignore the case p == i == j, then we see that the expressions for p == i and p == j are actually identical (e.g., take the p == j case and exchange j = i and k = l). So think of DVars as a block matrix; each block has dimension entries, and the blocks are indexed over i (rows), j (cols). Then we see that the code is block-symmetric: DVars_{d,i,j} = Dvars_{d,j,i}. So we can compute it by filling in the p-th block column and then copy that data into the p-th block row. Additionally, the derivative terms represent matrix-matrix products: C_{l,j} = K^-1_{l,k} * Ks_{k,j} (and K^-1_{k,l} * Ks_{l,i}, which is just a change of index labels) is a matrix product. We compute this using back-substitutions to avoid explicitly forming K^-1. C_{l,j} is num_sampled X num_to_sample. Then D_{d,i=p,j} = \\pderiv{Ks_{i=p,l}}{Xs_{d,p}} * C_{l,j} is another matrix product (result size dim * num_to_sample) (i = p indicates that index i collapses out since this deriv term is zero if p != i). Note that we store \\pderiv{Ks_{i=p,l}}{Xs_{d,p}} = \\pderiv{Ks_{l,i=p}}{Xs_{d,p}} as A_{d,l,i} and grab the i = p-th block. Again, only the p-th point of points_to_sample is differentiated against; p specfied in diff_index. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradVarianceOfPointsPerPoint(StateType *points_to_sample_state, int diff_index, double *restrict grad_var) const noexcept OL_NONNULL_POINTERS ComputeGradCovarianceOfPointsPerPoint() \\rst :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts :diff_index: index of points_to_sample in {0, .. num_to_sample-1} to be differentiated against \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :grad_chol[dim][num_to_sample][num_pts]: gradient of the cholesky-factored variance of the GP. grad_chol[d][i][j] is actually the gradients of var_{i,j} with respect to x_{d,k}, the d-th dimension of the k-th entry of points_to_sample, where k = diff_index \\endrst \\rst CORE IDEA** Similar to ComputeGradCholeskyVarianceOfPoints() below, except this function does not account for the cholesky decomposition. That is, it produces derivatives wrt Xs_{d,p} (points_to_sample) of: Vars = Kss - (V^T * V) = Kss - Ks^T * K^-1 * Ks (see ComputeVarianceOfPoints) .. NOTE:: normally Xs_p would be the p-th point of Xs (all dimensions); here Xs_{d,p} more explicitly refers to the d-th spatial dimension of the p-th point. This function only returns the derivative wrt a single choice of p, as specified by diff_index. Expanded index notation: Vars_{i,j} = Kss_{i,j} - Ks^T_{i,l} * K^-1_{l,k} * Ks_{k,j} Recall Ks_{k,i} = cov(X_k, Xs_i) = cov(Xs_i, Xs_k) where Xs is points_to_sample and X is points_sampled. (Note this is not equivalent to saying Ks = Ks^T, although this would be true if |Xs| == |X|.) As a result of this symmetry, \\pderiv{Ks_{k,i}}{Xs_{d,i}} = \\pderiv{Ks_{i,k}}{Xs_{d,i}} (that's d(cov(Xs_i, X_k))/d(Xs_i)) We are being more strict with index labels than is standard to clearly specify tensor dimensions. To be clear: i,j range over num_to_sample l,k are the only non-free indices; they range over num_sampled d,p describe the SPECIFIC point being differentiated against in Xs (points_to_sample): d over dimension, p* over num_to_sample NOTE: p is fixed! Unlike all other indices, p refers to a SPECIFIC point in the range [0, ..., num_to_sample-1]. Thus, \\pderiv{Ks_{k,i}}{Xs_{d,i}} is a 3-tensor (A_{d,k,i}) (repeated i is not summation since they denote components of a derivative) while \\pderiv{Ks_{i,l}}{Xs_{d,p}} is a 2-tensor (A_{d,l}) b/c only \\pderiv{Ks_{i=p,l}}{Xs_{d,p}} is nonzero, and {d,l} are the only remaining free indices. Then differentiating against Xs_{d,p} (recall that this is a specific point b/c p is fixed): | \\pderiv{Vars_{i,j}}{Xs_{d,p}} = \\pderiv{K_ss{i,j}}{Xs_{d,p}} - | (\\pderiv{Ks_{i,l}}{Xs_{d,p}} * K^-1_{l,k} * Ks_{k,j} + K_s{i,l} * K^-1_{l,k} * \\pderiv{Ks_{k,j}}{Xs_{d,p}}) Many of these terms are analytically known to be 0: \\pderiv{Ks_{i,l}}{Xs_{d,p}} = 0 when p != i (see NOTE above). A similar statement holds for the other gradient term. Observe that the second term in the parens, Ks_{i,l} * K^-1_{l,k} * \\pderiv{Ks_{k,j}}{Xs_{d,p}}, can be reordered to \"look\" like the first term. We use three symmetries: K^-1{l,k} = K^-1{k,l}, Ks_{i,l} = Ks_{l,i}, and \\pderiv{Ks_{k,j}}{Xs_{d,p}} = \\pderiv{Ks_{j,k}}{Xs_{d,p}} Then we can write: K_s{i,l} * K^-1_{l,k} * \\pderiv{Ks_{k,j}}{Xs_{d,p}} = \\pderiv{Ks_{j,k}}{Xs_{d,p}} * K^-1_{k,l} * K_s{l,i} Now left and right terms have the same index ordering (i,j match; k,l are not free and thus immaterial) The final result, accounting for analytic zeros is given here for convenience:: DVars_{d,i,j} \\equiv \\pderiv{Vars_{i,j}}{Xs_{d,p}} =`` { \\pderiv{K_ss{i,j}}{Xs_{d,p}} - 2\\pderiv{Ks_{i,l}}{Xs_{d,p}} * K^-1_{l,k} * Ks_{k,j} : WHEN p == i == j { \\pderiv{K_ss{i,j}}{Xs_{d,p}} - \\pderiv{Ks_{i,l}}{Xs_{d,p}} * K^-1_{l,k} * Ks_{k,j} : WHEN p == i != j { \\pderiv{K_ss{i,j}}{Xs_{d,p}} - \\pderiv{Ks_{j,k}}{Xs_{d,p}} * K^-1_{k,l} * K_s{l,i} : WHEN p == j != i { 0 : otherwise The first item has a factor of 2 b/c it gets a contribution from both parts of the sum since p == i and p == j. The ordering DVars_{d,i,j} is significant: this is the ordering (d changes the fastest) in storage. OPTIMIZATIONS** Implementing this formula naively results in a large amount of redundant computation, so we now describe the optimizations present in our implementation. The first thing to notice is that the result, \\pderiv{Vars_{i,j}}{Xs_{d,p}}, has a lot of 0s. In particular, only the p-th block row and p-th block column have nonzero entries (blocks are size dim, indexed d). Currently, we will not be taking advantage of this sparsity because the consumer of DVars, ComputeGradCholeskyVarianceOfPoints(), is not implemented with sparsity in mind. Similarly, the next thing to notice is that if we ignore the case p == i == j, then we see that the expressions for p == i and p == j are actually identical (e.g., take the p == j case and exchange j = i and k = l). So think of DVars as a block matrix; each block has dimension entries, and the blocks are indexed over i (rows), j (cols). Then we see that the code is block-symmetric: DVars_{d,i,j} = Dvars_{d,j,i}. So we can compute it by filling in the p-th block column and then copy that data into the p-th block row. Additionally, the derivative terms represent matrix-matrix products: C_{l,j} = K^-1_{l,k} * Ks_{k,j} (and K^-1_{k,l} * Ks_{l,i}, which is just a change of index labels) is a matrix product. We compute this using back-substitutions to avoid explicitly forming K^-1. C_{l,j} is num_sampled X num_to_sample. Then D_{d,i=p,j} = \\pderiv{Ks_{i=p,l}}{Xs_{d,p}} * C_{l,j} is another matrix product (result size dim * num_to_sample) (i = p indicates that index i collapses out since this deriv term is zero if p != i). Note that we store \\pderiv{Ks_{i=p,l}}{Xs_{d,p}} = \\pderiv{Ks_{l,i=p}}{Xs_{d,p}} as A_{d,l,i} and grab the i = p-th block. Again, only the p-th point of points_to_sample is differentiated against; p specfied in diff_index. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradCovarianceOfPointsPerPoint(StateType *points_to_sample_state, int diff_index, double const *restrict discrete_pts, int num_pts, int const *restrict gradients_discrete_pts, int num_gradients_discrete_pts, bool precomputed, double const *kt, double *restrict grad_var) const noexcept ComputeGradCholeskyVarianceOfPointsPerPoint() \\rst Computes the gradient of the cholesky factorization of the variance of this GP with respect to the diff_index-th point in points_to_sample. This internal method is meant to be used by ComputeGradCholeskyVarianceOfPoints() to construct the gradient wrt all points of points_to_sample. See that function for more details. :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :diff_index: index of points_to_sample in {0, .. num_to_sample-1} to be differentiated against :chol_var[num_to_sample][num_to_sample]: the variance (matrix) of this GP at each point of Xs (points_to_sample) e.g., from the cholesky factorization of ComputeVarianceOfPoints \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :grad_chol[dim][num_to_sample][num_to_sample]: gradient of the inverse of the cholesky-factored variance of the GP. grad_chol[d][i][j] is actually the gradients of var_{i,j} with respect to x_{d,k}, the d-th dimension of the k-th entry of points_to_sample, where k = diff_index \\endrst \\rst Differentiates the cholesky factorization of the GP variance. | Vars = Kss - (V^T * V) (see ComputeVarianceOfPoints) | C * C^T = Vars This function differentiates C wrt the p-th point of points_to_sample; p specfied in diff_index Just as users of a lower triangular matrix L[i][j] should not access the upper triangle (j > i), users of the result of this function, grad_chol[d][i][j], should not access the upper block triangle with j > i. See Smith 1995 for full details of computing gradients of the cholesky factorization store in the UPPER triangle. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradCholeskyVarianceOfPointsPerPoint(StateType *points_to_sample_state, int diff_index, double const *restrict chol_var, double *restrict grad_chol) const noexcept OL_NONNULL_POINTERS ComputeGradInverseCholeskyVarianceOfPointsPerPoint() \\rst Computes the gradient of the invers of the cholesky factorization of the variance of this GP with respect to the diff_index-th point in points_to_sample. This internal method is meant to be used by ComputeGradInverseCholeskyVarianceOfPoints() to construct the gradient wrt all points of points_to_sample. See that function for more details. :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :diff_index: index of points_to_sample in {0, .. num_to_sample-1} to be differentiated against :chol_var[num_to_sample][num_to_sample]: the variance (matrix) of this GP at each point of Xs (points_to_sample) e.g., from the cholesky factorization of ComputeVarianceOfPoints \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :grad_chol[dim][num_to_sample][num_to_sample]: gradient of the cholesky-factored variance of the GP. grad_chol[d][i][j] is actually the gradients of var_{i,j} with respect to x_{d,k}, the d-th dimension of the k-th entry of points_to_sample, where k = diff_index \\endrst \\rst Compute the derivatives of the inverse of the cholesky factor wrt to the points to sample. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradInverseCholeskyVarianceOfPointsPerPoint(StateType *points_to_sample_state, int diff_index, double const *restrict chol_var, double const *restrict var, double const *restrict cov, double const *restrict discrete_pts, int num_pts, bool precomputed, double const *kt, double *restrict grad_chol) const noexcept OL_NONNULL_POINTERS ComputeGradInverseCholeskyCovarianceOfPointsPerPoint() \\rst Computes the gradient of the invers of the cholesky factorization of the variance of this GP with respect to the diff_index-th point in points_to_sample. This internal method is meant to be used by ComputeGradInverseCholeskyVarianceOfPoints() to construct the gradient wrt all points of points_to_sample. See that function for more details. :points_to_sample_state[1] ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState) :diff_index: index of points_to_sample in {0, .. num_to_sample-1} to be differentiated against :chol_var[num_to_sample][num_to_sample]: the variance (matrix) of this GP at each point of Xs (points_to_sample) e.g., from the cholesky factorization of ComputeVarianceOfPoints \\output :points_to_sample_state[1]: ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated :grad_chol[dim][num_to_sample][num_to_sample]: gradient of the cholesky-factored variance of the GP. grad_chol[d][i][j] is actually the gradients of var_{i,j} with respect to x_{d,k}, the d-th dimension of the k-th entry of points_to_sample, where k = diff_index \\endrst \\rst Compute the derivatives of the inverse of the cholesky factor wrt to the points to sample. \\endrst Declaration void optimal_learning::GaussianProcess::ComputeGradInverseCholeskyCovarianceOfPointsPerPoint(StateType *points_to_sample_state, int diff_index, double const *restrict chol_var, double const *restrict grad_chol_pt, double const *restrict chol_inv_times_cov, double const *restrict discrete_pts, int num_pts, bool precomputed, double const *kt, double *restrict grad_inverse_chol) const noexcept OL_NONNULL_POINTERS RecomputeDerivedVariables() \\rst Recomputes (including resizing as needed) the derived quantities in this class. This function should be called any time state variables are changed. \\endrst Declaration void optimal_learning::GaussianProcess::RecomputeDerivedVariables(bool mean_change=true) RecomputeCholeskyVariables() Declaration void optimal_learning::GaussianProcess::RecomputeCholeskyVariables() RecomputeMeanVariables() Declaration void optimal_learning::GaussianProcess::RecomputeMeanVariables(bool mean_change=true)"
  },
  "api/optimal/learning/gradient-descent-optimizer-line-search.html": {
    "href": "api/optimal/learning/gradient-descent-optimizer-line-search.html",
    "title": "Class optimal_learning::GradientDescentOptimizerLineSearch | qiotoolkit",
    "keywords": "Class optimal_learning::GradientDescentOptimizerLineSearch \\rst Gradient descent (GD) optimization. This class optimizes using restarted GD (see comments on the Optimize()) function. \\endrst Inheritance optimal_learning::GradientDescentOptimizerLineSearch Constructors GradientDescentOptimizerLineSearch() Declaration optimal_learning::GradientDescentOptimizerLineSearch<ObjectiveFunctionEvaluator_, DomainType_>::GradientDescentOptimizerLineSearch()=default Methods Optimize() \\rst Optimize a given objective function (represented by ObjectiveFunctionEvaluator; see file comments for what this must provide) using restarted gradient descent (GD). See section 2a) and 3b, i) in the header docs and the docs for GradientDescentOptimization() for more details. Guaranteed to call GradientDescentOptimization() AT MOST max_num_restarts times. GradientDescentOptimization() implements gradient descent; see function comments above for details. This method calls gradient descent, then restarts (by calling GD again) from the GD's result point. This is done until max_num_restarts is reached or the result point stops changing (compared to tolerance). Note that we are using an absolute tolerance, based on the size of the most recent step*. Here, 'step' is the distance covered by the last restart, not the last GD iteration (as in GradientDescentOptimization()). The suggested value is 1.0e-7, although this may need to be loosened for problems with 'difficult' optima (e.g., the shape is not locally very peaked). Setting too high of a tolerance can cause wrong answerse.g., we stop at a point that is not an optima but simply an region with small gradient. Setting the tolerance too low may make convergence impossible; GD could get stuck (bouncing between the same few points) or numerical effects could make it impossible to satisfy tolerance. As opposed to say based on changes in the objective function. Solution is guaranteed to lie within the region specified by \"domain\"; note that this may not be a true optima (i.e., the gradient may be substantially nonzero). problem_size refers to objective_state->GetProblemSize(), the number of dimensions in a \"point\" aka the number of variables being optimized. (This might be the spatial dimension for EI or the number of hyperparameters for log likelihood.) :objective_evaluator reference to object that can compute the objective function and its gradient :gd_parameters: GradientDescentParameters object that describes the parameters controlling gradient descent optimization (e.g., number of iterations, tolerances, learning rate) :domain: object specifying the domain to optimize over (see gpp_domain.hpp) :objective_state[1]: a properly configured state object for the ObjectiveFunctionEvaluator template parameter objective_state.GetCurrentPoint() will be used to obtain the initial guess \\output :objective_state[1]: a state object whose temporary data members may have been modified objective_state.GetCurrentPoint() will return the point yielding the best objective function value according to gradient descent number of errors, always 0 \\endrst Declaration int optimal_learning::GradientDescentOptimizerLineSearch<ObjectiveFunctionEvaluator_, DomainType_>::Optimize(const ObjectiveFunctionEvaluator&objective_evaluator, const ParameterStruct&gd_parameters, const DomainType&domain, typename ObjectiveFunctionEvaluator::StateType *objective_state) const OL_NONNULL_POINTERS OL_DISALLOW_COPY_AND_ASSIGN() Declaration optimal_learning::GradientDescentOptimizerLineSearch<ObjectiveFunctionEvaluator_, DomainType_>::OL_DISALLOW_COPY_AND_ASSIGN(GradientDescentOptimizerLineSearch)"
  },
  "api/optimal/learning/gradient-descent-optimizer.html": {
    "href": "api/optimal/learning/gradient-descent-optimizer.html",
    "title": "Class optimal_learning::GradientDescentOptimizer | qiotoolkit",
    "keywords": "Class optimal_learning::GradientDescentOptimizer \\rst Gradient descent (GD) optimization. This class optimizes using restarted GD (see comments on the Optimize()) function. \\endrst Inheritance optimal_learning::GradientDescentOptimizer Constructors GradientDescentOptimizer() Declaration optimal_learning::GradientDescentOptimizer<ObjectiveFunctionEvaluator_, DomainType_>::GradientDescentOptimizer()=default Methods Optimize() \\rst Optimize a given objective function (represented by ObjectiveFunctionEvaluator; see file comments for what this must provide) using restarted gradient descent (GD). See section 2a) and 3b, i) in the header docs and the docs for GradientDescentOptimization() for more details. Guaranteed to call GradientDescentOptimization() AT MOST max_num_restarts times. GradientDescentOptimization() implements gradient descent; see function comments above for details. This method calls gradient descent, then restarts (by calling GD again) from the GD's result point. This is done until max_num_restarts is reached or the result point stops changing (compared to tolerance). Note that we are using an absolute tolerance, based on the size of the most recent step*. Here, 'step' is the distance covered by the last restart, not the last GD iteration (as in GradientDescentOptimization()). The suggested value is 1.0e-7, although this may need to be loosened for problems with 'difficult' optima (e.g., the shape is not locally very peaked). Setting too high of a tolerance can cause wrong answerse.g., we stop at a point that is not an optima but simply an region with small gradient. Setting the tolerance too low may make convergence impossible; GD could get stuck (bouncing between the same few points) or numerical effects could make it impossible to satisfy tolerance. As opposed to say based on changes in the objective function. Solution is guaranteed to lie within the region specified by \"domain\"; note that this may not be a true optima (i.e., the gradient may be substantially nonzero). problem_size refers to objective_state->GetProblemSize(), the number of dimensions in a \"point\" aka the number of variables being optimized. (This might be the spatial dimension for EI or the number of hyperparameters for log likelihood.) :objective_evaluator reference to object that can compute the objective function and its gradient :gd_parameters: GradientDescentParameters object that describes the parameters controlling gradient descent optimization (e.g., number of iterations, tolerances, learning rate) :domain: object specifying the domain to optimize over (see gpp_domain.hpp) :objective_state[1]: a properly configured state object for the ObjectiveFunctionEvaluator template parameter objective_state.GetCurrentPoint() will be used to obtain the initial guess \\output :objective_state[1]: a state object whose temporary data members may have been modified objective_state.GetCurrentPoint() will return the point yielding the best objective function value according to gradient descent number of errors, always 0 \\endrst Declaration int optimal_learning::GradientDescentOptimizer<ObjectiveFunctionEvaluator_, DomainType_>::Optimize(const ObjectiveFunctionEvaluator&objective_evaluator, const ParameterStruct&gd_parameters, const DomainType&domain, typename ObjectiveFunctionEvaluator::StateType *objective_state) const OL_NONNULL_POINTERS OL_DISALLOW_COPY_AND_ASSIGN() Declaration optimal_learning::GradientDescentOptimizer<ObjectiveFunctionEvaluator_, DomainType_>::OL_DISALLOW_COPY_AND_ASSIGN(GradientDescentOptimizer)"
  },
  "api/optimal/learning/gradient-descent-parameters.html": {
    "href": "api/optimal/learning/gradient-descent-parameters.html",
    "title": "Struct optimal_learning::GradientDescentParameters | qiotoolkit",
    "keywords": "Struct optimal_learning::GradientDescentParameters \\rst Container to hold parameters that specify the behavior of Gradient Descent. .. Note:: these comments are copied in build_gradient_descent_parameters() in cpp_wrappers/optimizer_parameters.py. That function wraps this struct's ctor. Iterations** The total number of gradient descent steps is at most num_multistarts * max_num_steps * max_num_restarts Generally, allowing more iterations leads to a better solution but costs more time. Averaging (TODO(GH-390): NOT IMPLEMTED YET)** When optimizing stochastic objective functions, it can often be beneficial to average some number of gradient descent steps to obtain the final result (vs just returning the last step). Polyak-Ruppert averaging: postprocessing step where we replace x_n with: \\overbar{x} = \\frac{1}{n - n_0} \\sum_{t=n_0 + 1}^n x_t n_0 = 0 averages all steps; n_0 = n - 1 is equivalent to returning x_n directly. Here, num_steps_averaged is n - n_0. num_steps_averaged < 0: averages all steps num_steps_averaged == 0: do not average num_steps_averaged > 0 and <= max_num_steps: average the specified number of steps max_steps_averaged > max_num_steps: average all steps Learning Rate** GD may be implemented using a learning rate: pre_mult * (i+1)^{-\\gamma}, where i is the current iteration Larger gamma causes the GD step size to (artificially) scale down faster. Smaller pre_mult (artificially) shrinks the GD step size. Generally, taking a very large number of small steps leads to the most robustness; but it is very slow. Tolerances** Larger relative changes are potentially less robust but lead to faster convergence. Large tolerances run faster but may lead to high errors or false convergence (e.g., if the tolerance is 1.0e-3 and the learning rate control forces steps to fall below 1.0e-3 quickly, then GD will quit \"successfully\" without genuinely converging.) \\endrst Constructors GradientDescentParameters() Declaration optimal_learning::GradientDescentParameters::GradientDescentParameters()=delete GradientDescentParameters() \\rst Construct a GradientDescentParameters object. Default, copy, and assignment constructor are disallowed. INPUTS: See member declarations below for a description of each parameter. \\endrst Declaration optimal_learning::GradientDescentParameters::GradientDescentParameters(int num_multistarts_in, int max_num_steps_in, int max_num_restarts_in, int num_steps_averaged_in, double gamma_in, double pre_mult_in, double max_relative_change_in, double tolerance_in) GradientDescentParameters() Declaration optimal_learning::GradientDescentParameters::GradientDescentParameters(GradientDescentParameters&&OL_UNUSED(other))=default"
  },
  "api/optimal/learning/identify-type.html": {
    "href": "api/optimal/learning/identify-type.html",
    "title": "Struct optimal_learning::IdentifyType | qiotoolkit",
    "keywords": "Struct optimal_learning::IdentifyType \\rst Work-around for compilers (icc <= v13.0, gcc <= v4.5, msvcc <= 2013) that cannot deal with:: struct Foo { SomeType i; }; Foo f; SomeType y = decltype(f)::i; That is, resolve the scoping operator with decltype(). Instead, do:: SomeType y = IdentifyType<decltype(f)>::type::i; This can be removed once the relevant compilers support decltype + scoping. \\endrst"
  },
  "api/optimal/learning/invalid-value-exception.html": {
    "href": "api/optimal/learning/invalid-value-exception.html",
    "title": "Class optimal_learning::InvalidValueException | qiotoolkit",
    "keywords": "Class optimal_learning::InvalidValueException \\rst Overview** Exception to capture value != truth (+/- tolerance). The tolerance parameter is optional and only usable with floating point data types. Stores value and truth (and tolerance as applicable) for debugging/logging/reacting purposes. Message Format** The what() message is formatted in the class ctor (capitals indicate variable information):: R\"%%( InvalidValueException: VALUE != TRUTH (value != truth). CUSTOM_MESSAGE FUNCTION_NAME FILE_LINE_INFO )%%\" OR :: R\"%%( InvalidValueException: VALUE != TRUTH  TOLERANCE (value != truth  tolerance). CUSTOM_MESSAGE FUNCTION_NAME FILE_LINE_INFO )%%\" Depending on which ctor was used. \\endrst Inheritance optimal_learning::OptimalLearningException optimal_learning::InvalidValueException Inherited Members OptimalLearningException OptimalLearningException what AppendCustomMessageAndDebugInfo OptimalLearningException Constructors InvalidValueException() \\rst Constructs a InvalidValueException object with extra fields to flesh out the what() message. :line_info[] ptr to char array containing FILE and LINE info; e.g., from OL_STRINGIFY_FILE_AND_LINE :func_info[]: optional ptr to char array from OL_CURRENT_FUNCTION_NAME or similar :custom_message[]: optional ptr to char array with any additional text/info to print/log :value: the invalid value :truth: what \"value\" is supposed to be \\endrst Declaration optimal_learning::InvalidValueException<ValueType>::InvalidValueException(char const *line_info, char const *func_info, char const *custom_message, ValueType value_in, ValueType truth_in) InvalidValueException() \\rst Constructs a InvalidValueException object with extra fields to flesh out the what() message. This ctor additionally has an input for tolerance, and is only enabled for floating point types. :line_info[] ptr to char array containing FILE and LINE info; e.g., from OL_STRINGIFY_FILE_AND_LINE :func_info[]: optional ptr to char array from OL_CURRENT_FUNCTION_NAME or similar :custom_message[]: optional ptr to char array with any additional text/info to print/log :value: the invalid value :truth: what \"value\" is supposed to be :tolerance: the maximum acceptable error in |value - truth| \\endrst Declaration optimal_learning::InvalidValueException<ValueType>::InvalidValueException(char const *line_info, char const *func_info, char const *custom_message, ValueType value_in, ValueType truth_in, ValueType tolerance_in) Methods value() Declaration ValueType optimal_learning::InvalidValueException<ValueType>::value() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT truth() Declaration ValueType optimal_learning::InvalidValueException<ValueType>::truth() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT tolerance() Declaration ValueType optimal_learning::InvalidValueException<ValueType>::tolerance() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT OL_DISALLOW_DEFAULT_AND_ASSIGN() Declaration optimal_learning::InvalidValueException<ValueType>::OL_DISALLOW_DEFAULT_AND_ASSIGN(InvalidValueException)"
  },
  "api/optimal/learning/knowledge-gradient-evaluator.html": {
    "href": "api/optimal/learning/knowledge-gradient-evaluator.html",
    "title": "Class optimal_learning::KnowledgeGradientEvaluator | qiotoolkit",
    "keywords": "Class optimal_learning::KnowledgeGradientEvaluator \\rst A class to encapsulate the computation of knowledge gradient and its spatial gradient. This class handles the general KG computation case using monte carlo integration; it can support q,p-KG optimization. It is designed to work with any GaussianProcess. Additionally, this class has no state and within the context of KG optimization, it is meant to be accessed by const reference only. The random numbers needed for KG computation will be passed as parameters instead of contained as members to make multithreading more straightforward. \\endrst Inheritance optimal_learning::KnowledgeGradientEvaluator Constructors KnowledgeGradientEvaluator() \\rst Constructs a KnowledgeGradientEvaluator object. All inputs are required; no default constructor nor copy/assignment are allowed. :gaussian_process GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the underlying GP :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts :num_mc_iterations: number of monte carlo iterations :best_so_far: best (minimum) objective function value (in points_sampled_value) \\endrst Declaration optimal_learning::KnowledgeGradientEvaluator<DomainType>::KnowledgeGradientEvaluator(const GaussianProcess&gaussian_process_in, const int num_fidelity, double const *discrete_pts, int num_pts, int num_mc_iterations, const DomainType&domain, const GradientDescentParameters&optimizer_parameters, double best_so_far) KnowledgeGradientEvaluator() Declaration optimal_learning::KnowledgeGradientEvaluator<DomainType>::KnowledgeGradientEvaluator(KnowledgeGradientEvaluator&&other) Methods dim() Declaration int optimal_learning::KnowledgeGradientEvaluator<DomainType>::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_fidelity() Declaration int optimal_learning::KnowledgeGradientEvaluator<DomainType>::num_fidelity() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_mc_iterations() Declaration int optimal_learning::KnowledgeGradientEvaluator<DomainType>::num_mc_iterations() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT best_so_far() Declaration double optimal_learning::KnowledgeGradientEvaluator<DomainType>::best_so_far() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT gradient_descent_params() Declaration GradientDescentParameters optimal_learning::KnowledgeGradientEvaluator<DomainType>::gradient_descent_params() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT domain() Declaration DomainType optimal_learning::KnowledgeGradientEvaluator<DomainType>::domain() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT number_discrete_pts() Declaration int optimal_learning::KnowledgeGradientEvaluator<DomainType>::number_discrete_pts() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT discrete_pts_copy() Declaration std::vector<double>optimal_learning::KnowledgeGradientEvaluator<DomainType>::discrete_pts_copy() const noexcept OL_WARN_UNUSED_RESULT discrete_points() Declaration std::vector<double>optimal_learning::KnowledgeGradientEvaluator<DomainType>::discrete_points(double const *discrete_pts, int num_pts) const noexcept OL_WARN_UNUSED_RESULT gaussian_process() Declaration const GaussianProcess* optimal_learning::KnowledgeGradientEvaluator<DomainType>::gaussian_process() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT ComputeObjectiveFunction() \\rst Wrapper for ComputeKnowledgeGradient(); see that function for details. \\endrst Declaration double optimal_learning::KnowledgeGradientEvaluator<DomainType>::ComputeObjectiveFunction(StateType *kg_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradObjectiveFunction() \\rst Wrapper for ComputeGradKnowledgeGradient(); see that function for details. \\endrst Declaration void optimal_learning::KnowledgeGradientEvaluator<DomainType>::ComputeGradObjectiveFunction(StateType *kg_state, double *restrict grad_KG) const OL_NONNULL_POINTERS ComputeKnowledgeGradient() \\rst Computes the knowledge gradient :kg_state[1] properly configured state object \\output :kg_state[1]: state with temporary storage modified; normal_rng modified the knowledge gradient from sampling points_to_sample with points_being_sampled concurrent experiments \\endrst \\rst Compute Knowledge Gradient This version requires the discretization of A (the feasibe domain). The discretization usually is: some set + points previous sampled + points being sampled + points to sample \\endrst Declaration double optimal_learning::KnowledgeGradientEvaluator<DomainType>::ComputeKnowledgeGradient(StateType *kg_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradKnowledgeGradient() \\rst Computes the (partial) derivatives of the knowledge gradient with respect to each point of points_to_sample. As with ComputeKnowledgeGradient(), this computation accounts for the effect of points_being_sampled concurrent experiments. points_to_sample is the \"q\" and points_being_sampled is the \"p\" in q,p-KG. :kg_state[1] properly configured state object \\output :kg_state[1]: state with temporary storage modified; normal_rng modified :grad_KG[dim][num_to_sample]: gradient of KG, \\pderiv{KG(Xq \\cup Xp)}{Xq_{d,i}} where Xq is points_to_sample and Xp is points_being_sampled (grad KG from sampling points_to_sample with points_being_sampled concurrent experiments wrt each dimension of the points in points_to_sample) \\endrst \\rst Computes gradient of KG (see KnowledgeGradientEvaluator::ComputeGradKnowledgeGradient) wrt points_to_sample (stored in union_of_points[0:num_to_sample]). Mechanism is similar to the computation of KG, where points' contributions to the gradient are thrown out of their corresponding improvement <= 0.0. Thus \\nabla(\\mu) only contributes when the winner (point w/best improvement this iteration) is the current point. That is, the gradient of \\mu at x_i wrt x_j is 0 unless i == j (and only this result is stored in kg_state->grad_mu). The interaction with kg_state->grad_chol_decomp is harder to know a priori (like with grad_mu) and has a more complex structure (rank 3 tensor), so the derivative wrt x_j is computed fully, and the relevant submatrix (indexed by the current winner) is accessed each iteration. .. Note:: comments here are copied to _compute_grad_knowledge_gradient_monte_carlo() in python_version/knowledge_gradient.py \\endrst Declaration double optimal_learning::KnowledgeGradientEvaluator<DomainType>::ComputeGradKnowledgeGradient(StateType *kg_state, double *restrict grad_KG) const OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::KnowledgeGradientEvaluator<DomainType>::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(KnowledgeGradientEvaluator)"
  },
  "api/optimal/learning/knowledge-gradient-mcmcevaluator.html": {
    "href": "api/optimal/learning/knowledge-gradient-mcmcevaluator.html",
    "title": "Class optimal_learning::KnowledgeGradientMCMCEvaluator | qiotoolkit",
    "keywords": "Class optimal_learning::KnowledgeGradientMCMCEvaluator \\rst A class to encapsulate the computation of knowledge gradient and its spatial gradient. This class handles the general KG computation case using monte carlo integration; it can support q,p-KG optimization. It is designed to work with any GaussianProcess. Additionally, this class has no state and within the context of KG optimization, it is meant to be accessed by const reference only. The random numbers needed for KG computation will be passed as parameters instead of contained as members to make multithreading more straightforward. \\endrst Inheritance optimal_learning::KnowledgeGradientMCMCEvaluator Constructors KnowledgeGradientMCMCEvaluator() \\rst Constructs a KnowledgeGradientEvaluator object. All inputs are required; no default constructor nor copy/assignment are allowed. :gaussian_process GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the underlying GP :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts :num_mc_iterations: number of monte carlo iterations :best_so_far: best (minimum) objective function value (in points_sampled_value) \\endrst Declaration optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::KnowledgeGradientMCMCEvaluator(const GaussianProcessMCMC&gaussian_process_mcmc, const int num_fidelity, double const *discrete_pts_lst, int num_pts, int num_mc_iterations, const DomainType&domain, const GradientDescentParameters&optimizer_parameters, double const *best_so_far, std::vector<typename KnowledgeGradientState<DomainType>::EvaluatorType>*evaluator_vector) Methods dim() Declaration int optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_fidelity() Declaration int optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::num_fidelity() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT number_discrete_pts() Declaration int optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::number_discrete_pts() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_mcmc() Declaration int optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::num_mcmc() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT knowledge_gradient_evaluator_list() Declaration std::vector<KnowledgeGradientEvaluator<DomainType>>* optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::knowledge_gradient_evaluator_list() const noexcept OL_WARN_UNUSED_RESULT discrete_points_list() Declaration std::vector<double>optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::discrete_points_list(double const *discrete_pts_lst, int num_pts) const noexcept OL_WARN_UNUSED_RESULT best_so_far_list() Declaration std::vector<double>optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::best_so_far_list(double const *best_so_far) const noexcept OL_WARN_UNUSED_RESULT ComputeCost() \\rst compute the cost. \\endrst Declaration double optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::ComputeCost(StateType *kg_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradCost() \\rst compute the gradient of the cost. \\endrst Declaration void optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::ComputeGradCost(StateType *kg_state, double *restrict grad_cost) const OL_NONNULL_POINTERS ComputeObjectiveFunction() \\rst Wrapper for ComputeKnowledgeGradient(); see that function for details. \\endrst Declaration double optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::ComputeObjectiveFunction(StateType *kg_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradObjectiveFunction() \\rst Wrapper for ComputeGradKnowledgeGradient(); see that function for details. \\endrst Declaration void optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::ComputeGradObjectiveFunction(StateType *kg_state, double *restrict grad_KG) const OL_NONNULL_POINTERS ComputeKnowledgeGradient() \\rst Computes the knowledge gradient :kg_state[1] properly configured state object \\output :kg_state[1]: state with temporary storage modified; normal_rng modified the knowledge gradient from sampling points_to_sample with points_being_sampled concurrent experiments \\endrst \\rst Compute Knowledge Gradient This version requires the discretization of A (the feasibe domain). The discretization usually is: some set + points previous sampled + points being sampled + points to sample \\endrst Declaration double optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::ComputeKnowledgeGradient(StateType *kg_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradKnowledgeGradient() \\rst Computes the (partial) derivatives of the knowledge gradient with respect to each point of points_to_sample. As with ComputeKnowledgeGradient(), this computation accounts for the effect of points_being_sampled concurrent experiments. points_to_sample is the \"q\" and points_being_sampled is the \"p\" in q,p-KG. :kg_state[1] properly configured state object \\output :kg_state[1]: state with temporary storage modified; normal_rng modified :grad_KG[dim][num_to_sample]: gradient of KG, \\pderiv{KG(Xq \\cup Xp)}{Xq_{d,i}} where Xq is points_to_sample and Xp is points_being_sampled (grad KG from sampling points_to_sample with points_being_sampled concurrent experiments wrt each dimension of the points in points_to_sample) \\endrst \\rst Computes gradient of KG (see KnowledgeGradientEvaluator::ComputeGradKnowledgeGradient) wrt points_to_sample (stored in union_of_points[0:num_to_sample]). Mechanism is similar to the computation of KG, where points' contributions to the gradient are thrown out of their corresponding improvement <= 0.0. Thus \\nabla(\\mu) only contributes when the winner (point w/best improvement this iteration) is the current point. That is, the gradient of \\mu at x_i wrt x_j is 0 unless i == j (and only this result is stored in kg_state->grad_mu). The interaction with kg_state->grad_chol_decomp is harder to know a priori (like with grad_mu) and has a more complex structure (rank 3 tensor), so the derivative wrt x_j is computed fully, and the relevant submatrix (indexed by the current winner) is accessed each iteration. .. Note:: comments here are copied to _compute_grad_knowledge_gradient_monte_carlo() in python_version/knowledge_gradient.py \\endrst Declaration void optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::ComputeGradKnowledgeGradient(StateType *kg_state, double *restrict grad_KG) const OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::KnowledgeGradientMCMCEvaluator<DomainType>::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(KnowledgeGradientMCMCEvaluator)"
  },
  "api/optimal/learning/knowledge-gradient-mcmcstate.html": {
    "href": "api/optimal/learning/knowledge-gradient-mcmcstate.html",
    "title": "Struct optimal_learning::KnowledgeGradientMCMCState | qiotoolkit",
    "keywords": "Struct optimal_learning::KnowledgeGradientMCMCState \\rst State object for KnowledgeGradientEvaluator. This tracks the points being sampled in concurrent experiments (points_being_sampled) ALONG with the points currently being evaluated via knowledge gradient for future experiments (called points_to_sample); these are the p and q of q,p-KG, respectively. points_to_sample joined with points_being_sampled is stored in union_of_points in that order. This struct also tracks the state of the GaussianProcess that underlies the knowledge gradient computation: the GP state is built to handle the initial union_of_points, and subsequent updates to points_to_sample in this object also update the GP state. This struct also holds a pointer to a random number generator needed for Monte Carlo integrated KG computations. .. WARNING:: Users MUST guarantee that multiple state objects DO NOT point to the same RNG (in a multithreaded env). See general comments on State structs in gpp_common.hpp's header docs. \\endrst Constructors KnowledgeGradientMCMCState() \\rst Constructs an KnowledgeGradientMCMCState object with a specified source of randomness for the purpose of computing KG (and its gradient) over the specified set of points to sample. This establishes properly sized/initialized temporaries for KG computation, including dependent state from the associated Gaussian Process (which arrives as part of the kg_evaluator). .. WARNING:: This object is invalidated if the associated kg_evaluator is mutated. SetupState() should be called to reset. .. WARNING:: Using this object to compute gradients when configure_for_gradients := false results in UNDEFINED BEHAVIOR. :kg_evaluator knowledge gradient evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: points at which to evaluate KG and/or its gradient to check their value in future experiments (i.e., test points for GP predictions) :points_being_sampled[dim][num_being_sampled]: points being sampled in concurrent experiments :num_to_sample: number of potential future samples; gradients are evaluated wrt these points (i.e., the \"q\" in q,p-KG) :num_being_sampled: number of points being sampled in concurrent experiments (i.e., the \"p\" in q,p-KG) :configure_for_gradients: true if this object will be used to compute gradients, false otherwise :normal_rng[1]: pointer to a properly initialized* NormalRNG object .. NOTE:: The NormalRNG object must already be seeded. If multithreaded computation is used for KG, then every state object must have a different NormalRNG (different seeds, not just different objects). \\endrst Declaration optimal_learning::KnowledgeGradientMCMCState<DomainType>::KnowledgeGradientMCMCState(const EvaluatorType&kg_evaluator, double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample_in, int num_being_sampled_in, int num_pts_in, int const *restrict gradients_in, int num_gradients_in, bool configure_for_gradients, NormalRNGInterface *normal_rng_in, std::vector<typename KnowledgeGradientEvaluator<DomainType>::StateType>*kg_state_vector) KnowledgeGradientMCMCState() Declaration optimal_learning::KnowledgeGradientMCMCState<DomainType>::KnowledgeGradientMCMCState(KnowledgeGradientMCMCState&&other) Methods GetProblemSize() Declaration int optimal_learning::KnowledgeGradientMCMCState<DomainType>::GetProblemSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetCurrentPoint() \\rst Get the points_to_sample: potential future samples whose KG (and/or gradients) are being evaluated \\output :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::KnowledgeGradientMCMCState<DomainType>::GetCurrentPoint(double *restrict points_to_sample) const noexcept OL_NONNULL_POINTERS SetCurrentPoint() \\rst Change the potential samples whose KG (and/or gradient) are being evaluated. Update the state's derived quantities to be consistent with the new points. :kg_evaluator expected improvement evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::KnowledgeGradientMCMCState<DomainType>::SetCurrentPoint(const EvaluatorType&kg_evaluator, double const *restrict points_to_sample_in) OL_NONNULL_POINTERS SetupState() \\rst Configures this state object with new points_to_sample, the location of the potential samples whose KG is to be evaluated. Ensures all state variables & temporaries are properly sized. Properly sets all dependent state variables (e.g., GaussianProcess's state) for KG evaluation. .. WARNING:: This object's state is INVALIDATED if the kg_evaluator (including the GaussianProcess it depends on) used in SetupState is mutated! SetupState() should be called again in such a situation. :kg_evaluator knowledge gradient evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::KnowledgeGradientMCMCState<DomainType>::SetupState(const EvaluatorType&kg_evaluator, double const *restrict points_to_sample) OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::KnowledgeGradientMCMCState<DomainType>::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(KnowledgeGradientMCMCState) BuildUnionOfPoints() \\rst Create a vector with the union of points_to_sample and points_being_sampled (the latter is appended to the former). Note the l-value return. Assigning the return to a std::vector or passing it as an argument to the ctor will result in copy-elision or move semantics; no copying/performance loss. :points_to_sample[dim][num_to_sample]: points at which to evaluate KG and/or its gradient to check their value in future experiments (i.e., test points for GP predictions) :points_being_sampled[dim][num_being_sampled]: points being sampled in concurrent experiments :num_to_sample: number of potential future samples; gradients are evaluated wrt these points (i.e., the \"q\" in q,p-KG) :num_being_sampled: number of points being sampled in concurrent experiments (i.e., the \"p\" in q,p-KG) :dim: the number of spatial dimensions of each point array std::vector with the union of the input arrays: points_being_sampled is appended to points_to_sample \\endrst Declaration static std::vector<double>optimal_learning::KnowledgeGradientMCMCState<DomainType>::BuildUnionOfPoints(double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, int dim) noexcept OL_WARN_UNUSED_RESULT"
  },
  "api/optimal/learning/knowledge-gradient-state.html": {
    "href": "api/optimal/learning/knowledge-gradient-state.html",
    "title": "Struct optimal_learning::KnowledgeGradientState | qiotoolkit",
    "keywords": "Struct optimal_learning::KnowledgeGradientState \\rst State object for KnowledgeGradientEvaluator. This tracks the points being sampled in concurrent experiments (points_being_sampled) ALONG with the points currently being evaluated via knowledge gradient for future experiments (called points_to_sample); these are the p and q of q,p-KG, respectively. points_to_sample joined with points_being_sampled is stored in union_of_points in that order. This struct also tracks the state of the GaussianProcess that underlies the knowledge gradient computation: the GP state is built to handle the initial union_of_points, and subsequent updates to points_to_sample in this object also update the GP state. This struct also holds a pointer to a random number generator needed for Monte Carlo integrated KG computations. .. WARNING:: Users MUST guarantee that multiple state objects DO NOT point to the same RNG (in a multithreaded env). See general comments on State structs in gpp_common.hpp's header docs. \\endrst Constructors KnowledgeGradientState() \\rst Constructs an KnowledgeGradientState object with a specified source of randomness for the purpose of computing KG (and its gradient) over the specified set of points to sample. This establishes properly sized/initialized temporaries for KG computation, including dependent state from the associated Gaussian Process (which arrives as part of the kg_evaluator). .. WARNING:: This object is invalidated if the associated kg_evaluator is mutated. SetupState() should be called to reset. .. WARNING:: Using this object to compute gradients when configure_for_gradients := false results in UNDEFINED BEHAVIOR. :kg_evaluator knowledge gradient evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: points at which to evaluate KG and/or its gradient to check their value in future experiments (i.e., test points for GP predictions) :points_being_sampled[dim][num_being_sampled]: points being sampled in concurrent experiments :num_to_sample: number of potential future samples; gradients are evaluated wrt these points (i.e., the \"q\" in q,p-KG) :num_being_sampled: number of points being sampled in concurrent experiments (i.e., the \"p\" in q,p-KG) :configure_for_gradients: true if this object will be used to compute gradients, false otherwise :normal_rng[1]: pointer to a properly initialized* NormalRNG object .. NOTE:: The NormalRNG object must already be seeded. If multithreaded computation is used for KG, then every state object must have a different NormalRNG (different seeds, not just different objects). \\endrst Declaration optimal_learning::KnowledgeGradientState<DomainType>::KnowledgeGradientState(const EvaluatorType&kg_evaluator, double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample_in, int num_being_sampled_in, int, int const *restrict gradients_in, int num_gradients_in, bool configure_for_gradients, NormalRNGInterface *normal_rng_in) KnowledgeGradientState() Declaration optimal_learning::KnowledgeGradientState<DomainType>::KnowledgeGradientState(KnowledgeGradientState&&other) Methods SubsetData() Declaration std::vector<double>optimal_learning::KnowledgeGradientState<DomainType>::SubsetData(double const *restrict union_of_points_in, int num_union_in, int num_fidelity) noexcept OL_WARN_UNUSED_RESULT GetProblemSize() Declaration int optimal_learning::KnowledgeGradientState<DomainType>::GetProblemSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetCurrentPoint() \\rst Get the points_to_sample: potential future samples whose KG (and/or gradients) are being evaluated \\output :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::KnowledgeGradientState<DomainType>::GetCurrentPoint(double *restrict points_to_sample) const noexcept OL_NONNULL_POINTERS SetCurrentPoint() \\rst Change the potential samples whose KG (and/or gradient) are being evaluated. Update the state's derived quantities to be consistent with the new points. :kg_evaluator expected improvement evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::KnowledgeGradientState<DomainType>::SetCurrentPoint(const EvaluatorType&kg_evaluator, double const *restrict points_to_sample) OL_NONNULL_POINTERS SetupState() \\rst Configures this state object with new points_to_sample, the location of the potential samples whose KG is to be evaluated. Ensures all state variables & temporaries are properly sized. Properly sets all dependent state variables (e.g., GaussianProcess's state) for KG evaluation. .. WARNING:: This object's state is INVALIDATED if the kg_evaluator (including the GaussianProcess it depends on) used in SetupState is mutated! SetupState() should be called again in such a situation. :kg_evaluator knowledge gradient evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::KnowledgeGradientState<DomainType>::SetupState(const EvaluatorType&kg_evaluator, double const *restrict points_to_sample) PreCompute() \\rst Pre-compute to_sample_mean_, and cholesky_to_sample_var \\endrst Declaration void optimal_learning::KnowledgeGradientState<DomainType>::PreCompute(const EvaluatorType&kg_evaluator, double const *restrict) OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::KnowledgeGradientState<DomainType>::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(KnowledgeGradientState) BuildUnionOfPoints() \\rst Create a vector with the union of points_to_sample and points_being_sampled (the latter is appended to the former). Note the l-value return. Assigning the return to a std::vector or passing it as an argument to the ctor will result in copy-elision or move semantics; no copying/performance loss. :points_to_sample[dim][num_to_sample]: points at which to evaluate KG and/or its gradient to check their value in future experiments (i.e., test points for GP predictions) :points_being_sampled[dim][num_being_sampled]: points being sampled in concurrent experiments :num_to_sample: number of potential future samples; gradients are evaluated wrt these points (i.e., the \"q\" in q,p-KG) :num_being_sampled: number of points being sampled in concurrent experiments (i.e., the \"p\" in q,p-KG) :dim: the number of spatial dimensions of each point array std::vector with the union of the input arrays: points_being_sampled is appended to points_to_sample \\endrst Declaration static std::vector<double>optimal_learning::KnowledgeGradientState<DomainType>::BuildUnionOfPoints(double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, int dim) noexcept OL_WARN_UNUSED_RESULT"
  },
  "api/optimal/learning/log-marginal-likelihood-evaluator.html": {
    "href": "api/optimal/learning/log-marginal-likelihood-evaluator.html",
    "title": "Class optimal_learning::LogMarginalLikelihoodEvaluator | qiotoolkit",
    "keywords": "Class optimal_learning::LogMarginalLikelihoodEvaluator \\rst This serves as a quick summary of the Log Marginal Likelihood (LML). Please see the file comments here and in the corresponding .cpp file for further details. Class for computing the Log Marginal Likelihood. Given a particular covariance function (including hyperparameters) and training data ((point, function value, measurement noise) tuples), the log marginal likelihood is the log probability that the data were observed from a Gaussian Process would have generated the observed function values at the given measurement points. So log marginal likelihood tells us \"the probability of the observations given the assumptions of the model.\" Log marginal sits well with the Bayesian Inference camp. (Rasmussen & Williams p118) This quantity primarily deals with the trade-off between model fit and model complexity. Handling this trade-off is automatic in the log marginal likelihood calculation. See Rasmussen & Williams 5.2 and 5.4.1 for more details. We can use the log marginal likelihood to determine how good our model is. Additionally, we can maximize it by varying hyperparameters (or even changing covariance functions) to improve our model quality. Hence this class provides access to functions for computing log marginal likelihood and its hyperparameter gradients. .. Note:: These class comments are duplicated in Python: cpp_wrappers.log_likelihood.LogMarginalLikelihood \\endrst Inheritance optimal_learning::LogMarginalLikelihoodEvaluator Constructors LogMarginalLikelihoodEvaluator() \\rst Constructs a LogMarginalLikelihoodEvaluator object. All inputs are required; no default constructor nor copy/assignment are allowed. :covariance the CovarianceFunction object encoding assumptions about the GP's behavior on our data :points_sampled[dim][num_sampled]: points that have already been sampled :points_sampled_value[num_derivatives+1][num_sampled]: values of the already-sampled points :noise_variance[num_sampled]: the \\sigma_n^2 (noise variance) associated w/observation, points_sampled_value :dim: the spatial dimension of a point (i.e., number of independent params in experiment) :num_sampled: number of already-sampled points \\endrst Declaration optimal_learning::LogMarginalLikelihoodEvaluator::LogMarginalLikelihoodEvaluator(double const *restrict points_sampled_in, double const *restrict points_sampled_value_in, int const *derivatives_in, int num_derivatives_in, int dim_in, int num_sampled_in) OL_NONNULL_POINTERS Methods dim() Declaration int optimal_learning::LogMarginalLikelihoodEvaluator::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_sampled() Declaration int optimal_learning::LogMarginalLikelihoodEvaluator::num_sampled() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_derivatives() Declaration int optimal_learning::LogMarginalLikelihoodEvaluator::num_derivatives() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT ComputeObjectiveFunction() \\rst Wrapper for ComputeLogLikelihood(); see that function for details. \\endrst Declaration double optimal_learning::LogMarginalLikelihoodEvaluator::ComputeObjectiveFunction(StateType *log_likelihood_state) const noexcept OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradObjectiveFunction() \\rst Wrapper for ComputeGradLogLikelihood(); see that function for details. \\endrst Declaration void optimal_learning::LogMarginalLikelihoodEvaluator::ComputeGradObjectiveFunction(StateType *log_likelihood_state, double *restrict grad_log_marginal) const noexcept OL_NONNULL_POINTERS FillLogLikelihoodState() \\rst Wrapper for ComputeHessianLogLikelihood(); see that function for details. \\endrst \\rst Sets up the LogMarginalLikelihoodState object so that it can be used to compute log marginal and its gradients. ASSUMES all needed space is ALREADY ALLOCATED. This function should not be called directly; instead use LogMarginalLikelihoodState::SetupState. :log_likelihood_state[1] constructed state object with appropriate sized allocations \\output :log_likelihood_state[1]: fully configured state object, ready for use by this class's member functions \\endrst Declaration void optimal_learning::LogMarginalLikelihoodEvaluator::FillLogLikelihoodState(StateType *log_likelihood_state) const OL_NONNULL_POINTERS ComputeLogLikelihood() \\rst Computes the log marginal likelihood, log(p(y | X, \\theta)). That is, the probability of observing the training values, y, given the training points, X, and hyperparameters (of the covariance function), \\theta. This is a measure of how likely it is that the observed values came from our Gaussian Process Prior. :log_likelihood_state properly configured state oboject natural log of the marginal likelihood of the GP model \\endrst \\rst .. NOTE:: These comments have been copied into the matching method of LogMarginalLikelihood in python_version/log_likelihood.py. log p(y | X, \\theta) = -\\frac{1}{2} * y^T * K^-1 * y - \\frac{1}{2} * \\log(det(K)) - \\frac{n}{2} * \\log(2*pi) where n is num_sampled, \\theta are the hyperparameters, and \\log is the natural logarithm. In the following, term1 = -\\frac{1}{2} * y^T * K^-1 * y term2 = -\\frac{1}{2} * \\log(det(K)) term3 = -\\frac{n}{2} * \\log(2*pi) For an SPD matrix K = L * L^T, det(K) = \\Pi_i L_ii^2 We could compute this directly and then take a logarithm. But we also know: \\log(det(K)) = 2 * \\sum_i \\log(L_ii) The latter method is (currently) preferred for computing \\log(det(K)) due to reduced chance for overflow and (possibly) better numerical conditioning. \\endrst Declaration double optimal_learning::LogMarginalLikelihoodEvaluator::ComputeLogLikelihood(const StateType&log_likelihood_state) const noexcept OL_WARN_UNUSED_RESULT ComputeGradLogLikelihood() \\rst Computes the (partial) derivatives of the log marginal likelihood with respect to each hyperparameter of our covariance function. Let n_hyper = covariance_ptr->GetNumberOfHyperparameters(); :log_likelihood_state[1] properly configured state oboject \\output :log_likelihood_state[1]: state with temporary storage modified :grad_log_marginal[n_hyper]: gradient of log marginal likelihood wrt each hyperparameter of covariance \\endrst Declaration void optimal_learning::LogMarginalLikelihoodEvaluator::ComputeGradLogLikelihood(StateType *log_likelihood_state, double *restrict grad_log_marginal) const noexcept OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() \\rst Constructs the Hessian matrix of the log marginal likelihood function. This matrix is symmetric. It is also negative definite near maxima of the log marginal. See HyperparameterHessianCovariance() docs in CovarianceInterface (gpp_covariance.hpp) for details on the structure of the Hessian matrix. :log_likelihood_state[1] properly configured state oboject \\output :log_likelihood_state[1]: state with temporary storage modified :hessian_log_marginal[n_hyper][n_hyper]: (i,j)-th entry is \\mixpderiv{LML}{\\theta_i}{\\theta_j}, where LML = log(p(y | X, \\theta)) \\endrst Declaration optimal_learning::LogMarginalLikelihoodEvaluator::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(LogMarginalLikelihoodEvaluator) BuildHyperparameterGradCovarianceMatrix() \\rst Constructs the tensor of gradients (wrt hyperparameters) of the covariance function at all pairs of points_sampled_. The result is stored in state->grad_hyperparameter_cov_matrix. So we are computing \\pderiv{cov(X_i, X_j)}{\\theta_k}. These data are ordered as: grad_hyperparameter_cov_matrix[i][j][k] (i.e., num_hyperparmeters matrices of size Square(num_sampled_)). .. Note:: grad_hyperparameter_cov_matrix[i][j][k] == grad_hyperparameter_cov_matrix[j][i][k] :log_likelihood_state[1] properly configured state object \\output :log_likelihood_state[1]: state with grad_hyperparameter_cov_matrix filled \\endrst Declaration void optimal_learning::LogMarginalLikelihoodEvaluator::BuildHyperparameterGradCovarianceMatrix(StateType *log_likelihood_state) const noexcept"
  },
  "api/optimal/learning/log-marginal-likelihood-state.html": {
    "href": "api/optimal/learning/log-marginal-likelihood-state.html",
    "title": "Struct optimal_learning::LogMarginalLikelihoodState | qiotoolkit",
    "keywords": "Struct optimal_learning::LogMarginalLikelihoodState \\rst State object for LogMarginalLikelihoodEvaluator. This object tracks the covariance object as well as derived quantities that (along with the training points/values in the Evaluator class) fully specify the log marginal likelihood. Since this is used to optimize the log marginal likelihood, the covariance's hyperparameters are variable. See general comments on State structs in gpp_common.hpp's header docs. \\endrst Constructors LogMarginalLikelihoodState() \\rst Constructs a LogMarginalLikelihoodState object with a specified covariance object (in particular, new hyperparameters). Ensures all state variables & temporaries are properly sized. Properly sets all state variables so that the Evaluator can be used to compute log marginal likelihood, gradients, etc. .. WARNING:: This object's state is INVALIDATED if the log_likelihood_eval used in construction is mutated! SetupState() should be called again in such a situation. :log_likelihood_eval LogMarginalLikelihoodEvaluator object that this state is being used with :covariance_in: the CovarianceFunction object encoding assumptions about the GP's behavior on our data \\endrst Declaration optimal_learning::LogMarginalLikelihoodState::LogMarginalLikelihoodState(const EvaluatorType&log_likelihood_eval, const CovarianceInterface&covariance_in, const std::vector<double>noise_variance_in) LogMarginalLikelihoodState() Declaration optimal_learning::LogMarginalLikelihoodState::LogMarginalLikelihoodState(LogMarginalLikelihoodState&&other) Methods GetProblemSize() Declaration int optimal_learning::LogMarginalLikelihoodState::GetProblemSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT SetCurrentPoint() Declaration void optimal_learning::LogMarginalLikelihoodState::SetCurrentPoint(const EvaluatorType&log_likelihood_eval, double const *restrict hyperparameters) OL_NONNULL_POINTERS GetCurrentPoint() Declaration void optimal_learning::LogMarginalLikelihoodState::GetCurrentPoint(double *restrict hyperparameters) OL_NONNULL_POINTERS GetHyperparameters() \\rst Get hyperparameters of underlying covariance function. \\output :hyperparameters[num_hyperparameters]: covariance's hyperparameters \\endrst Declaration void optimal_learning::LogMarginalLikelihoodState::GetHyperparameters(double *restrict hyperparameters) const noexcept OL_NONNULL_POINTERS SetHyperparameters() \\rst Change the hyperparameters of the underlying covariance function. Update the state's derived quantities to be consistent with the new hyperparameters. :log_likelihood_eval LogMarginalLikelihoodEvaluator object that this state is being used with :hyperparameters[num_hyperparameters]: hyperparameters to change to \\endrst \\rst Computes the Hessian matrix of the log (marginal) likelihood wrt the hyperparameters:: \\mixpderiv{log(p(y | X, \\theta_k))}{\\theta_i}{\\theta_j} = (-\\alpha * \\pderiv{K}{\\theta_i} * K^-1 * \\pderiv{K}{\\theta_j} * \\alpha) (\\alpha * \\mixpderiv{K}{\\theta_i}{\\theta_j} * \\alpha) 0.5 * tr(-K^-1 * \\pderiv{K}{\\theta_i} * K^-1 * \\pderiv{K}{\\theta_j} + K^-1 * \\mixpderiv{K}{\\theta_i}{\\theta_j}) Note that as usual, K is the covariance matrix (bearing its own two indices, say K_{k,l}) which are omitted here. This expression arises from differentating each entry of the gradient (see function comments for LogMarginalLikelihoodEvaluator::ComputeGradLogLikelihood for expression) of the log marginal wrt each hyperparameter. We use the identity: \\pderiv{K^-1}{X} = -K^-1 * \\pderiv{K}{X} * K^-1; as well as the fact that \\partial tr(A) = tr(\\partial A). That is, since trace is linear, the order can be interchanged with the differential operator; and the various symmetries of the gradient/hessians of K (see function declaration comments for details on symmetry). \\endrst Declaration void optimal_learning::LogMarginalLikelihoodState::SetHyperparameters(const EvaluatorType&log_likelihood_eval, double const *restrict hyperparameters) OL_NONNULL_POINTERS SetupState() \\rst Configures this state object with new hyperparameters. Ensures all state variables & temporaries are properly sized. Properly sets all state variables for log likelihood (+ gradient) evaluation. .. WARNING:: This object's state is INVALIDATED if the log_likelihood used in SetupState is mutated! SetupState() should be called again in such a situation. :log_likelihood_eval log likelihood evaluator object that describes the training/already-measured data :hyperparameters[num_hyperparameters]: hyperparameters to change to \\endrst Declaration void optimal_learning::LogMarginalLikelihoodState::SetupState(const EvaluatorType&log_likelihood_eval, double const *restrict hyperparameters) OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::LogMarginalLikelihoodState::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(LogMarginalLikelihoodState)"
  },
  "api/optimal/learning/lower-bound-exception.html": {
    "href": "api/optimal/learning/lower-bound-exception.html",
    "title": "Class optimal_learning::LowerBoundException | qiotoolkit",
    "keywords": "Class optimal_learning::LowerBoundException \\rst Exception to capture value < min_value. Simple subclass of BoundsException that sets the max argument to std::numeric_limits ::max() See BoundsException for what() message format. \\endrst Inheritance optimal_learning::BoundsException optimal_learning::LowerBoundException Inherited Members max value OL_DISALLOW_DEFAULT_AND_ASSIGN BoundsException min BoundsException OptimalLearningException OptimalLearningException what AppendCustomMessageAndDebugInfo OptimalLearningException Constructors LowerBoundException() \\rst Constructs a LowerBoundException object with extra fields to flesh out the what() message. :line_info[] ptr to char array containing FILE and LINE info; e.g., from OL_STRINGIFY_FILE_AND_LINE :func_info[]: optional ptr to char array from OL_CURRENT_FUNCTION_NAME or similar :custom_message[]: optional ptr to char array with any additional text/info to print/log :value: the value that violates its min or max bound :min: the minimum bound for value \\endrst Declaration optimal_learning::LowerBoundException<ValueType>::LowerBoundException(char const *line_info, char const *func_info, char const *custom_message, ValueType value_in, ValueType min_in) Methods OL_DISALLOW_DEFAULT_AND_ASSIGN() Declaration optimal_learning::LowerBoundException<ValueType>::OL_DISALLOW_DEFAULT_AND_ASSIGN(LowerBoundException)"
  },
  "api/optimal/learning/matern-nu2p5.html": {
    "href": "api/optimal/learning/matern-nu2p5.html",
    "title": "Class optimal_learning::MaternNu2p5 | qiotoolkit",
    "keywords": "Class optimal_learning::MaternNu2p5 \\rst Implements a case of the Matern class of covariance functions with \\nu = 5/2 (smoothness parameter). See docs for MaternNu1p5 for more details on the Matern class of covariance fucntions. cov_{\\nu=5/2}(r) = (1 + \\sqrt{5}\\frac{r}[l} + \\frac{5}{3}\\frac{r^2}{l^2})\\exp(-\\sqrt{5}\\frac{r}{l}) We also implement the augmented kernel function with the gradient obervations. See CovarianceInterface for descriptions of the virtual functions. \\endrst Inheritance optimal_learning::CovarianceInterface optimal_learning::MaternNu2p5 Inherited Members ~CovarianceInterface Constructors MaternNu2p5() \\rst Constructs a MaternNu2p5 object with constant length-scale across all dimensions. :dim the number of spatial dimensions :alpha: the hyperparameter \\alpha (e.g., signal variance, \\sigma_f^2) :length: the constant length scale to use for all hyperparameter length scales \\endrst Declaration optimal_learning::MaternNu2p5::MaternNu2p5(int dim, double alpha, double length) MaternNu2p5() \\rst Constructs a MaternNu2p5 object with the specified hyperparameters. :dim the number of spatial dimensions :alpha: the hyperparameter \\alpha, (e.g., signal variance, \\sigma_f^2) :lengths[dim]: the hyperparameter length scales, one per spatial dimension \\endrst Declaration optimal_learning::MaternNu2p5::MaternNu2p5(int dim, double alpha, double const *restrict lengths) OL_NONNULL_POINTERS MaternNu2p5() \\rst Constructs a MaternNu2p5 object with the specified hyperparameters. :dim the number of spatial dimensions :alpha: the hyperparameter \\alpha, (e.g., signal variance, \\sigma_f^2) :lengths: the hyperparameter length scales, one per spatial dimension \\endrst Declaration optimal_learning::MaternNu2p5::MaternNu2p5(int dim, double alpha, std::vector<double>lengths) MaternNu2p5() Declaration optimal_learning::MaternNu2p5::MaternNu2p5(const MaternNu2p5&source) Methods Covariance() \\rst Computes the covariance function of the function values and their gradients of two points, cov(point_one, point_two). Points must be arrays with length dim. The covariance function is guaranteed to be symmetric by definition: Covariance(x, y) = Covariance(y, x). This function is also positive definite by definition. :point_one[dim] first spatial coordinate :derivatives_one[dim]: which derivatives of point_one are available :num_derivatives_one: int, the number of derivatives of point one :point_two[dim]: second spatial coordinate :derivatives_two[dim]: which derivatives of point_two are available :num_derivatives_two: int, the number of derivatives of point two cov[1+num_derivatives_one][1+num_derivatives_two]: value of covariance between the function values and their gradients of the input points \\endrst Declaration void optimal_learning::MaternNu2p5::Covariance(double const *restrict point_one, int const *restrict derivatives_one, int length_one, double const *restrict point_two, int const *restrict derivatives_two, int length_two, double *restrict cov) const noexcept override OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT GradCovariance() \\rst Computes the gradient of this.Covariance(point_one, point_two) with respect to the FIRST argument, point_one. This distinction is important for maintaining the desired symmetry. Cov(x, y) = Cov(y, x). Additionally, \\pderiv{Cov(x, y)}{x} = \\pderiv{Cov(y, x)}{x}. However, in general, \\pderiv{Cov(x, y)}{x} != \\pderiv{Cov(y, x)}{y} (NOT equal! These may differ by a negative sign) Hence to avoid separate implementations for differentiating against first vs second argument, this function only handles differentiation against the first argument. If you need \\pderiv{Cov(y, x)}{x}, just swap points x and y. :point_one[dim] first spatial coordinate :derivatives_one[dim]: which derivatives of point_one are available :num_derivatives_one: int, the number of derivatives of point one :point_two[dim]: second spatial coordinate :derivatives_two[dim]: which derivatives of point_two are available :num_derivatives_two: int, the number of derivatives of point two \\output grad_cov[dim][1+num_derivatives_one][1+num_derivatives_two]: (i, j, k)-th entry is \\pderiv{cov(x_1, x_2)(j, k))}{x1_i} \\endrst Declaration void optimal_learning::MaternNu2p5::GradCovariance(double const *restrict point_one, int const *restrict derivatives_one, int length_one, double const *restrict point_two, int const *restrict derivatives_two, int length_two, double *restrict grad_cov) const noexcept override OL_NONNULL_POINTERS GetNumberOfHyperparameters() \\rst Returns the number of hyperparameters. This base class only allows for a maximum of dim + 1 hyperparameters but subclasses may implement additional ones. The number of hyperparameters. Return 0 to disable hyperparameter-related gradients, optimizations. \\endrst Declaration virtual int optimal_learning::MaternNu2p5::GetNumberOfHyperparameters() const noexcept override OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT HyperparameterGradCovariance() \\rst Similar to GradCovariance(), except gradients are computed w.r.t. the hyperparameters. Unlike GradCovariance(), the order of point_one and point_two is irrelevant here (since we are not differentiating against either of them). Thus the matrix of grad covariances (wrt hyperparameters) is symmetric. :point_one[dim] first spatial coordinate :derivatives_one[dim]: which derivatives of point_one are available :num_derivatives_one: int, the number of derivatives of point one :point_two[dim]: second spatial coordinate :derivatives_two[dim]: which derivatives of point_two are available :num_derivatives_two: int, the number of derivatives of point two \\output :grad_hyperparameter_cov[this.GetNumberOfHyperparameters()][1+num_derivatives_one][1+num_derivatives_two]: (i, j, k)-th entry is \\pderiv{cov(x_1, x_2)(j, k)}{\\theta_i} \\endrst Declaration void optimal_learning::MaternNu2p5::HyperparameterGradCovariance(double const *restrict point_one, int const *restrict derivatives_one, int length_one, double const *restrict point_two, int const *restrict derivatives_two, int length_two, double *restrict grad_hyperparameter_cov) const noexcept override OL_NONNULL_POINTERS SetHyperparameters() \\rst Sets the hyperparameters. Hyperparameter ordering is defined implicitly by GetHyperparameters: [alpha=\\sigma_f^2, length_0, ..., length_{n-1}] :hyperparameters[this.GetNumberOfHyperparameters()] hyperparameters to set \\endrst Declaration virtual void optimal_learning::MaternNu2p5::SetHyperparameters(double const *restrict hyperparameters) noexcept override OL_NONNULL_POINTERS GetHyperparameters() \\rst Gets the hyperparameters. Ordering is [alpha=\\sigma_f^2, length_0, ..., length_{n-1}] \\output :hyperparameters[this.GetNumberOfHyperparameters()]: values of current hyperparameters \\endrst Declaration virtual void optimal_learning::MaternNu2p5::GetHyperparameters(double *restrict hyperparameters) const noexcept override OL_NONNULL_POINTERS Clone() \\rst For implementing the virtual (copy) constructor idiom. :Pointer to a constructed object that is a subclass of CovarianceInterface \\endrst Declaration CovarianceInterface * optimal_learning::MaternNu2p5::Clone() const override Initialize() \\rst Validate and initialize class data members. \\endrst Declaration void optimal_learning::MaternNu2p5::Initialize() OL_DISALLOW_DEFAULT_AND_ASSIGN() Declaration optimal_learning::MaternNu2p5::OL_DISALLOW_DEFAULT_AND_ASSIGN(MaternNu2p5)"
  },
  "api/optimal/learning/mock-expected-improvement-environment.html": {
    "href": "api/optimal/learning/mock-expected-improvement-environment.html",
    "title": "Class optimal_learning::MockExpectedImprovementEnvironment | qiotoolkit",
    "keywords": "Class optimal_learning::MockExpectedImprovementEnvironment \\rst Class to conveniently hold and generate random data that are commonly needed for testing functions in gpp_math.cpp. In particular, this mock is used for testing GP mean, GP variance, and expected improvement (and their gradients). This class holds arrays: points_sampled, points_sampled_value, points_to_sample, and points_being_sampled which are sized according to the parameters specified in Initialize(), and filled with random numbers. TODO(GH-125): we currently generate the point sets by repeated calls to rand(). This is generally unwise since the distribution of points is not particularly random. Additionally, our current covariance functions are all stationary, so we would rather generate a random base point x, and then a random (direction, radius) pair so that y = x + direction*radius. We better cover the different behavioral regimes of our code in this case, since it's the radius value that actually correlates to results. \\endrst Inheritance optimal_learning::MockExpectedImprovementEnvironment Constructors MockExpectedImprovementEnvironment() \\rst Construct a MockExpectedImprovementEnvironment and set invalid values for all size parameters (so that Initialize must be called to do anything useful) and pre-allocate some space. \\endrst Declaration optimal_learning::MockExpectedImprovementEnvironment::MockExpectedImprovementEnvironment() Methods Initialize() \\rst (Re-)initializes the data data in this function: this includes space allocation and random number generation. If any of the size parameters are changed from their current values, space will be realloc'd. Then it re-draws another set of uniform random points (in [-5, 5]) for the member arrays points_sampled, points_sampled_value, points_to_sample, and points_being_sampled. :dim the spatial dimension of a point (i.e., number of independent params in experiment) :num_to_sample: number of points to be sampled in future experiments :num_being_sampled: number of points being sampled concurrently :num_sampled: number of already-sampled points \\endrst Declaration void optimal_learning::MockExpectedImprovementEnvironment::Initialize(int dim_in, int num_to_sample_in, int num_being_sampled_in, int num_sampled_in, int num_derivatives_in) Initialize() Declaration void optimal_learning::MockExpectedImprovementEnvironment::Initialize(int dim_in, int num_to_sample_in, int num_being_sampled_in, int num_sampled_in, int num_derivatives_in, UniformRandomGenerator *uniform_generator) points_sampled() Declaration double* optimal_learning::MockExpectedImprovementEnvironment::points_sampled() points_sampled_value() Declaration double* optimal_learning::MockExpectedImprovementEnvironment::points_sampled_value() points_to_sample() Declaration double* optimal_learning::MockExpectedImprovementEnvironment::points_to_sample() points_being_sampled() Declaration double* optimal_learning::MockExpectedImprovementEnvironment::points_being_sampled() OL_DISALLOW_COPY_AND_ASSIGN() Declaration optimal_learning::MockExpectedImprovementEnvironment::OL_DISALLOW_COPY_AND_ASSIGN(MockExpectedImprovementEnvironment)"
  },
  "api/optimal/learning/mock-gaussian-process-prior-data.html": {
    "href": "api/optimal/learning/mock-gaussian-process-prior-data.html",
    "title": "Struct optimal_learning::MockGaussianProcessPriorData | qiotoolkit",
    "keywords": "Struct optimal_learning::MockGaussianProcessPriorData \\rst Struct to generate data randomly from a GaussianProcess. This object contains the generated GaussianProcess as well as the inputs needed to generate it (e.g., hyperparameters, domain, etc). This struct is intended for convenience (so that test writers do not need to repeat these lines in every test that builds its input data from a GP). It has a fire-and-forget constructor that builds all fields randomly, but it also exposes all internal state/functions used by that ctor so that power users can customize their test scenarios further. Implementation: this object uses std::unique_ptr to hide complex object definitions in the corresponding cpp file \\endrst Constructors MockGaussianProcessPriorData() \\rst Construct an empty MockGaussianProcessPriorData. Member int, double, and vectors are initialized appropriately. covariance_ptr is cloned from covariance. BUT domain_ptr and gaussian_process_ptr ARE NOT initialized. Use this class's member functions to properly initialize these more complex data. :covariance the CovarianceInterface object encoding assumptions about the GP's behavior on our data :noise_variance: the \\sigma_n^2 (noise variance) associated w/observation, i-th entry will be associated with the i-th point generated by the GP :dim: the spatial dimension of a point (i.e., number of independent params in experiment) :num_sampled: number of already-sampled points (that we want the GP to hold) \\endrst Declaration optimal_learning::MockGaussianProcessPriorData<DomainType>::MockGaussianProcessPriorData(const CovarianceInterface&covariance, const std::vector<int>&derivatives_in, int num_derivatives_in, int dim_in, int num_sampled_in) MockGaussianProcessPriorData() \\rst Completely constructs a MockGaussianProcessPriorData, initializing all fields. Builds a GP based on a randomly generated domain and hyperparameters. :covariance the CovarianceInterface object encoding assumptions about the GP's behavior on our data :noise_variance: the \\sigma_n^2 (noise variance) associated w/observation, i-th entry will be associated with the i-th point generated by the GP :dim: the spatial dimension of a point (i.e., number of independent params in experiment) :num_sampled: number of already-sampled points (that we want the GP to hold) :uniform_double_domain_lower: [min, max] range from which to draw domain lower bounds :uniform_double_domain_upper: [min, max] range from which to draw domain upper bounds :uniform_double_hyperparameters: [min, max] range from which to draw hyperparameters :uniform_generator[1]: a UniformRandomGenerator object providing the random engine for uniform random numbers \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws \\endrst Declaration optimal_learning::MockGaussianProcessPriorData<DomainType>::MockGaussianProcessPriorData(const CovarianceInterface&covariance, const std::vector<int>&derivatives_in, int num_derivatives_in, int dim_in, int num_sampled_in, const boost::uniform_real<double>&uniform_double_domain_lower, const boost::uniform_real<double>&uniform_double_domain_upper, const boost::uniform_real<double>&uniform_double_hyperparameters, UniformRandomGenerator *uniform_generator) Methods ~MockGaussianProcessPriorData() \\rst Prevent inline destructor: the dtor of std::unique_ptr needs access to T's dtor (b/c unique_ptr's dtor basically calls delete on T*). But we want to forward-declare all of our T objects, so the dtor must be defined in the cpp file where those defintions are visible. \\endrst Declaration optimal_learning::MockGaussianProcessPriorData<DomainType>::~MockGaussianProcessPriorData() InitializeHyperparameters() \\rst Sets hyperparameters of covariance with random draws from the specified interval. Modifies: hyperparameters, covariance_ptr :uniform_double_hyperparameters [min, max] range from which to draw hyperparameters :uniform_generator[1]: a UniformRandomGenerator object providing the random engine for uniform random numbers \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws \\endrst Declaration void optimal_learning::MockGaussianProcessPriorData<DomainType>::InitializeHyperparameters(const boost::uniform_real<double>&uniform_double_hyperparameters, UniformRandomGenerator *uniform_generator) InitializeDomain() \\rst Sets the domain from which the GP's historical data will be generated. For each dimension, we draw [min, max] bounds (domain_bounds); then we construct a domain object. Modifies: domain_bounds, domain_ptr :uniform_double_domain_lower [min, max] range from which to draw domain lower bounds :uniform_double_domain_upper: [min, max] range from which to draw domain upper bounds :uniform_generator[1]: a UniformRandomGenerator object providing the random engine for uniform random numbers \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws \\endrst Declaration void optimal_learning::MockGaussianProcessPriorData<DomainType>::InitializeDomain(const boost::uniform_real<double>&uniform_double_domain_lower, const boost::uniform_real<double>&uniform_double_domain_upper, UniformRandomGenerator *uniform_generator) InitializeGaussianProcess() \\rst Builds a GaussianProcess with num_sampled points (drawn randomly from the domain) whose values are drawn randomly from the GP (sampled one at a time and added to the prior). Users MUST call InitializeHyperparameters() and InitializeDomain() (or otherwise initialize hyperparameters and domain_ptr) before calling this function. Modifies: covariance_ptr, best_so_far, gaussian_procss_ptr :uniform_generator[1] a UniformRandomGenerator object providing the random engine for uniform random numbers \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws \\endrst Declaration void optimal_learning::MockGaussianProcessPriorData<DomainType>::InitializeGaussianProcess(UniformRandomGenerator *uniform_generator)"
  },
  "api/optimal/learning/mock-knowledge-gradient-environment.html": {
    "href": "api/optimal/learning/mock-knowledge-gradient-environment.html",
    "title": "Class optimal_learning::MockKnowledgeGradientEnvironment | qiotoolkit",
    "keywords": "Class optimal_learning::MockKnowledgeGradientEnvironment \\rst Checks that multithreaded KG optimization behaves the same way that single threaded does. number of test failures: 0 if KG multi/single threaded optimization are consistent \\endrst \\rst Checks that KG optimization is working on tensor product or simplex domain using monte-carlo KG evaluation. :domain_type type of the domain to test on (e.g., tensor product, simplex) number of test failures: 0 if KG optimization is working properly \\endrst \\rst Checks that ComputeKGOptimalPointsToSample works on a tensor product domain. This test exercises the the code tested in: KnowledgeGradientOptimizationTest(kTensorProduct) for ei_mode = {kAnalytic, kMonteCarlo}. This test checks the generation of multiple, simultaneous experimental points to sample. number of test failures: 0 if KG optimization is working properly \\endrst \\rst Tests EvaluateKGAtPointList (computes KG at a specified list of points, multithreaded). Checks that the returned best point is in fact the best. Verifies multithreaded consistency. number of test failures: 0 if function evaluation is working properly \\endrst \\rst Class to conveniently hold and generate random data that are commonly needed for testing functions in gpp_math.cpp. In particular, this mock is used for testing GP mean, GP variance, and expected improvement (and their gradients). This class holds arrays: points_sampled, points_sampled_value, points_to_sample, and points_being_sampled which are sized according to the parameters specified in Initialize(), and filled with random numbers. TODO(GH-125): we currently generate the point sets by repeated calls to rand(). This is generally unwise since the distribution of points is not particularly random. Additionally, our current covariance functions are all stationary, so we would rather generate a random base point x, and then a random (direction, radius) pair so that y = x + direction*radius. We better cover the different behavioral regimes of our code in this case, since it's the radius value that actually correlates to results. \\endrst Inheritance optimal_learning::MockKnowledgeGradientEnvironment Constructors MockKnowledgeGradientEnvironment() \\rst Construct a MockExpectedImprovementEnvironment and set invalid values for all size parameters (so that Initialize must be called to do anything useful) and pre-allocate some space. \\endrst Declaration optimal_learning::MockKnowledgeGradientEnvironment::MockKnowledgeGradientEnvironment() Methods Initialize() \\rst (Re-)initializes the data data in this function: this includes space allocation and random number generation. If any of the size parameters are changed from their current values, space will be realloc'd. Then it re-draws another set of uniform random points (in [-5, 5]) for the member arrays points_sampled, points_sampled_value, points_to_sample, and points_being_sampled. :dim the spatial dimension of a point (i.e., number of independent params in experiment) :num_to_sample: number of points to be sampled in future experiments :num_being_sampled: number of points being sampled concurrently :num_sampled: number of already-sampled points \\endrst Declaration void optimal_learning::MockKnowledgeGradientEnvironment::Initialize(int dim_in, int num_to_sample_in, int num_being_sampled_in, int num_sampled_in, int num_pts_in, int num_derivatives_in) Initialize() Declaration void optimal_learning::MockKnowledgeGradientEnvironment::Initialize(int dim_in, int num_to_sample_in, int num_being_sampled_in, int num_sampled_in, int num_pts_in, int num_derivatives_in, UniformRandomGenerator *uniform_generator) points_sampled() Declaration double* optimal_learning::MockKnowledgeGradientEnvironment::points_sampled() points_sampled_value() Declaration double* optimal_learning::MockKnowledgeGradientEnvironment::points_sampled_value() points_to_sample() Declaration double* optimal_learning::MockKnowledgeGradientEnvironment::points_to_sample() points_being_sampled() Declaration double* optimal_learning::MockKnowledgeGradientEnvironment::points_being_sampled() discrete_pts() Declaration double* optimal_learning::MockKnowledgeGradientEnvironment::discrete_pts() OL_DISALLOW_COPY_AND_ASSIGN() Declaration optimal_learning::MockKnowledgeGradientEnvironment::OL_DISALLOW_COPY_AND_ASSIGN(MockKnowledgeGradientEnvironment)"
  },
  "api/optimal/learning/multistart-optimizer.html": {
    "href": "api/optimal/learning/multistart-optimizer.html",
    "title": "Class optimal_learning::MultistartOptimizer | qiotoolkit",
    "keywords": "Class optimal_learning::MultistartOptimizer \\rst This is a general, template class for multistart optimization. It is designed to be used with the various Optimizer classes in this file (e.g., NullOptimizer, GradientDescentOptimizer, NewtonOptimizer). The multistart process is multithreaded using OpenMP so that we can start from multiple initial guesses across multiple threads simultaneously. See section 2c) and 3b, iii) in the header docs at the top of the file for more details. The use with GradientDescentOptimizer, NewtonOptimizer, etc. are standard practice in nonlinear optimization. In particular, without special properties like convexity, single-start optimizers can converge to local optima. In general, a nonlinear function can have many local optima, so the only way to improve* your chances of finding the global optimum is to start from many different locations. This will be the typical use case for MultistartOptimizer<...>::MultistartOptimize(). Improve is intentional here. In the general case, you are not guaranteed (in finite time) to find the global optimum. Use with NullOptimizer requires special mention here as it might seem silly. This case reduces to evaluating the objective function at every point of initial_guesses. Through function_values, you can get the objective value at each of point of initial_guesses too (e.g., for plotting). So use MultistartOptimize with NullOptimzer to perform a 'dumb' search (e.g., initial_guesses can be obtained from a grid, random sampling, etc.). NullOptimizer allows 'dumb' search to use the same code as multistart optimization. 'Dumb' search is inaccurate but it never fails, so we often use it as a fall-back when more advanced (e.g., gradient descent) techniques fail. This class provides just one method (for now), MultistartOptimize(); see below. .. Note:: comments copied to MultistartOptimizer in python_version/optimization.py. \\endrst Inheritance optimal_learning::MultistartOptimizer Constructors MultistartOptimizer() Declaration optimal_learning::MultistartOptimizer<Optimizer_>::MultistartOptimizer()=default Methods MultistartOptimize() \\rst Performs multistart optimization with the specified Optimizer (class template parameter) to optimize the specified ObjectiveFunctionEvaluator over the specified DomainType. Optimizer behavior is controlled by the specified ParameterStruct. See class docs and header docs of this file, section 2c and 3b, iii), for more information. The method allows you to specify what the current best is, so that if optimization cannot beat it, no improvement will be reported. It will otherwise report the overall best improvement (through io_container) as well as the result of every individual multistart run if desired (through function_values). .. Note:: comments copied to MultistartOptimizer.optimize() in python_version/optimization.py. Generally, you will not call this function directly. Instead, it is intended to be used in wrappers that set up state, thread_schedule, etc. for the specific optimization problem at hand. For examples with Expected Improvement (EI), see gpp_math: EvaluateEIAtPointList() ComputeOptimalPointsToSampleViaMultistartGradientDescent() or gpp_model_selection: EvaluateLogLikelihoodAtPointList() MultistartGradientDescentHyperparameterOptimization() MultistartNewtonHyperparameterOptimization() problem_size refers to objective_state->GetProblemSize(), the number of dimensions in a \"point\" aka the number of variables being optimized. (This might be the spatial dimension for EI or the number of hyperparameters for log likelihood.) :optimizer object with the desired Optimize() functionality (e.g., do nothing for 'dumb' search, gradient descent, etc.) :objective_evaluator: reference to object that can compute the objective function, its gradient, and/or its hessian, depending on the needs of optimizer :optimizer_parameters: Optimizer::ParameterStruct object that describes the parameters for optimization (e.g., number of iterations, tolerances, scale factors, etc.) :domain: object specifying the domain to optimize over (see gpp_domain.hpp) :thread_schedule: struct instructing OpenMP on how to schedule threads; i.e., max_num_threads, schedule type, chunk_size :initial_guesses[problem_size][num_multistarts]: list of points at which to start optimization runs; all points must lie INSIDE the specified domain :num_multistarts: number of random points to use from initial guesses :objective_state_vector[thread_schedule.max_num_threads]: properly constructed/configured ObjectiveFunctionEvaluator::State objects, at least one per thread objective_state.GetCurrentPoint() will be used to obtain the initial guess :io_container[1]: object with best_objective_value_so_far and corresponding best_point properly initialized. See struct docs in gpp_optimization.hpp for details. \\output :objective_state_vector[thread_schedule.max_num_threads]: internal states of state objects may be modified :function_values[num_multistarts]: objective fcn value at the end of each optimization run, in the same order as initial_guesses. Can be used to check what each optimization run converged to. More commonly used only with NullOptimizer to get a list of objective values at each point of initial_guesses. Never dereferenced if nullptr. :io_container[1]: object container new best_objective_value_so_far and corresponding best_point IF found_flag is true. Unchanged from input otherwise. See struct docs in gpp_optimization.hpp for details. \\raise if any of objective_state_vector->SetCurrentPoint(), optimizer.Optimize(), or objective_evaluator.ComputeObjectiveFunction() throws, the exception (or one of the exceptions in the event of multiple throws due to threading, usually the first temporally) will be saved and rethrown by this function. io_container will be in a valid state; function_values may not. \\endrst Declaration void optimal_learning::MultistartOptimizer<Optimizer_>::MultistartOptimize(const Optimizer&optimizer, const ObjectiveFunctionEvaluator&objective_evaluator, const ParameterStruct&optimizer_parameters, const DomainType&domain, const ThreadSchedule&thread_schedule, double const *restrict initial_guesses, int num_multistarts, typename ObjectiveFunctionEvaluator::StateType *objective_state_vector, double *restrict function_values, OptimizationIOContainer *restrict io_container) OL_DISALLOW_COPY_AND_ASSIGN() Declaration optimal_learning::MultistartOptimizer<Optimizer_>::OL_DISALLOW_COPY_AND_ASSIGN(MultistartOptimizer)"
  },
  "api/optimal/learning/newton-optimizer.html": {
    "href": "api/optimal/learning/newton-optimizer.html",
    "title": "Class optimal_learning::NewtonOptimizer | qiotoolkit",
    "keywords": "Class optimal_learning::NewtonOptimizer \\rst Newton optimization. This class optimizes using Newton's method with a refinement step (see comments on the Optimize()) function. \\endrst Inheritance optimal_learning::NewtonOptimizer Constructors NewtonOptimizer() Declaration optimal_learning::NewtonOptimizer<ObjectiveFunctionEvaluator_, DomainType_>::NewtonOptimizer()=default Methods Optimize() \\rst Uses Newton's Method to optimize the value of an objective function, f (e.g., log marginal likelihood). .. NOTE:: this function wraps NewtonOptimization(), see above. It first calls that function directly, then calls it again with a modified newton_parameters struct: the param struct is modified to run newton with a small number of iterations at a huge time_factor (to remove the diagonal dominance adjustment entirely). We do this to ensure that Newton has converged. See section 2b) and 3b, ii) in the header docs and the docs for NewtonOptimization() for more details. Solution is guaranteed to lie within the region specified by \"domain\"; note that this may not be a true optima (i.e., the gradient may be substantially nonzero). problem_size refers to objective_state->GetProblemSize(), the number of dimensions in a \"point\" aka the number of variables being optimized. (This might be the spatial dimension for EI or the number of hyperparameters for log likelihood.) :objective_evaluator reference to object that can compute the objective function and its gradient :newton_parameters: NewtonParameters object that describes the parameters newton optimization (e.g., number of iterations, tolerances, additional diagonal dominance) :domain: object specifying the domain to optimize over (see gpp_domain.hpp) :objective_state[1]: a properly configured state object for the ObjectiveFunctionEvaluator template parameter objective_state.GetCurrentPoint() will be used to obtain the initial guess \\output :objective_state[1]: a state object whose temporary data members may have been modified objective_state.GetCurrentPoint() will return the point yielding the best objective function value according to newton number of errors \\endrst Declaration int optimal_learning::NewtonOptimizer<ObjectiveFunctionEvaluator_, DomainType_>::Optimize(const ObjectiveFunctionEvaluator&objective_evaluator, const ParameterStruct&newton_parameters, const DomainType&domain, typename ObjectiveFunctionEvaluator::StateType *objective_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT OL_DISALLOW_COPY_AND_ASSIGN() Declaration optimal_learning::NewtonOptimizer<ObjectiveFunctionEvaluator_, DomainType_>::OL_DISALLOW_COPY_AND_ASSIGN(NewtonOptimizer)"
  },
  "api/optimal/learning/newton-parameters.html": {
    "href": "api/optimal/learning/newton-parameters.html",
    "title": "Struct optimal_learning::NewtonParameters | qiotoolkit",
    "keywords": "Struct optimal_learning::NewtonParameters \\rst Container to hold parameters that specify the behavior of Newton. .. Note:: these comments are copied in build_newton_parameters() in cpp_wrappers/optimizer_parameters.py. That function wraps this struct's ctor. Diagonal dominance control: gamma and time_factor** On i-th newton iteration, we add 1/(time_factor*gamma^{i+1}) * I to the Hessian to improve robustness Choosing a small gamma (e.g., 1.0 < gamma <= 1.01) and time_factor (e.g., 0 < time_factor <= 1.0e-3) leads to more consistent/stable convergence at the cost of slower performance (and in fact for gamma or time_factor too small, gradient descent is preferred). Conversely, choosing more aggressive values may lead to very fast convergence at the cost of more cases failing to converge. gamma = 1.01, time_factor = 1.0e-3 should lead to good robustness at reasonable speed. This should be a fairly safe default. gamma = 1.05, time_factor = 1.0e-1 will be several times faster but not as robust. for \"easy\" problems, these can be much more aggressive, e.g., gamma = 2.0, time_factor = 1.0e1 or more. \\endrst Constructors NewtonParameters() Declaration optimal_learning::NewtonParameters::NewtonParameters()=delete NewtonParameters() \\rst Construct a NewtonParameters object. Default, copy, and assignment constructor are disallowed. INPUTS: See member declarations below for a description of each parameter. \\endrst Declaration optimal_learning::NewtonParameters::NewtonParameters(int num_multistarts_in, int max_num_steps_in, double gamma_in, double time_factor_in, double max_relative_change_in, double tolerance_in) NewtonParameters() Declaration optimal_learning::NewtonParameters::NewtonParameters(NewtonParameters&&OL_UNUSED(other))=default"
  },
  "api/optimal/learning/normal-rng.html": {
    "href": "api/optimal/learning/normal-rng.html",
    "title": "Class optimal_learning::NormalRNG | qiotoolkit",
    "keywords": "Class optimal_learning::NormalRNG \\rst Functor for computing normally distributed (N(0, 1)) random numbers. Uses/maintains an uniform RNG (currently UniformRandomGenerator) and transforms the output to be distributed ~ N(0, 1). .. Note:: seed values take type EngineType::result_type. Do not pass in a wider integer type! .. WARNING:: this class is NOT THREAD-SAFE. You must construct one object per thread (and ensure that the seeds are different for practical computations). \\endrst Inheritance optimal_learning::NormalRNGInterface optimal_learning::NormalRNG Inherited Members ~NormalRNGInterface Constructors NormalRNG() \\rst Default-constructs a NormalRNG, seeding with kDefaultSeed. \\endrst Declaration optimal_learning::NormalRNG::NormalRNG() noexcept NormalRNG() \\rst Construct a NormalRNG, seeding with the specified seed. See NormalRNG::SetExplicitSeed for details. :seed new seed to set \\endrst Declaration optimal_learning::NormalRNG::NormalRNG(EngineType::result_type seed) noexcept NormalRNG() \\rst Construct a NormalRNG, seeding with an automatically selected seed based on time, thread_id, etc. See NormalRNG::SetRandomizedSeed for details. :base_seed base value for the new seed :thread_id: id of the thread using this object \\endrst Declaration optimal_learning::NormalRNG::NormalRNG(EngineType::result_type seed, int thread_id) noexcept Methods GetEngine() \\rst Get a reference to the RNG engine used by this class. reference to the underlying RNG engine \\endrst Declaration EngineType&optimal_learning::NormalRNG::GetEngine() noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT operator()() \\rst Generate a random number from standard normal distribution. a random number from a standard (N(0, 1)) normal distribution \\endrst Declaration double optimal_learning::NormalRNG::operator()() last_seed() Declaration EngineType::result_type optimal_learning::NormalRNG::last_seed() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT ResetGenerator() \\rst Clears state of normal_distribution_ so that future uses do not depend on any previous actions. This is important: the underlying normal distribution likely generates numbers \\emph{two} at a time. So re-seeding the engine WITHOUT resetting can lead to surprising behavior. \\endrst Declaration void optimal_learning::NormalRNG::ResetGenerator() noexcept SetExplicitSeed() \\rst Seed the random number generator with the input value. See UniformRandomGenerator::SetExplicitSeed() for more information. :seed new seed to set \\endrst Declaration void optimal_learning::NormalRNG::SetExplicitSeed(EngineType::result_type seed) noexcept SetRandomizedSeed() \\rst Set a new seed for the random number generator. A \"random\" seed is selected based on the input seed value, the current time, and the thread_id. See UniformRandomGenerator::SetExplicitSeed() for more information. :seed base value for the new seed :thread_id: id of the thread using this object \\endrst Declaration void optimal_learning::NormalRNG::SetRandomizedSeed(EngineType::result_type seed, int thread_id) noexcept ResetToMostRecentSeed() \\rst Reseeds the generator with its most recently specified seed value. Useful for testinge.g., can conduct multiple runs with the same initial conditions \\endrst Declaration void optimal_learning::NormalRNG::ResetToMostRecentSeed() noexcept PrintState() \\rst Prints the state of the generator to specified ostream. For testing. :out_stream[1] a std::ostream object ready for operator<< use \\output :out_stream[1]: std::ostream with engine state <<'d to it \\endrst Declaration void optimal_learning::NormalRNG::PrintState(std::ostream *out_stream) const OL_NONNULL_POINTERS"
  },
  "api/optimal/learning/normal-rnginterface.html": {
    "href": "api/optimal/learning/normal-rnginterface.html",
    "title": "Class optimal_learning::NormalRNGInterface | qiotoolkit",
    "keywords": "Class optimal_learning::NormalRNGInterface \\rst Abstract class for a functor for generating random numbers distributed ~ N(0, 1) (i.e., standard normal). This interface currently does not specify many facilities for seeding the underlying RNG as these (particularly variable width) can vary by implementation. This class only has pure virtual functions. \\endrst Inheritance optimal_learning::NormalRNGInterface optimal_learning::NormalRNG optimal_learning::NormalRNGSimulator Methods operator()() \\rst Generate a random number from standard normal distribution. a random number from a standard (N(0, 1)) normal distribution \\endrst Declaration virtual double optimal_learning::NormalRNGInterface::operator()()=0 ResetToMostRecentSeed() \\rst Reseeds the generator with its most recently specified seed value. Useful for testinge.g., can conduct multiple runs with the same initial conditions \\endrst Declaration virtual void optimal_learning::NormalRNGInterface::ResetToMostRecentSeed() noexcept=0 ~NormalRNGInterface() Declaration virtual optimal_learning::NormalRNGInterface::~NormalRNGInterface()=default"
  },
  "api/optimal/learning/normal-rngsimulator.html": {
    "href": "api/optimal/learning/normal-rngsimulator.html",
    "title": "Class optimal_learning::NormalRNGSimulator | qiotoolkit",
    "keywords": "Class optimal_learning::NormalRNGSimulator \\rst RNG that generates normally distributed (N(0,1)) random numbers simply by reading random numbers stored in its \"random_number_table\", a data member in this class. .. Note:: this class is used in unit test only, and you have to be careful to ensure the total number of random numbers generated from last reset must be smaller than size of \"random_number_table\", otherwise exception will be thrown. .. Warning:: this class is NOT THREAD-SAFE. You must construct one object per thread. \\endrst Inheritance optimal_learning::NormalRNGInterface optimal_learning::NormalRNGSimulator Inherited Members ~NormalRNGInterface Constructors NormalRNGSimulator() \\rst Construct a NormalRNGSimulator by providing table storing random numbers, and size of this random table. :random_number_table_in pointer to the table storing random numbers :size_of_table_in: size of the random table \\endrst Declaration optimal_learning::NormalRNGSimulator::NormalRNGSimulator(const std::vector<double>&random_number_table_in) Methods operator()() \\rst Generate a random number from standard normal distribution. a random number from a standard (N(0, 1)) normal distribution \\endrst Declaration double optimal_learning::NormalRNGSimulator::operator()() ResetToMostRecentSeed() \\rst Reseeds the generator with its most recently specified seed value. Useful for testinge.g., can conduct multiple runs with the same initial conditions \\endrst Declaration void optimal_learning::NormalRNGSimulator::ResetToMostRecentSeed() noexcept index() Declaration int optimal_learning::NormalRNGSimulator::index() const OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::NormalRNGSimulator::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(NormalRNGSimulator)"
  },
  "api/optimal/learning/null-optimizer.html": {
    "href": "api/optimal/learning/null-optimizer.html",
    "title": "Class optimal_learning::NullOptimizer | qiotoolkit",
    "keywords": "Class optimal_learning::NullOptimizer \\rst The \"null\" or identity optimizer: it does nothing, giving the same output its inputs This is useful to allow the multistart optimizer template to be reused for 'dumb' searches and nontrivial optimization. In the former, we just need to evaluate the objective at each initial guess, so there is no optimization to be done at each point (hence null optimizer). In the latter, we kick off an optimization run (e.g., gradient descent, newton) at each initial guess. \\endrst Inheritance optimal_learning::NullOptimizer Constructors NullOptimizer() Declaration optimal_learning::NullOptimizer<ObjectiveFunctionEvaluator_, DomainType_>::NullOptimizer()=default Methods Optimize() \\rst Perform a null optimization: this does nothing. :objective_state[1] a properly configured state object for the ObjectiveFunctionEvaluator template parameter objective_state.GetCurrentPoint() will be used to obtain the initial guess \\output :objective_state[1]: a state object whose temporary data members may have been modified objective_state.GetCurrentPoint() will return the point as the intial guess number of errors, always 0 \\endrst Declaration int optimal_learning::NullOptimizer<ObjectiveFunctionEvaluator_, DomainType_>::Optimize(const ObjectiveFunctionEvaluator&OL_UNUSED(objective_evaluator), const ParameterStruct&OL_UNUSED(parameters), const DomainType&OL_UNUSED(domain), typename ObjectiveFunctionEvaluator::StateType *OL_UNUSED(objective_state)) const noexcept OL_NONNULL_POINTERS OL_PURE_FUNCTION OL_DISALLOW_COPY_AND_ASSIGN() Declaration optimal_learning::NullOptimizer<ObjectiveFunctionEvaluator_, DomainType_>::OL_DISALLOW_COPY_AND_ASSIGN(NullOptimizer)"
  },
  "api/optimal/learning/null-parameters.html": {
    "href": "api/optimal/learning/null-parameters.html",
    "title": "Struct optimal_learning::NullParameters | qiotoolkit",
    "keywords": "Struct optimal_learning::NullParameters \\rst Empty container for optimizers that do not require any parameters (e.g., the null optimizer). \\endrst"
  },
  "api/optimal/learning/one-potential-sample-expected-improvement-evaluator.html": {
    "href": "api/optimal/learning/one-potential-sample-expected-improvement-evaluator.html",
    "title": "Class optimal_learning::OnePotentialSampleExpectedImprovementEvaluator | qiotoolkit",
    "keywords": "Class optimal_learning::OnePotentialSampleExpectedImprovementEvaluator \\rst This is a specialization of the ExpectedImprovementEvaluator class for when the number of potential samples is 1; i.e., num_to_sample == 1 and the number of concurrent samples is 0; i.e. num_being_sampled == 0. In other words, this class only supports the computation of 1,0-EI. In this case, we have analytic formulas for computing EI and its gradient. Thus this class does not perform any explicit numerical integration, nor do its EI functions require access to a random number generator. This class's methods have some parameters that are unused or redundant. This is so that the interface matches that of the more general ExpectedImprovementEvaluator. For other details, see ExpectedImprovementEvaluator for more complete description of what EI is and the outputs of EI and grad EI computations. \\endrst Inheritance optimal_learning::OnePotentialSampleExpectedImprovementEvaluator Constructors OnePotentialSampleExpectedImprovementEvaluator() \\rst Constructs a OnePotentialSampleExpectedImprovementEvaluator object. All inputs are required; no default constructor nor copy/assignment are allowed. :gaussian_process GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the underlying GP :best_so_far: best (minimum) objective function value (in points_sampled_value) \\endrst Declaration optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::OnePotentialSampleExpectedImprovementEvaluator(const GaussianProcess&gaussian_process_in, double best_so_far) OnePotentialSampleExpectedImprovementEvaluator() Declaration optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::OnePotentialSampleExpectedImprovementEvaluator(OnePotentialSampleExpectedImprovementEvaluator&&other) Methods dim() Declaration int optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT best_so_far() Declaration double optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::best_so_far() noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT gaussian_process() Declaration const GaussianProcess* optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::gaussian_process() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT ComputeObjectiveFunction() \\rst Wrapper for ComputeExpectedImprovement(); see that function for details. \\endrst Declaration double optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::ComputeObjectiveFunction(StateType *ei_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradObjectiveFunction() \\rst Wrapper for ComputeGradExpectedImprovement(); see that function for details. \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::ComputeGradObjectiveFunction(StateType *ei_state, double *restrict grad_EI) const OL_NONNULL_POINTERS ComputeExpectedImprovement() \\rst Computes the expected improvement EI(Xs) = E_n[[f^*_n(X) - min(f(Xs_1),...,f(Xs_m))]^+] Uses analytic formulas to evaluate the expected improvement. :ei_state[1] properly configured state object \\output :ei_state[1]: state with temporary storage modified the expected improvement from sampling point_to_sample \\endrst Declaration double optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::ComputeExpectedImprovement(StateType *ei_state) const ComputeGradExpectedImprovement() \\rst Computes the (partial) derivatives of the expected improvement with respect to the point to sample. Uses analytic formulas to evaluate the spatial gradient of the expected improvement. :ei_state[1] properly configured state object \\output :ei_state[1]: state with temporary storage modified :grad_EI[dim]: gradient of EI, \\pderiv{EI(x)}{x_d}, where x is points_to_sample \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::ComputeGradExpectedImprovement(StateType *ei_state, double *restrict grad_EI) const OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::OnePotentialSampleExpectedImprovementEvaluator::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(OnePotentialSampleExpectedImprovementEvaluator)"
  },
  "api/optimal/learning/one-potential-sample-expected-improvement-mcmcevaluator.html": {
    "href": "api/optimal/learning/one-potential-sample-expected-improvement-mcmcevaluator.html",
    "title": "Class optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator | qiotoolkit",
    "keywords": "Class optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator \\rst A class to encapsulate the computation of knowledge gradient and its spatial gradient. This class handles the general KG computation case using monte carlo integration; it can support q,p-KG optimization. It is designed to work with any GaussianProcess. Additionally, this class has no state and within the context of KG optimization, it is meant to be accessed by const reference only. The random numbers needed for KG computation will be passed as parameters instead of contained as members to make multithreading more straightforward. \\endrst Inheritance optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator Constructors OnePotentialSampleExpectedImprovementMCMCEvaluator() \\rst Constructs a KnowledgeGradientEvaluator object. All inputs are required; no default constructor nor copy/assignment are allowed. :gaussian_process GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the underlying GP :discrete_pts[dim][num_pts]: the set of points to approximate the KG factor :num_pts: number of points in discrete_pts :num_mc_iterations: number of monte carlo iterations :best_so_far: best (minimum) objective function value (in points_sampled_value) \\endrst Declaration optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::OnePotentialSampleExpectedImprovementMCMCEvaluator(const GaussianProcessMCMC&gaussian_process_mcmc, double const *best_so_far, std::vector<typename OnePotentialSampleExpectedImprovementState::EvaluatorType>*evaluator_vector) Methods dim() Declaration int optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_mcmc() Declaration int optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::num_mcmc() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT expected_improvement_evaluator_list() Declaration std::vector<OnePotentialSampleExpectedImprovementEvaluator>* optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::expected_improvement_evaluator_list() const noexcept OL_WARN_UNUSED_RESULT best_so_far_list() Declaration std::vector<double>optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::best_so_far_list(double const *best_so_far) const noexcept OL_WARN_UNUSED_RESULT ComputeObjectiveFunction() \\rst Wrapper for ComputeKnowledgeGradient(); see that function for details. \\endrst Declaration double optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::ComputeObjectiveFunction(StateType *ei_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradObjectiveFunction() \\rst Wrapper for ComputeGradKnowledgeGradient(); see that function for details. \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::ComputeGradObjectiveFunction(StateType *ei_state, double *restrict grad_EI) const OL_NONNULL_POINTERS ComputeExpectedImprovement() \\rst Computes the knowledge gradient :kg_state[1] properly configured state object \\output :kg_state[1]: state with temporary storage modified; normal_rng modified the knowledge gradient from sampling points_to_sample with points_being_sampled concurrent experiments \\endrst \\rst Compute Knowledge Gradient This version requires the discretization of A (the feasibe domain). The discretization usually is: some set + points previous sampled + points being sampled + points to sample \\endrst Declaration double optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::ComputeExpectedImprovement(StateType *ei_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradExpectedImprovement() \\rst Computes the (partial) derivatives of the knowledge gradient with respect to each point of points_to_sample. As with ComputeKnowledgeGradient(), this computation accounts for the effect of points_being_sampled concurrent experiments. points_to_sample is the \"q\" and points_being_sampled is the \"p\" in q,p-KG. :kg_state[1] properly configured state object \\output :kg_state[1]: state with temporary storage modified; normal_rng modified :grad_KG[dim][num_to_sample]: gradient of KG, \\pderiv{KG(Xq \\cup Xp)}{Xq_{d,i}} where Xq is points_to_sample and Xp is points_being_sampled (grad KG from sampling points_to_sample with points_being_sampled concurrent experiments wrt each dimension of the points in points_to_sample) \\endrst \\rst Computes gradient of KG (see KnowledgeGradientEvaluator::ComputeGradKnowledgeGradient) wrt points_to_sample (stored in union_of_points[0:num_to_sample]). Mechanism is similar to the computation of KG, where points' contributions to the gradient are thrown out of their corresponding improvement <= 0.0. Thus \\nabla(\\mu) only contributes when the winner (point w/best improvement this iteration) is the current point. That is, the gradient of \\mu at x_i wrt x_j is 0 unless i == j (and only this result is stored in kg_state->grad_mu). The interaction with kg_state->grad_chol_decomp is harder to know a priori (like with grad_mu) and has a more complex structure (rank 3 tensor), so the derivative wrt x_j is computed fully, and the relevant submatrix (indexed by the current winner) is accessed each iteration. .. Note:: comments here are copied to _compute_grad_knowledge_gradient_monte_carlo() in python_version/knowledge_gradient.py \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::ComputeGradExpectedImprovement(StateType *ei_state, double *restrict grad_EI) const OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::OnePotentialSampleExpectedImprovementMCMCEvaluator::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(OnePotentialSampleExpectedImprovementMCMCEvaluator)"
  },
  "api/optimal/learning/one-potential-sample-expected-improvement-mcmcstate.html": {
    "href": "api/optimal/learning/one-potential-sample-expected-improvement-mcmcstate.html",
    "title": "Struct optimal_learning::OnePotentialSampleExpectedImprovementMCMCState | qiotoolkit",
    "keywords": "Struct optimal_learning::OnePotentialSampleExpectedImprovementMCMCState \\rst State object for KnowledgeGradientEvaluator. This tracks the points being sampled in concurrent experiments (points_being_sampled) ALONG with the points currently being evaluated via knowledge gradient for future experiments (called points_to_sample); these are the p and q of q,p-KG, respectively. points_to_sample joined with points_being_sampled is stored in union_of_points in that order. This struct also tracks the state of the GaussianProcess that underlies the knowledge gradient computation: the GP state is built to handle the initial union_of_points, and subsequent updates to points_to_sample in this object also update the GP state. This struct also holds a pointer to a random number generator needed for Monte Carlo integrated KG computations. .. WARNING:: Users MUST guarantee that multiple state objects DO NOT point to the same RNG (in a multithreaded env). See general comments on State structs in gpp_common.hpp's header docs. \\endrst Constructors OnePotentialSampleExpectedImprovementMCMCState() \\rst Constructs an KnowledgeGradientMCMCState object with a specified source of randomness for the purpose of computing KG (and its gradient) over the specified set of points to sample. This establishes properly sized/initialized temporaries for KG computation, including dependent state from the associated Gaussian Process (which arrives as part of the kg_evaluator). .. WARNING:: This object is invalidated if the associated kg_evaluator is mutated. SetupState() should be called to reset. .. WARNING:: Using this object to compute gradients when configure_for_gradients := false results in UNDEFINED BEHAVIOR. :kg_evaluator knowledge gradient evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: points at which to evaluate KG and/or its gradient to check their value in future experiments (i.e., test points for GP predictions) :points_being_sampled[dim][num_being_sampled]: points being sampled in concurrent experiments :num_to_sample: number of potential future samples; gradients are evaluated wrt these points (i.e., the \"q\" in q,p-KG) :num_being_sampled: number of points being sampled in concurrent experiments (i.e., the \"p\" in q,p-KG) :configure_for_gradients: true if this object will be used to compute gradients, false otherwise :normal_rng[1]: pointer to a properly initialized* NormalRNG object .. NOTE:: The NormalRNG object must already be seeded. If multithreaded computation is used for KG, then every state object must have a different NormalRNG (different seeds, not just different objects). \\endrst Declaration optimal_learning::OnePotentialSampleExpectedImprovementMCMCState::OnePotentialSampleExpectedImprovementMCMCState(const EvaluatorType&ei_evaluator, double const *restrict point_to_sample, double const *restrict OL_UNUSED(points_being_sampled), int OL_UNUSED(num_to_sample_in), int OL_UNUSED(num_being_sampled_in), bool configure_for_gradients, NormalRNGInterface *OL_UNUSED(normal_rng_in), std::vector<typename OnePotentialSampleExpectedImprovementEvaluator::StateType>*ei_state_vector) OnePotentialSampleExpectedImprovementMCMCState() Declaration optimal_learning::OnePotentialSampleExpectedImprovementMCMCState::OnePotentialSampleExpectedImprovementMCMCState(OnePotentialSampleExpectedImprovementMCMCState&&other) Methods GetProblemSize() Declaration int optimal_learning::OnePotentialSampleExpectedImprovementMCMCState::GetProblemSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetCurrentPoint() \\rst Get the points_to_sample: potential future samples whose KG (and/or gradients) are being evaluated \\output :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementMCMCState::GetCurrentPoint(double *restrict points_to_sample) const noexcept OL_NONNULL_POINTERS SetCurrentPoint() \\rst Change the potential samples whose KG (and/or gradient) are being evaluated. Update the state's derived quantities to be consistent with the new points. :kg_evaluator expected improvement evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementMCMCState::SetCurrentPoint(const EvaluatorType&kg_evaluator, double const *restrict points_to_sample_in) OL_NONNULL_POINTERS SetupState() \\rst Configures this state object with new points_to_sample, the location of the potential samples whose KG is to be evaluated. Ensures all state variables & temporaries are properly sized. Properly sets all dependent state variables (e.g., GaussianProcess's state) for KG evaluation. .. WARNING:: This object's state is INVALIDATED if the kg_evaluator (including the GaussianProcess it depends on) used in SetupState is mutated! SetupState() should be called again in such a situation. :kg_evaluator knowledge gradient evaluator object that specifies the parameters & GP for KG evaluation :points_to_sample[dim][num_to_sample]: potential future samples whose KG (and/or gradients) are being evaluated \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementMCMCState::SetupState(const EvaluatorType&ei_evaluator, double const *restrict points_to_sample) OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::OnePotentialSampleExpectedImprovementMCMCState::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(OnePotentialSampleExpectedImprovementMCMCState)"
  },
  "api/optimal/learning/one-potential-sample-expected-improvement-state.html": {
    "href": "api/optimal/learning/one-potential-sample-expected-improvement-state.html",
    "title": "Struct optimal_learning::OnePotentialSampleExpectedImprovementState | qiotoolkit",
    "keywords": "Struct optimal_learning::OnePotentialSampleExpectedImprovementState \\rst State object for OnePotentialSampleExpectedImprovementEvaluator. This tracks the ONE point_to_sample being evaluated via expected improvement. This is just a special case of ExpectedImprovementState; see those class docs for more details. See general comments on State structs in gpp_common.hpp's header docs. \\endrst Constructors OnePotentialSampleExpectedImprovementState() \\rst Constructs an OnePotentialSampleExpectedImprovementState object for the purpose of computing EI (and its gradient) over the specified point to sample. This establishes properly sized/initialized temporaries for EI computation, including dependent state from the associated Gaussian Process (which arrives as part of the ei_evaluator). .. WARNING:: This object is invalidated if the associated ei_evaluator is mutated. SetupState() should be called to reset. .. WARNING:: Using this object to compute gradients when configure_for_gradients := false results in UNDEFINED BEHAVIOR. :ei_evaluator expected improvement evaluator object that specifies the parameters & GP for EI evaluation :point_to_sample[dim]: point at which to evaluate EI and/or its gradient to check their value in future experiments (i.e., test point for GP predictions) :configure_for_gradients: true if this object will be used to compute gradients, false otherwise \\endrst Declaration optimal_learning::OnePotentialSampleExpectedImprovementState::OnePotentialSampleExpectedImprovementState(const EvaluatorType&ei_evaluator, double const *restrict point_to_sample_in, bool configure_for_gradients) OnePotentialSampleExpectedImprovementState() \\rst Constructor wrapper to match the signature of the ctor for ExpectedImprovementState(). \\endrst Declaration optimal_learning::OnePotentialSampleExpectedImprovementState::OnePotentialSampleExpectedImprovementState(const EvaluatorType&ei_evaluator, double const *restrict points_to_sample, double const *restrict OL_UNUSED(points_being_sampled), int OL_UNUSED(num_to_sample_in), int OL_UNUSED(num_being_sampled_in), bool configure_for_gradients, NormalRNGInterface *OL_UNUSED(normal_rng_in)) OnePotentialSampleExpectedImprovementState() Declaration optimal_learning::OnePotentialSampleExpectedImprovementState::OnePotentialSampleExpectedImprovementState(OnePotentialSampleExpectedImprovementState&&other) Methods GetProblemSize() Declaration int optimal_learning::OnePotentialSampleExpectedImprovementState::GetProblemSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetCurrentPoint() \\rst Get point_to_sample: the potential future sample whose EI (and/or gradients) is being evaluated \\output :point_to_sample[dim]: potential sample whose EI is being evaluted \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementState::GetCurrentPoint(double *restrict point_to_sample_out) const noexcept OL_NONNULL_POINTERS SetCurrentPoint() \\rst Change the potential sample whose EI (and/or gradient) is being evaluated. Update the state's derived quantities to be consistent with the new point. :ei_evaluator expected improvement evaluator object that specifies the parameters & GP for EI evaluation :point_to_sample[dim]: potential future sample whose EI (and/or gradients) is being evaluated \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementState::SetCurrentPoint(const EvaluatorType&ei_evaluator, double const *restrict point_to_sample_in) OL_NONNULL_POINTERS SetupState() \\rst Configures this state object with a new point_to_sample, the location of the potential sample whose EI is to be evaluated. Ensures all state variables & temporaries are properly sized. Properly sets all dependent state variables (e.g., GaussianProcess's state) for EI evaluation. .. WARNING:: This object's state is INVALIDATED if the ei_evaluator (including the GaussianProcess it depends on) used in SetupState is mutated! SetupState() should be called again in such a situation. :ei_evaluator expected improvement evaluator object that specifies the parameters & GP for EI evaluation :point_to_sample[dim]: potential future sample whose EI (and/or gradients) is being evaluated \\endrst Declaration void optimal_learning::OnePotentialSampleExpectedImprovementState::SetupState(const EvaluatorType&ei_evaluator, double const *restrict point_to_sample_in) OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::OnePotentialSampleExpectedImprovementState::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(OnePotentialSampleExpectedImprovementState)"
  },
  "api/optimal/learning/optimal-learning-exception.html": {
    "href": "api/optimal/learning/optimal-learning-exception.html",
    "title": "Class optimal_learning::OptimalLearningException | qiotoolkit",
    "keywords": "Class optimal_learning::OptimalLearningException \\rst Overview** Exception to handle general runtime errors (e.g., not fitting into other exception types). Subclasses std::exception. Serves as the superclass for all other custom exceptions in the optimal_learning library. This class is essentially the same as std::runtime_error but it includes a ctor with some extra logic for formatting the error message. Holds only a std::string containing the message produced by what(). .. Note: exceptions from std::string operations (e.g., std::bad_alloc) will cause std::terminate(). Message Format** The what() message is formatted in the class ctor (capitals indicate variable information):: R\"%%( OptimalLearningException: CUSTOM_MESSAGE FUNCTION_NAME FILE_LINE_INFO )%%\" This format should be overriden by subclasses (at the minimum showing a different exception name). \\endrst Inheritance optimal_learning::OptimalLearningException optimal_learning::BoundsException optimal_learning::InvalidValueException optimal_learning::SingularMatrixException Constructors OptimalLearningException() \\rst Constructs a OptimalLearningException with the specified message. :line_info[] ptr to char array containing FILE and LINE info; e.g., from OL_STRINGIFY_FILE_AND_LINE :func_info[]: optional ptr to char array from OL_CURRENT_FUNCTION_NAME or similar :custom_message[]: optional ptr to char array with any additional text/info to print/log \\endrst Declaration optimal_learning::OptimalLearningException::OptimalLearningException(char const *line_info, char const *func_info, char const *custom_message) OptimalLearningException() Declaration optimal_learning::OptimalLearningException::OptimalLearningException()=delete OptimalLearningException() \\rst Constructs a OptimalLearningException with the specified name. This is used by subclasses to override the class name in the message text. :name[] the exception name to write into the message \\endrst Declaration optimal_learning::OptimalLearningException::OptimalLearningException(char const *name) Methods what() \\rst Provides a C-string containing information about the conditions of the exception. See: http://en.cppreference.com/w/cpp/error/exception C-style char string describing the exception. \\endrst Declaration virtual const char* optimal_learning::OptimalLearningException::what() const noexcept override OL_WARN_UNUSED_RESULT AppendCustomMessageAndDebugInfo() \\rst Utility function to append some additional info (file/line number, function name, and/or a custom message) to a specified string. This is meant to be used for constructing what() messages for the exception classes in gpp_exception.hpp. :line_info[] ptr to char array containing FILE and LINE info; e.g., from OL_STRINGIFY_FILE_AND_LINE :func_info[]: optional ptr to char array from OL_CURRENT_FUNCTION_NAME or similar :custom_message[]: optional ptr to char array with any additional text/info to print/log \\endrst Declaration void optimal_learning::OptimalLearningException::AppendCustomMessageAndDebugInfo(char const *line_info, char const *func_info, char const *custom_message)"
  },
  "api/optimal/learning/optimization-iocontainer.html": {
    "href": "api/optimal/learning/optimization-iocontainer.html",
    "title": "Struct optimal_learning::OptimizationIOContainer | qiotoolkit",
    "keywords": "Struct optimal_learning::OptimizationIOContainer \\rst This object holds the input/output fields for optimizers (maximization). On input, this can be used to specify the current best known point (i.e., the optimizer will indicate no new optima found if it cannot beat this value). Upon completion, this struct should be read to determine the result of optimization. Since this object is used to communicate some inputs/outputs to the various optimization functions, any function using this object MUST obey its contract. The contract: On input, the optimizer will read best_objective_value_so_far. IF optimization results in a LARGER objective value, then: best_objective_value_so_far will be set to that new larger value best_point will be set to the point producing this new larger objective value found_flag will be SET to true ELSE: best_objective_value_so_far will be unmodified best_point will be unmodified found_flag will be SET to false The idea is for the user to be able to indicate what an improvement is. For example, to optimize log likelihood as a function of hyperparameters, we could do: best_point = argmax_{x \\in initial_guesses} f(x) best_objective_value = f(best_point) And then call multistart gradient descent (MGD). Now, MGD will only change the best point/value if it converges to a better solution. If convergence fails or MGD settles on a worse local maxima, found_flag will be SET to false, and the other fields of IOContainer will be unmodified. If it finds a better solution, then found_flag will be SET to true and the other fields will report the new solution. \\endrst Constructors OptimizationIOContainer() \\rst Build an empty OptimizationIOContainer. best_objective_value and best_point are initialized to zero; THIS MAY BE AN INVALID STATE. See class docs for details. :problem_size number of dimensions in the optimization problem (e.g., size of best_point) \\endrst Declaration optimal_learning::OptimizationIOContainer::OptimizationIOContainer(int problem_size_in) OptimizationIOContainer() \\rst Build and fully initialize a OptimizationIOContainer. See class docs for details. :problem_size number of dimensions in the optimization problem (e.g., size of best_point) :best_objective_value: the best objective function value seen so far :best_point: the point to associate with best_objective_value \\endrst Declaration optimal_learning::OptimizationIOContainer::OptimizationIOContainer(int problem_size_in, double best_objective_value, double const *restrict best_point_in) OptimizationIOContainer() Declaration optimal_learning::OptimizationIOContainer::OptimizationIOContainer(OptimizationIOContainer&&OL_UNUSED(other))=default Methods OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::OptimizationIOContainer::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(OptimizationIOContainer)"
  },
  "api/optimal/learning/pingable-matrix-input-vector-output-interface.html": {
    "href": "api/optimal/learning/pingable-matrix-input-vector-output-interface.html",
    "title": "Class optimal_learning::PingableMatrixInputVectorOutputInterface | qiotoolkit",
    "keywords": "Class optimal_learning::PingableMatrixInputVectorOutputInterface \\rst Class to enable numerical and analytic differentiation of functions of the form: f_{k} = f(X_{d,i}) with derivatives taken wrt each member of X_{d,i}, gradf_{k,d,i} = \\frac{\\partial f_k}{\\partial X_{d,i}} In the nomenclature used in the class: d indexes over num_rows (set in GetInputSizes()) i indexes over num_cols (set in GetInputSizes()) k indexes over GetOutputSize() Typically d is the spatial_dimension of the problem. So if i ranges over 1 .. num_points, then X_{d,i} is a matrix of num_points points each with dimension spatial_dim. And k refers to num_outputs. X_{d,i} can of course be any arbitrary matrix; d, i need not refer to spatial dimension and num_points. But that is currently the most common use case. This class enables easy pinging of a multitude of f, X combinations. Since it abstracts away indexing, it does not limit how implementations store/compute f() and its gradient. Generally, usage goes as follows: Use GetInputSizes(), GetOutputSize(), and possibly GetGradientsSize() to inspect the dimensions of the problem EvaluateAndStoreAnalyticGradient(): compute and internally store the gradient evaluated at a given input* GetAnalyticGradient: returns the value of the analytic gradient for a given output (k), wrt a given point (d,i) EvaluateFunction: returns all outputs of the function for a given input It is not necessary to fully evaluate the gradient here. Instead, the input point can be stored and evaluation can happen on-the-fly in GetAnalyticGradient() if desired. So to ping a derivative, you can: f_p = EvaluateFunction(X + h), f_m = EvaluateFunction(X - h) Compare: (f_p - f_m)/(2h) to GetAnalyticGradient See PingDerivative() docs for more details. \\endrst Inheritance optimal_learning::PingableMatrixInputVectorOutputInterface Constructors PingableMatrixInputVectorOutputInterface() Declaration optimal_learning::PingableMatrixInputVectorOutputInterface::PingableMatrixInputVectorOutputInterface()=default Methods GetInputSizes() \\rst Number of rows and columns of the input, X_{d,i}, to f(). For example, the input might be a N_d X N_i matrix, points_to_sample, where N_d = spatial dimension (rows) and N_i = number of points (columns). \\output :num_rows[1]: the number of rows of the input matrix X :num_cols[1]: the number of columns of the input matrix X \\endrst Declaration virtual void optimal_learning::PingableMatrixInputVectorOutputInterface::GetInputSizes(int *num_rows, int *num_cols) const noexcept OL_NONNULL_POINTERS=0 GetOutputSize() \\rst Number of outputs of the function f_k = f(X_{d,i}); i.e., length(f_k) The number of entries in f_k aka number of outputs of f() \\endrst Declaration virtual int optimal_learning::PingableMatrixInputVectorOutputInterface::GetOutputSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT=0 GetGradientsSize() \\rst Number of entries in the gradient of the output wrt each entry of the input. This should generally not be used unless you require direct access to the analytic gradient. MUST be num_rows*num_cols*GetOutputSize() invalid memory read/writes may occur \\endrst Declaration virtual int optimal_learning::PingableMatrixInputVectorOutputInterface::GetGradientsSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT=0 EvaluateAndStoreAnalyticGradient() \\rst Setup so that GetAnalyticGradient(row, column, output) will be able to return gradf[row][column][output] evaluated at X, \"input_matrix.\" Typically this will entail computing and storing the analytic gradient. But the only thing that needs to be saved is the contents of input_matrix for later access. MUST BE CALLED before using GetAnalyticGradient! :input_matrix[num_rows][num_cols] the input, X_{d,i} \\output :gradients[num_rows][num_cols][num_outputs]: filled with the gradient evaluated at input_matrix. Ignored if nullptr. IMPLEMENTATION NOT REQUIRED. \\endrst Declaration virtual void optimal_learning::PingableMatrixInputVectorOutputInterface::EvaluateAndStoreAnalyticGradient(double const *restrict input_matrix, double *restrict gradients) noexcept OL_NONNULL_POINTERS_LIST(2)=0 GetAnalyticGradient() \\rst The gradients are indexed by: dA[input_row][input_column][output_index], where row, column index the input matrix and output_index indexes the output. Returns the gradient computed/stored by EvaluateAndStoreAnalyticGradient(). :row_index row_index (d) of the input to be differentiated with respect to :column_index: column_index (i) of the input to be differentiated with respect to :output_index: (flat) index into the output The [row_index][column_index][output_index]'th entry of the analytic gradient evaluated at input_matrix (where input matrix was specified in EvaluateAndStoreAnalyticGradient()). \\endrst Declaration virtual double optimal_learning::PingableMatrixInputVectorOutputInterface::GetAnalyticGradient(int row_index, int column_index, int output_index) const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT=0 EvaluateFunction() \\rst Evalutes f_k = f(X_{d,i}). X_{d,i} is the \"input_matrix\" and f_k is in \"function_values.\" :input_matrix[num_rows][num_cols] the matrix of inputs \\output :function_values[num_outputs]: vector of outputs of f() \\endrst Declaration virtual void optimal_learning::PingableMatrixInputVectorOutputInterface::EvaluateFunction(double const *restrict input_matrix, double *restrict function_values) const noexcept OL_NONNULL_POINTERS=0 OL_DISALLOW_COPY_AND_ASSIGN() Declaration optimal_learning::PingableMatrixInputVectorOutputInterface::OL_DISALLOW_COPY_AND_ASSIGN(PingableMatrixInputVectorOutputInterface) ~PingableMatrixInputVectorOutputInterface() Declaration virtual optimal_learning::PingableMatrixInputVectorOutputInterface::~PingableMatrixInputVectorOutputInterface()=default"
  },
  "api/optimal/learning/plane.html": {
    "href": "api/optimal/learning/plane.html",
    "title": "Struct optimal_learning::Plane | qiotoolkit",
    "keywords": "Struct optimal_learning::Plane \\rst We specify a plane in dim-space using the Hesse (or Hessian) Normal Form: http://mathworld.wolfram.com/HessianNormalForm.html The equation of plane requires dim + 1 real numbers: \\ms a_0 + \\sum_{i=0}^{dim} n_i x_i = 0\\me Hence we can describe any plane as a vector, \\ms [n_0, n_2, ..., n_{dim-1}]\\me, and a real number, \\ms a_0\\me. Let \\ms n_{vec} = [n_0, ..., n_{dim-1}]\\me be the (outward) unit normal vector. By convention, \\ms |n_{vec}|2 = 1\\me. \\ms a_0\\me is the signed distance to the origin. This is the distance from the plane to the origin in the direction of \\ms n{vec}\\me. Put another way, \\ms a_0\\me is positive if the origin is in the same half-space \"pointed to\" by \\ms n_{vec}\\me and negative otherwise. Note: \\ms a_0\\me is measured in units of \\ms |n_{vec}|\\me, so if it is not an unit vector, that is analogous to scaling \\ms a_0\\nme. As an example, let's consider 4 planes with dim = 2: \\ms a_0\\me = -1, and \\ms n_{vec}\\me = { 1.0, 0.0}: the plane x = 1 with rightward pointing normal. \\ms a_0\\me = -1, and \\ms n_{vec}\\me = {-1.0, 0.0}: the plane x = -1 with leftward pointing normal. \\ms a_0\\me = 1, and \\ms n_{vec}\\me = { 1.0, 0.0}: the plane x = -1 with rightward pointing normal. \\ms a_0\\me = 1, and \\ms n_{vec}\\me = {-1.0, 0.0}: the plane x = 1 with leftward pointing normal. Be careful with the signs. Another common way of specifying a plane is via a point \\ms x_0\\me and an unit normal, \\ms n_{vec}\\me. A point x is in the plane if and only if \\ms (x-x_0) \\cdot n_{vec} = 0\\me. Since \\ms x_0\\me is constant, we can precompute and store \\ms ms x_0 \\cdot n_{vec} = -a_0\\me, yielding: \\ms x \\cdot n_{vec} - x_0 \\cdot n_{vec} = x \\cdot n_{vec} + a_0\\me, which is our original equation of a plane. \\endrst Constructors Plane() Declaration optimal_learning::Plane::Plane()=delete Plane() \\rst Creates a zero-initialized plane object with enough space for dim-dimensions. .. NOTE:: This plane is invalid (unit_normal := zero is not a unit vector) and needs to have its members initialized. That said, no member functions will fail even without complete initialization. :dim the number of spatial dimensions \\endrst Declaration optimal_learning::Plane::Plane(int dim_in) Plane() \\rst Creates a plane in dim-dimensions with the specified unit normal (\\ms n_i\\me) and offset (\\ms a_0\\me): \\ms a_0 + \\sum_{i=0}^{dim} n_i * x_i = 0\\me .. NOTE:: Failure to specify a unit normal will result in surprising behavior. \\ms a_0\\me is really in units of \\ms|n_{vec}|\\me, so if \\ms|n_{vec}| = 3.5\\me, then the actual distance to the origin is \\ms 3.5 a_0\\me. :dim the number of spatial dimensions :unit_normal[dim]: the unit normal vector. VectorNorm(unit_normal, dim) must be 1.0 :offset: a_0, the signed distance to the origin (see class docs) \\endrst Declaration optimal_learning::Plane::Plane(double const *restrict unit_normal_in, double offset_in, int dim_in) OL_NONNULL_POINTERS Plane() \\rst Creates a plane in dim-dimensions that contains \"point,\" with the specified unit normal. :dim the number of spatial dimensions :unit_normal[dim]: the unit normal vector. VectorNorm(unit_normal, dim) must be 1.0 :point[dim]: a point contained in the plane \\endrst Declaration optimal_learning::Plane::Plane(double const *restrict unit_normal_in, double const *restrict point, int dim_in) OL_NONNULL_POINTERS Methods dim() \\rst the number of spatial dimensions (that this plane lives in) \\endrst Declaration int optimal_learning::Plane::dim() const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT OrthogonalDistanceToPoint() \\rst Computes the signed, shortest distance from this plane to point: positive means the point is in the half-space determined by the direction of unit_normal. .. Note:: if point is the origin, this yields precisely \\ms a_0\\me (offset). :point[dim] point to compute distance to signed, shortest distance from this plane to point, where positive means the point and normal are in the same half-space \\endrst Declaration double optimal_learning::Plane::OrthogonalDistanceToPoint(double const *restrict point) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT OrthogonalProjectionOntoPlane() \\rst Projects a point (orthogonally) onto a plane; i.e., finds the point on the plane that is closest to the input point. :point[dim] point to project onto plane \\output :point[dim]: point projected onto plane \\endrst Declaration void optimal_learning::Plane::OrthogonalProjectionOntoPlane(double *restrict point) const OL_NONNULL_POINTERS DistanceToPlaneAlongVector() \\rst Computes the signed distance from the specified point to this plane along the specified vector. This result is computed in units of \\ms|vector|_2\\me. That is, a distance of 3.14 means if we compute: new_point = 3.14*vector + point, then new_point will be on this plane. A negative distance means the plane is \"behind\" the ray. :point[dim] point to compute distance from :vector[dim]: vector to compute distance along Signed distance along the given vector; positive means the intersection is in the same direction as the vector. This result is in units of \\ms|vector|_2\\me; normalize vector if you want an actual distance. \\endrst Declaration double optimal_learning::Plane::DistanceToPlaneAlongVector(double const *restrict point, double const *restrict vector) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT"
  },
  "api/optimal/learning/points-to-sample-state.html": {
    "href": "api/optimal/learning/points-to-sample-state.html",
    "title": "Struct optimal_learning::PointsToSampleState | qiotoolkit",
    "keywords": "Struct optimal_learning::PointsToSampleState \\rst This object holds the state needed for a GaussianProcess object characterize the distribution of function values arising from sampling the GP at a list of points_to_sample. This object is required by the GaussianProcess to access functionality for computing the mean, variance, and spatial gradients thereof. The \"independent variables\" for this object are points_to_sample. These points are both the \"p\" and the \"q\" in q,p-EI; i.e., they are the parameters of both ongoing experiments and new predictions. Recall that in q,p-EI, the q points are called points_to_sample and the p points are called points_being_sampled. Here, we need to make predictions about both point sets with the GP, so we simply call the union of point sets points_to_sample. In GP computations, there is really no distinction between the \"q\" and \"p\" points from EI, points_to_sample and points_being_sampled, respectively. However, in EI optimization, we only need gradients of GP quantities wrt points_to_sample, so users should build PointsToSampleState() with num_derivatives = num_to_sample. Once constructed, this object provides the SetupState() function to update it for computations at different sets of potential points to sample. See general comments on State structs in gpp_common.hpp's header docs. \\endrst Constructors PointsToSampleState() \\rst Constructs a PointsToSampleState object with new points_to_sample. Ensures all state variables & temporaries are properly sized. Properly sets all state variables so that GaussianProcess's mean, variance (and gradients thereof) functions can be called. .. WARNING:: This object's state is INVALIDATED if the gaussian_process used in construction is mutated! SetupState() should be called again in such a situation. .. WARNING:: Using this object to compute gradients when num_derivatives := 0 results in UNDEFINED BEHAVIOR. :gaussian_process GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the underlying GP :points_to_sample[dim][num_to_sample]: points at which to compute GP-derived quantities (mean, variance, etc.) :num_to_sample: number of points being sampled concurrently :num_derivatives: configure this object to compute num_derivatives derivative terms wrt points_to_sample[:][0:num_derivatives]; 0 means no gradient computation will be performed. \\endrst Declaration optimal_learning::PointsToSampleState::PointsToSampleState(const GaussianProcess&gaussian_process, double const *restrict points_to_sample_in, int num_to_sample_in, int const *restrict gradients_in, int num_gradients_in, int num_derivatives_in, bool precomputed_in=true, bool precomputed_grad_K_inv_times_K_star_in=false) PointsToSampleState() Declaration optimal_learning::PointsToSampleState::PointsToSampleState(PointsToSampleState&&other) Methods SetupState() \\rst Configures this object with new points_to_sample. Ensures all state variables & temporaries are properly sized. Properly sets all state variables so that GaussianProcess's mean, variance (and gradients thereof) functions can be called. .. WARNING:: This object's state is INVALIDATED if the gaussian_process used in SetupState is mutated! SetupState() should be called again in such a situation. .. WARNING:: Using this object to compute gradients when num_derivatives := 0 results in UNDEFINED BEHAVIOR. :gaussian_process GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the underlying GP :points_to_sample[dim][num_to_sample]: points at which to compute GP-derived quantities (mean, variance, etc.) :num_to_sample: number of points being sampled concurrently :num_derivatives: configure this object to compute num_derivatives derivative terms wrt points_to_sample[:][0:num_derivatives]; 0 means no gradient computation will be performed. \\endrst Declaration void optimal_learning::PointsToSampleState::SetupState(const GaussianProcess&gaussian_process, double const *restrict points_to_sample_in, int num_to_sample_in, int num_gradients_in, int num_derivatives_in, bool precomputed_in=true, bool precomputed_grad_K_inv_times_K_star_in=false) OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() the gradient of covariance(x_1, x_2) wrt x_1 Declaration optimal_learning::PointsToSampleState::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(PointsToSampleState)"
  },
  "api/optimal/learning/posterior-mean-evaluator.html": {
    "href": "api/optimal/learning/posterior-mean-evaluator.html",
    "title": "Class optimal_learning::PosteriorMeanEvaluator | qiotoolkit",
    "keywords": "Class optimal_learning::PosteriorMeanEvaluator \\rst This is a specialization of the ExpectedImprovementEvaluator class for when the number of potential samples is 1; i.e., num_to_sample == 1 and the number of concurrent samples is 0; i.e. num_being_sampled == 0. In other words, this class only supports the computation of 1,0-EI. In this case, we have analytic formulas for computing EI and its gradient. Thus this class does not perform any explicit numerical integration, nor do its EI functions require access to a random number generator. This class's methods have some parameters that are unused or redundant. This is so that the interface matches that of the more general ExpectedImprovementEvaluator. For other details, see ExpectedImprovementEvaluator for more complete description of what EI is and the outputs of EI and grad EI computations. \\endrst Inheritance optimal_learning::PosteriorMeanEvaluator Constructors PosteriorMeanEvaluator() \\rst Constructs a OnePotentialSampleExpectedImprovementEvaluator object. All inputs are required; no default constructor nor copy/assignment are allowed. :gaussian_process GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the underlying GP :best_so_far: best (minimum) objective function value (in points_sampled_value) \\endrst Declaration optimal_learning::PosteriorMeanEvaluator::PosteriorMeanEvaluator(const GaussianProcess&gaussian_process_in) Methods dim() Declaration int optimal_learning::PosteriorMeanEvaluator::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT gaussian_process() Declaration const GaussianProcess* optimal_learning::PosteriorMeanEvaluator::gaussian_process() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT ComputeObjectiveFunction() \\rst Wrapper for ComputeExpectedImprovement(); see that function for details. \\endrst Declaration double optimal_learning::PosteriorMeanEvaluator::ComputeObjectiveFunction(StateType *ps_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT ComputeGradObjectiveFunction() \\rst Wrapper for ComputeGradExpectedImprovement(); see that function for details. \\endrst Declaration void optimal_learning::PosteriorMeanEvaluator::ComputeGradObjectiveFunction(StateType *ps_state, double *restrict grad_PS) const OL_NONNULL_POINTERS ComputePosteriorMean() \\rst Computes the expected improvement EI(Xs) = E_n[[f^*_n(X) - min(f(Xs_1),...,f(Xs_m))]^+] Uses analytic formulas to evaluate the expected improvement. :ei_state[1] properly configured state object \\output :ei_state[1]: state with temporary storage modified the expected improvement from sampling point_to_sample \\endrst \\rst Uses analytic formulas to compute EI when num_to_sample = 1 and num_being_sampled = 0 (occurs only in 1,0-EI). In this case, the single-parameter (posterior) GP is just a Gaussian. So the integral in EI (previously eval'd with MC) can be computed 'exactly' using high-accuracy routines for the pdf & cdf of a Gaussian random variable. See Ginsbourger, Le Riche, and Carraro. \\endrst Declaration double optimal_learning::PosteriorMeanEvaluator::ComputePosteriorMean(StateType *ps_state) const ComputeGradPosteriorMean() \\rst Computes the (partial) derivatives of the expected improvement with respect to the point to sample. Uses analytic formulas to evaluate the spatial gradient of the expected improvement. :ei_state[1] properly configured state object \\output :ei_state[1]: state with temporary storage modified :grad_EI[dim]: gradient of EI, \\pderiv{EI(x)}{x_d}, where x is points_to_sample \\endrst \\rst Differentiates OnePotentialSampleExpectedImprovementEvaluator::ComputeExpectedImprovement wrt points_to_sample (which is just ONE point; i.e., 1,0-EI). Again, this uses analytic formulas in terms of the pdf & cdf of a Gaussian since the integral in EI (and grad EI) can be evaluated exactly for this low dimensional case. See Ginsbourger, Le Riche, and Carraro. \\endrst Declaration void optimal_learning::PosteriorMeanEvaluator::ComputeGradPosteriorMean(StateType *ps_state, double *restrict grad_PS) const OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::PosteriorMeanEvaluator::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(PosteriorMeanEvaluator)"
  },
  "api/optimal/learning/posterior-mean-state.html": {
    "href": "api/optimal/learning/posterior-mean-state.html",
    "title": "Struct optimal_learning::PosteriorMeanState | qiotoolkit",
    "keywords": "Struct optimal_learning::PosteriorMeanState \\rst State object for OnePotentialSampleExpectedImprovementEvaluator. This tracks the ONE point_to_sample being evaluated via expected improvement. This is just a special case of ExpectedImprovementState; see those class docs for more details. See general comments on State structs in gpp_common.hpp's header docs. \\endrst Constructors PosteriorMeanState() \\rst Constructs an OnePotentialSampleExpectedImprovementState object for the purpose of computing EI (and its gradient) over the specified point to sample. This establishes properly sized/initialized temporaries for EI computation, including dependent state from the associated Gaussian Process (which arrives as part of the ei_evaluator). .. WARNING:: This object is invalidated if the associated ei_evaluator is mutated. SetupState() should be called to reset. .. WARNING:: Using this object to compute gradients when configure_for_gradients := false results in UNDEFINED BEHAVIOR. :ei_evaluator expected improvement evaluator object that specifies the parameters & GP for EI evaluation :point_to_sample[dim]: point at which to evaluate EI and/or its gradient to check their value in future experiments (i.e., test point for GP predictions) :configure_for_gradients: true if this object will be used to compute gradients, false otherwise \\endrst Declaration optimal_learning::PosteriorMeanState::PosteriorMeanState(const EvaluatorType&ps_evaluator, const int num_fidelity_in, double const *restrict point_to_sample_in, bool configure_for_gradients) PosteriorMeanState() Declaration optimal_learning::PosteriorMeanState::PosteriorMeanState(PosteriorMeanState&&other) Methods GetProblemSize() Declaration int optimal_learning::PosteriorMeanState::GetProblemSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT BuildUnionOfPoints() Declaration std::vector<double>optimal_learning::PosteriorMeanState::BuildUnionOfPoints(double const *restrict points_to_sample) noexcept OL_WARN_UNUSED_RESULT GetCurrentPoint() \\rst Get point_to_sample: the potential future sample whose EI (and/or gradients) is being evaluated \\output :point_to_sample[dim]: potential sample whose EI is being evaluted \\endrst Declaration void optimal_learning::PosteriorMeanState::GetCurrentPoint(double *restrict point_to_sample_out) const noexcept OL_NONNULL_POINTERS SetCurrentPoint() \\rst Change the potential sample whose EI (and/or gradient) is being evaluated. Update the state's derived quantities to be consistent with the new point. :ei_evaluator expected improvement evaluator object that specifies the parameters & GP for EI evaluation :point_to_sample[dim]: potential future sample whose EI (and/or gradients) is being evaluated \\endrst Declaration void optimal_learning::PosteriorMeanState::SetCurrentPoint(const EvaluatorType&ps_evaluator, double const *restrict point_to_sample_in) OL_NONNULL_POINTERS SetupState() \\rst Configures this state object with a new point_to_sample, the location of the potential sample whose EI is to be evaluated. Ensures all state variables & temporaries are properly sized. Properly sets all dependent state variables (e.g., GaussianProcess's state) for EI evaluation. .. WARNING:: This object's state is INVALIDATED if the ei_evaluator (including the GaussianProcess it depends on) used in SetupState is mutated! SetupState() should be called again in such a situation. :ei_evaluator expected improvement evaluator object that specifies the parameters & GP for EI evaluation :point_to_sample[dim]: potential future sample whose EI (and/or gradients) is being evaluated \\endrst Declaration void optimal_learning::PosteriorMeanState::SetupState(const EvaluatorType&ps_evaluator, double const *restrict point_to_sample_in) OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::PosteriorMeanState::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(PosteriorMeanState)"
  },
  "api/optimal/learning/repeated-domain.html": {
    "href": "api/optimal/learning/repeated-domain.html",
    "title": "Class optimal_learning::RepeatedDomain | qiotoolkit",
    "keywords": "Class optimal_learning::RepeatedDomain \\rst A generic domain type for simultaneously manipulating num_repeats points in a \"regular\" domain (the kernel). .. Note:: Comments in this class are copied to RepeatedDomain in optimal_learning/python/repated_domain.py. .. Note:: the kernel domain is not copied. Instead, the kernel functions are called num_repeats times in a loop. In some cases, data reordering is also necessary to preserve the output properties (e.g., uniform distribution). For some use cases (e.g., q,p-EI optimization with q > 1), we need to simultaneously manipulate several points within the same domain. To support this use case, we have the RepeatedDomain, a light-weight wrapper around any DomainType object that kernalizes that object's functionality. In general, kernel domain operations need be performed num_repeats times, once for each point. This class hides the looping logic so that use cases like various Optimizer implementations (gpp_optimization.hpp) do not need to be explicitly aware of whether they are optimizing 1 point or 50 points. Instead, an optimizable Evaluator/State pair provides GetProblemSize() and appropriately sized gradient information. Coupled with RepeatedDomain, Optimizers can remain oblivious. In simpler terms, say we want to solve 5,0-EI in a parameter-space of dimension 3. So we would have 5 points moving around in a 3D space. The 3D space, whatever it is, is the kernel domain. We \"repeat\" the kernel 5 times; in practice this mostly amounts to simple loops around kernel functions and sometimes data reordering is also needed. .. Note:: this operation is more complex than just working in a higher dimensional space. 3 points in a 2D simplex is not the same as 1 point in a 6D simplex; e.g., [(0.5, 0.5), (0.5, 0.5), (0.5, 0.5)] is valid in the first scenario but not in the second. Where the member domain takes kernel_input, this class's members take an array with of num_repeats data with the same size as kernel_input, ordered sequentially. So if we have kernel_input[dim][num_points], we now have repeated_input[dim][num_points][num_repeats]. The same is true for outputs. For example, CheckPointInside() calls the kernel domain's CheckPointInside() function num_repeats times, returning True only if all num_repeats input points are inside the kernel domain. \\endrst Inheritance optimal_learning::RepeatedDomain Constructors RepeatedDomain() Declaration optimal_learning::RepeatedDomain<DomainType_>::RepeatedDomain()=delete RepeatedDomain() \\rst Construct a RepeatedDomain object, which kernalizes and repeats an input DomainType object. .. Note:: this class maintains a pointer to the input domain. Do not let the domain object go out of scope before this object goes out of scope. :domain the domain to repeat :num_repeats: number of times to repeat the input domain \\endrst Declaration optimal_learning::RepeatedDomain<DomainType_>::RepeatedDomain(const DomainType&domain, int num_repeats_in) Methods dim() Declaration int optimal_learning::RepeatedDomain<DomainType_>::dim() const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT num_repeats() Declaration int optimal_learning::RepeatedDomain<DomainType_>::num_repeats() const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT CheckPointInside() \\rst Check if a point is inside the domain/on its domain or outside :point[dim][num_repeats] point to check true if point is inside the domain or on its boundary, false otherwise \\endrst Declaration bool optimal_learning::RepeatedDomain<DomainType_>::CheckPointInside(double const *restrict point) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT GeneratePointInDomain() \\rst Generates \"point\" such that CheckPointInside(point) returns true. May use rejection sampling so point generation may fail. :uniform_generator[1] a UniformRandomGenerator object providing the random engine for uniform random numbers :random_point[dim][num_repeats]: properly sized array \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws :random_point[dim][num_repeats]: point with coordinates inside the domain (left in invalid state if fcn returns false) true if point generation succeeded \\endrst Declaration bool optimal_learning::RepeatedDomain<DomainType_>::GeneratePointInDomain(UniformRandomGenerator *uniform_generator, double *restrict random_point) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT GenerateUniformPointsInDomain() \\rst Generates AT MOST num_points points in the domain (i.e., such that CheckPointInside(point) returns true). The points will be uniformly distributed. May use rejection sampling so we are not guaranteed to generate num_points samples. :num_points number of random points to generate :uniform_generator[1]: a UniformRandomGenerator object providing the random engine for uniform random numbers :random_points[dim][num_repeats][num_points]: properly sized array \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws :random_points[dim][num_repeats][num_points]: point with coordinates inside the domain number of points actually generated \\endrst Declaration int optimal_learning::RepeatedDomain<DomainType_>::GenerateUniformPointsInDomain(int num_points, UniformRandomGenerator *uniform_generator, double *restrict random_points) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT LimitUpdate() \\rst Changes update_vector so that: point_new = point + update_vector has coordinates such that CheckPointInside(point_new) returns true. update_vector is UNMODIFIED if point_new is already inside the domain. .. Note:: we modify update_vector (instead of returning point_new) so that further update limiting/testing may be performed. :max_relative_change max change allowed per update (as a relative fraction of current distance to boundary) :current_point[dim][num_repeats]: starting point :update_vector[dim][num_repeats]: proposed update \\output :update_vector[dim][num_repeats]: modified update so that the final point remains inside the domain \\endrst Declaration void optimal_learning::RepeatedDomain<DomainType_>::LimitUpdate(double max_relative_change, double const *restrict current_point, double *restrict update_vector) const OL_NONNULL_POINTERS"
  },
  "api/optimal/learning/simple-objective-function-evaluator.html": {
    "href": "api/optimal/learning/simple-objective-function-evaluator.html",
    "title": "Class optimal_learning::SimpleObjectiveFunctionEvaluator | qiotoolkit",
    "keywords": "Class optimal_learning::SimpleObjectiveFunctionEvaluator \\rst Class to evaluate the function f(x_1,...,x_{dim}) = -\\sum_i (x_i - s_i)^2, i = 1..dim. This is a simple quadratic form with maxima at (s_1, ..., s_{dim}). \\endrst Inheritance optimal_learning::SimpleObjectiveFunctionEvaluator Constructors SimpleObjectiveFunctionEvaluator() Declaration optimal_learning::SimpleObjectiveFunctionEvaluator::SimpleObjectiveFunctionEvaluator()=default Methods ~SimpleObjectiveFunctionEvaluator() Declaration virtual optimal_learning::SimpleObjectiveFunctionEvaluator::~SimpleObjectiveFunctionEvaluator()=default dim() Declaration virtual int optimal_learning::SimpleObjectiveFunctionEvaluator::dim() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT=0 GetOptimumValue() \\rst Helpful for testing so we know what the optimum is. This value should be the result of:: GetOptimumPoint(point); state.SetCurrentPoint(point); optimum_value = ComputeObjectiveFunction(state); Then optimum_value ==[ GetOptimumValue()](xref:classoptimal__learning_1_1SimpleObjectiveFunctionEvaluator_1abfebb38321e12587a44e243d0a1d1f30). the optimum value of the polynomial. \\endrst Declaration virtual double optimal_learning::SimpleObjectiveFunctionEvaluator::GetOptimumValue() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT=0 GetOptimumPoint() \\rst Helpful for testing so we know where the optimum (value returned by GetOptimumValue) occurs. .. NOTE:: if the optimal point is not unique, this function may return one arbitrarily. \\input :point[dim]: space to write the output OUTPUTS: :point[dim]: the point at which the polynomial obtains its optimum value \\endrst Declaration virtual void optimal_learning::SimpleObjectiveFunctionEvaluator::GetOptimumPoint(double *restrict point) const noexcept OL_NONNULL_POINTERS=0 ComputeObjectiveFunction() \\rst Compute the quadratic objective function: f(x_1,...,x_{dim}) = -\\sum_i (x_i - s_i)^2. quadratic_dummy_state[1] ptr to a FULLY CONFIGURED StateType (e.g., SimpleObjectiveFunctionState) \\output quadratic_dummy_state[1]: ptr to a FULLY CONFIGURED StateType; only temporary state may be mutated the value of the objective at quadratic_dummy_state.GetCurrentPoint() \\endrst Declaration virtual double optimal_learning::SimpleObjectiveFunctionEvaluator::ComputeObjectiveFunction(StateType *quadratic_dummy_state) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT=0 ComputeGradObjectiveFunction() \\rst Compute the gradient of the objective function: f'(x_1,...,x_{dim})_i = -2 * (x_i - s_i). quadratic_dummy_state[1] ptr to a FULLY CONFIGURED StateType (e.g., SimpleObjectiveFunctionState) \\output quadratic_dummy_state[1]: ptr to a FULLY CONFIGURED StateType; only temporary state may be mutated grad_polynomial[dim]: gradient of the objective \\endrst Declaration virtual void optimal_learning::SimpleObjectiveFunctionEvaluator::ComputeGradObjectiveFunction(StateType *quadratic_dummy_state, double *restrict grad_polynomial) const OL_NONNULL_POINTERS=0 ComputeHessianObjectiveFunction() \\rst Compute the gradient of the objective function: f''(x_1,...,x_{dim})_{i,j} = -2 * \\delta_{i,j}. quadratic_dummy_state[1] ptr to a FULLY CONFIGURED StateType (e.g., SimpleObjectiveFunctionState) \\output quadratic_dummy_state[1]: ptr to a FULLY CONFIGURED StateType; only temporary state may be mutated hessian_polynomial[dim][dim]: hessian of the objective \\endrst Declaration virtual void optimal_learning::SimpleObjectiveFunctionEvaluator::ComputeHessianObjectiveFunction(StateType *quadratic_dummy_state, double *restrict hessian_polynomial) const OL_NONNULL_POINTERS=0 OL_DISALLOW_COPY_AND_ASSIGN() Declaration optimal_learning::SimpleObjectiveFunctionEvaluator::OL_DISALLOW_COPY_AND_ASSIGN(SimpleObjectiveFunctionEvaluator)"
  },
  "api/optimal/learning/simple-objective-function-state.html": {
    "href": "api/optimal/learning/simple-objective-function-state.html",
    "title": "Struct optimal_learning::SimpleObjectiveFunctionState | qiotoolkit",
    "keywords": "Struct optimal_learning::SimpleObjectiveFunctionState Constructors SimpleObjectiveFunctionState() Declaration optimal_learning::SimpleObjectiveFunctionState::SimpleObjectiveFunctionState(const EvaluatorType&quadratic_eval, double const *restrict current_point_in) SimpleObjectiveFunctionState() Declaration optimal_learning::SimpleObjectiveFunctionState::SimpleObjectiveFunctionState(SimpleObjectiveFunctionState&&OL_UNUSED(other))=default Methods GetProblemSize() Declaration int optimal_learning::SimpleObjectiveFunctionState::GetProblemSize() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetCurrentPoint() Declaration void optimal_learning::SimpleObjectiveFunctionState::GetCurrentPoint(double *restrict current_point_in) const noexcept OL_NONNULL_POINTERS SetCurrentPoint() Declaration void optimal_learning::SimpleObjectiveFunctionState::SetCurrentPoint(const EvaluatorType&OL_UNUSED(ei_eval), double const *restrict current_point_in) noexcept OL_NONNULL_POINTERS OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN() Declaration optimal_learning::SimpleObjectiveFunctionState::OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN(SimpleObjectiveFunctionState)"
  },
  "api/optimal/learning/simplex-intersect-tensor-product-domain.html": {
    "href": "api/optimal/learning/simplex-intersect-tensor-product-domain.html",
    "title": "Class optimal_learning::SimplexIntersectTensorProductDomain | qiotoolkit",
    "keywords": "Class optimal_learning::SimplexIntersectTensorProductDomain \\rst Domain class for the intersection of the unit simplex with an arbitrary tensor product domain. To that end, this object has a TensorProductDomain object as a data member and uses its functions when possible. See TensorProductDomain for what that means. The unit d-simplex is defined as the set of x_i such that: x_i >= 0 \\forall i (i ranging over dimension) \\sum_i x_i <= 1 (Implying that x_i <= 1 \\forall i) ASSUMPTION: most of the volume of the tensor product region lies inside the simplex region. \\endrst Inheritance optimal_learning::SimplexIntersectTensorProductDomain Constructors SimplexIntersectTensorProductDomain() Declaration optimal_learning::SimplexIntersectTensorProductDomain::SimplexIntersectTensorProductDomain()=delete SimplexIntersectTensorProductDomain() \\rst Constructs a SimplexIntersectTensorProductDomain. The bounds of the tensor product region are specified through the \"domain\" input, just as with TensorProductDomain. :domain[dim] array of ClosedInterval specifying the boundaries of a dim-dimensional tensor-product domain. :dim_in: number of spatial dimensions \\endrst Declaration optimal_learning::SimplexIntersectTensorProductDomain::SimplexIntersectTensorProductDomain(ClosedInterval const *restrict domain, int dim_in) Methods dim() Declaration int optimal_learning::SimplexIntersectTensorProductDomain::dim() const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetMaxNumberOfBoundaryPlanes() \\rst Maximum number of planes that define the boundary of this domain. Used for testing. This result is NOT exact. max number of planes defining the boundary of this domain \\endrst Declaration int optimal_learning::SimplexIntersectTensorProductDomain::GetMaxNumberOfBoundaryPlanes() const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetBoundaryPlanes() \\rst Fills an input array with all bounding planes of this domain. See struct Plane in gpp_geometry.hpp for how to specify a plane. Used for testing. Let max_num_bound = GetMaxNumberOfBoundaryPlanes() :planes[max_num_bound] properly allocated space: max_num_bound Plane objects in dim spatial dimensions \\output :planes[max_num_bound]: array of planes of this domain \\endrst Declaration void optimal_learning::SimplexIntersectTensorProductDomain::GetBoundaryPlanes(Plane *restrict planes) const OL_NONNULL_POINTERS CheckPointInside() \\rst Check if a point is inside the domain/on its domain or outside :point[dim] point to check true if point is inside the domain or on its boundary, false otherwise \\endrst Declaration bool optimal_learning::SimplexIntersectTensorProductDomain::CheckPointInside(double const *restrict point) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT GeneratePointInDomain() \\rst Generates \"point\" such that CheckPointInside(point) returns true. Uses rejection sampling so point generation may fail. :uniform_generator[1] a UniformRandomGenerator object providing the random engine for uniform random numbers :random_point[dim]: properly sized array \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws :random_point[dim]: point with coordinates inside the domain (left in invalid state if fcn returns false) true if point generation succeeded \\endrst Declaration bool optimal_learning::SimplexIntersectTensorProductDomain::GeneratePointInDomain(UniformRandomGenerator *uniform_generator, double *restrict random_point) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT GenerateUniformPointsInDomain() \\rst Generates AT MOST num_points points in the domain (i.e., such that CheckPointInside(point) returns true). The points will be uniformly distributed. Uses rejection sampling so we are not guaranteed to generate num_points samples. :num_points number of random points to generate :uniform_generator[1]: a UniformRandomGenerator object providing the random engine for uniform random numbers :random_points[dim][num_points]: properly sized array \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws :random_points[dim][num_points]: point with coordinates inside the domain number of points actually generated \\endrst Declaration int optimal_learning::SimplexIntersectTensorProductDomain::GenerateUniformPointsInDomain(int num_points, UniformRandomGenerator *uniform_generator, double *restrict random_points) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT LimitUpdate() \\rst Changes update_vector so that: point_new = point + update_vector has coordinates such that CheckPointInside(point_new) returns true. update_vector is UNMODIFIED if point_new is already inside the domain. .. Note:: we modify update_vector (instead of returning point_new) so that further update limiting/testing may be performed. :max_relative_change max change allowed per update (as a relative fraction of current distance to boundary) :current_point[dim]: starting point :update_vector[dim]: proposed update \\output :update_vector[dim]: modified update so that the final point remains inside the domain \\endrst Declaration void optimal_learning::SimplexIntersectTensorProductDomain::LimitUpdate(double max_relative_change, double const *restrict current_point, double *restrict update_vector) const OL_NONNULL_POINTERS"
  },
  "api/optimal/learning/singular-matrix-exception.html": {
    "href": "api/optimal/learning/singular-matrix-exception.html",
    "title": "Class optimal_learning::SingularMatrixException | qiotoolkit",
    "keywords": "Class optimal_learning::SingularMatrixException \\rst Overview** Exception to capture when a square matrix A (\\in R^{m x m}) is singular. Stores the matrix (in a std::vector) and its dimensions along with the index of the leading minor that is non-SPD. .. Note:: std::vector ctor can throw and cause std::terminate(). Message Format** The what() message is formatted in the class ctor (capitals indicate variable information):: R\"%%( SingularMatrixException: M x M matrix is singular; i-th leading minor is not SPD. CUSTOM_MESSAGE FUNCTION_NAME FILE_LINE_INFO )%%\" .. Note:: this exception currently does not print the full matrix. Use a debugger and call PrintMatrix() (gpp_logging.hpp) or catch the exception and proecss the matrix. \\endrst Inheritance optimal_learning::OptimalLearningException optimal_learning::SingularMatrixException Inherited Members OptimalLearningException OptimalLearningException what AppendCustomMessageAndDebugInfo OptimalLearningException Constructors SingularMatrixException() \\rst Constructs a SingularMatrixException object with extra fields to flesh out the what() message. :line_info[] ptr to char array containing FILE and LINE info; e.g., from OL_STRINGIFY_FILE_AND_LINE :func_info[]: optional ptr to char array from OL_CURRENT_FUNCTION_NAME or similar :custom_message[]: optional ptr to char array with any additional text/info to print/log :matrix[num_rows][num_cols]: the singular matrix :num_rows: number of rows (= number of columns) in the matrix :leading_minor_index: index of the first non-positive definite (principal) leading minor \\endrst Declaration optimal_learning::SingularMatrixException::SingularMatrixException(char const *line_info, char const *func_info, char const *custom_message, double const *matrix_in, int num_rows_in, int leading_minor_index_in) Methods num_rows() Declaration int optimal_learning::SingularMatrixException::num_rows() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT leading_minor_index() Declaration int optimal_learning::SingularMatrixException::leading_minor_index() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT matrix() Declaration const std::vector<double>&optimal_learning::SingularMatrixException::matrix() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT OL_DISALLOW_DEFAULT_AND_ASSIGN() Declaration optimal_learning::SingularMatrixException::OL_DISALLOW_DEFAULT_AND_ASSIGN(SingularMatrixException)"
  },
  "api/optimal/learning/square-exponential.html": {
    "href": "api/optimal/learning/square-exponential.html",
    "title": "Class optimal_learning::SquareExponential | qiotoolkit",
    "keywords": "Class optimal_learning::SquareExponential \\rst Implements the square exponential covariance function: cov(x_1, x_2) = \\alpha * \\exp(-1/2 * ((x_1 - x_2)^T * L * (x_1 - x_2)) ) where L is the diagonal matrix with i-th diagonal entry 1/lengths[i]/lengths[i] We also implement the augmented kernel function with the gradient obervations. This covariance object has dim+1 hyperparameters: \\alpha, lengths_i See CovarianceInterface for descriptions of the virtual functions. \\endrst Inheritance optimal_learning::CovarianceInterface optimal_learning::SquareExponential Inherited Members ~CovarianceInterface Constructors SquareExponential() \\rst Constructs a SquareExponential object with constant length-scale across all dimensions. :dim the number of spatial dimensions :alpha: the hyperparameter \\alpha (e.g., signal variance, \\sigma_f^2) :length: the constant length scale to use for all hyperparameter length scales \\endrst Declaration optimal_learning::SquareExponential::SquareExponential(int dim, double alpha, double length) SquareExponential() \\rst Constructs a SquareExponential object with the specified hyperparameters. :dim the number of spatial dimensions :alpha: the hyperparameter \\alpha, (e.g., signal variance, \\sigma_f^2) :lengths[dim]: the hyperparameter length scales, one per spatial dimension \\endrst Declaration optimal_learning::SquareExponential::SquareExponential(int dim, double alpha, double const *restrict lengths) OL_NONNULL_POINTERS SquareExponential() \\rst Constructs a SquareExponential object with the specified hyperparameters. :dim the number of spatial dimensions :alpha: the hyperparameter \\alpha, (e.g., signal variance, \\sigma_f^2) :lengths: the hyperparameter length scales, one per spatial dimension \\endrst Declaration optimal_learning::SquareExponential::SquareExponential(int dim, double alpha, std::vector<double>lengths) SquareExponential() Declaration optimal_learning::SquareExponential::SquareExponential(const SquareExponential&source) Methods Covariance() \\rst Computes the covariance function of the function values and their gradients of two points, cov(point_one, point_two). Points must be arrays with length dim. The covariance function is guaranteed to be symmetric by definition: Covariance(x, y) = Covariance(y, x). This function is also positive definite by definition. :point_one[dim] first spatial coordinate :derivatives_one[dim]: which derivatives of point_one are available :num_derivatives_one: int, the number of derivatives of point one :point_two[dim]: second spatial coordinate :derivatives_two[dim]: which derivatives of point_two are available :num_derivatives_two: int, the number of derivatives of point two cov[1+num_derivatives_one][1+num_derivatives_two]: value of covariance between the function values and their gradients of the input points \\endrst Declaration void optimal_learning::SquareExponential::Covariance(double const *restrict point_one, int const *restrict derivatives_one, int num_derivatives_one, double const *restrict point_two, int const *restrict derivatives_two, int num_derivatives_two, double *restrict cov) const noexcept override OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT GradCovariance() \\rst Computes the gradient of this.Covariance(point_one, point_two) with respect to the FIRST argument, point_one. This distinction is important for maintaining the desired symmetry. Cov(x, y) = Cov(y, x). Additionally, \\pderiv{Cov(x, y)}{x} = \\pderiv{Cov(y, x)}{x}. However, in general, \\pderiv{Cov(x, y)}{x} != \\pderiv{Cov(y, x)}{y} (NOT equal! These may differ by a negative sign) Hence to avoid separate implementations for differentiating against first vs second argument, this function only handles differentiation against the first argument. If you need \\pderiv{Cov(y, x)}{x}, just swap points x and y. :point_one[dim] first spatial coordinate :derivatives_one[dim]: which derivatives of point_one are available :num_derivatives_one: int, the number of derivatives of point one :point_two[dim]: second spatial coordinate :derivatives_two[dim]: which derivatives of point_two are available :num_derivatives_two: int, the number of derivatives of point two \\output grad_cov[dim][1+num_derivatives_one][1+num_derivatives_two]: (i, j, k)-th entry is \\pderiv{cov(x_1, x_2)(j, k))}{x1_i} \\endrst Declaration void optimal_learning::SquareExponential::GradCovariance(double const *restrict point_one, int const *restrict derivatives_one, int num_derivatives_one, double const *restrict point_two, int const *restrict derivatives_two, int num_derivatives_two, double *restrict grad_cov) const noexcept override OL_NONNULL_POINTERS GetNumberOfHyperparameters() \\rst Returns the number of hyperparameters. This base class only allows for a maximum of dim + 1 hyperparameters but subclasses may implement additional ones. The number of hyperparameters. Return 0 to disable hyperparameter-related gradients, optimizations. \\endrst Declaration virtual int optimal_learning::SquareExponential::GetNumberOfHyperparameters() const noexcept override OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT HyperparameterGradCovariance() \\rst Similar to GradCovariance(), except gradients are computed w.r.t. the hyperparameters. Unlike GradCovariance(), the order of point_one and point_two is irrelevant here (since we are not differentiating against either of them). Thus the matrix of grad covariances (wrt hyperparameters) is symmetric. :point_one[dim] first spatial coordinate :derivatives_one[dim]: which derivatives of point_one are available :num_derivatives_one: int, the number of derivatives of point one :point_two[dim]: second spatial coordinate :derivatives_two[dim]: which derivatives of point_two are available :num_derivatives_two: int, the number of derivatives of point two \\output :grad_hyperparameter_cov[this.GetNumberOfHyperparameters()][1+num_derivatives_one][1+num_derivatives_two]: (i, j, k)-th entry is \\pderiv{cov(x_1, x_2)(j, k)}{\\theta_i} \\endrst Declaration void optimal_learning::SquareExponential::HyperparameterGradCovariance(double const *restrict point_one, int const *restrict derivatives_one, int num_derivatives_one, double const *restrict point_two, int const *restrict derivatives_two, int num_derivatives_two, double *restrict grad_hyperparameter_cov) const noexcept override OL_NONNULL_POINTERS SetHyperparameters() \\rst Sets the hyperparameters. Hyperparameter ordering is defined implicitly by GetHyperparameters: [alpha=\\sigma_f^2, length_0, ..., length_{n-1}] :hyperparameters[this.GetNumberOfHyperparameters()] hyperparameters to set \\endrst Declaration virtual void optimal_learning::SquareExponential::SetHyperparameters(double const *restrict hyperparameters) noexcept override OL_NONNULL_POINTERS GetHyperparameters() \\rst Gets the hyperparameters. Ordering is [alpha=\\sigma_f^2, length_0, ..., length_{n-1}] \\output :hyperparameters[this.GetNumberOfHyperparameters()]: values of current hyperparameters \\endrst Declaration virtual void optimal_learning::SquareExponential::GetHyperparameters(double *restrict hyperparameters) const noexcept override OL_NONNULL_POINTERS Clone() \\rst For implementing the virtual (copy) constructor idiom. :Pointer to a constructed object that is a subclass of CovarianceInterface \\endrst Declaration CovarianceInterface * optimal_learning::SquareExponential::Clone() const override OL_WARN_UNUSED_RESULT OL_DISALLOW_DEFAULT_AND_ASSIGN() Declaration optimal_learning::SquareExponential::OL_DISALLOW_DEFAULT_AND_ASSIGN(SquareExponential) Initialize() \\rst Validate and initialize class data members. \\endrst Declaration void optimal_learning::SquareExponential::Initialize()"
  },
  "api/optimal/learning/tensor-product-domain.html": {
    "href": "api/optimal/learning/tensor-product-domain.html",
    "title": "Class optimal_learning::TensorProductDomain | qiotoolkit",
    "keywords": "Class optimal_learning::TensorProductDomain \\rst Domain type for a tensor product domain. A d-dimensional tensor product domain is D = [x_0_{min}, x_0_{max}] X [x_1_{min}, x_1_{max}] X ... X [x_d_{min}, x_d_{max}] \\endrst Inheritance optimal_learning::TensorProductDomain Constructors TensorProductDomain() Declaration optimal_learning::TensorProductDomain::TensorProductDomain()=delete TensorProductDomain() \\rst Constructs a TensorProductDomain. :domain[dim] array of ClosedInterval specifying the boundaries of a dim-dimensional tensor-product domain. :dim_in: number of spatial dimensions \\endrst Declaration optimal_learning::TensorProductDomain::TensorProductDomain(ClosedInterval const *restrict domain, int dim_in) Methods dim() Declaration int optimal_learning::TensorProductDomain::dim() const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT SetDomain() \\rst Explicitly set the domain boundaries. Assumes specified domain is non-empty. :domain[dim] array of ClosedInterval specifying the boundaries of a dim-dimensional tensor-product domain. \\endrst Declaration void optimal_learning::TensorProductDomain::SetDomain(ClosedInterval const *restrict domain) OL_NONNULL_POINTERS GetMaxNumberOfBoundaryPlanes() \\rst Maximum number of planes that define the boundary of this domain. Used for testing. This result is exact. max number of planes defining the boundary of this domain \\endrst Declaration int optimal_learning::TensorProductDomain::GetMaxNumberOfBoundaryPlanes() const OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT GetBoundaryPlanes() \\rst Fills an input array with all bounding planes of this domain. See struct Plane in gpp_geometry.hpp for how to specify a plane. Used for testing. Let max_num_bound = GetMaxNumberOfBoundaryPlanes() :planes[max_num_bound] properly allocated space: max_num_bound Plane objects in dim spatial dimensions \\output :planes[max_num_bound]: array of planes of this domain \\endrst Declaration void optimal_learning::TensorProductDomain::GetBoundaryPlanes(Plane *restrict planes) const OL_NONNULL_POINTERS CheckPointInside() \\rst Check if a point is inside the domain/on its boundary or outside. :point[dim] point to check true if point is inside the domain or on its boundary, false otherwise \\endrst Declaration bool optimal_learning::TensorProductDomain::CheckPointInside(double const *restrict point) const OL_NONNULL_POINTERS OL_WARN_UNUSED_RESULT GeneratePointInDomain() \\rst Generates \"point\" such that CheckPointInside(point) returns true. :uniform_generator[1] a UniformRandomGenerator object providing the random engine for uniform random numbers :random_point[dim]: properly sized array \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws :random_point[dim]: point with coordinates inside the domain (left in invalid state if fcn returns false) true if point generation succeeded \\endrst Declaration bool optimal_learning::TensorProductDomain::GeneratePointInDomain(UniformRandomGenerator *uniform_generator, double *restrict random_point) const OL_NONNULL_POINTERS GenerateUniformPointsInDomain() \\rst Generates num_points points in the domain (i.e., such that CheckPointInside(point) returns true). The points will be uniformly distributed. :num_points number of random points to generate :uniform_generator[1]: a UniformRandomGenerator object providing the random engine for uniform random numbers :random_points[dim][num_points]: properly sized array \\output :uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws :random_points[dim][num_points]: point with coordinates inside the domain number of points generated (always num_points; ok to not use this result) \\endrst Declaration int optimal_learning::TensorProductDomain::GenerateUniformPointsInDomain(int num_points, UniformRandomGenerator *uniform_generator, double *restrict random_points) const OL_NONNULL_POINTERS LimitUpdate() \\rst Changes update_vector so that: point_new = point + update_vector has coordinates such that CheckPointInside(point_new) returns true. update_vector is UNMODIFIED if point_new is already inside the domain. .. Note:: we modify update_vector (instead of returning point_new) so that further update limiting/testing may be performed. :max_relative_change max change allowed per update (as a relative fraction of current distance to boundary) :current_point[dim]: starting point :update_vector[dim]: proposed update \\output :update_vector[dim]: modified update so that the final point remains inside the domain \\endrst Declaration void optimal_learning::TensorProductDomain::LimitUpdate(double max_relative_change, double const *restrict current_point, double *restrict update_vector) const OL_NONNULL_POINTERS"
  },
  "api/optimal/learning/thread-schedule.html": {
    "href": "api/optimal/learning/thread-schedule.html",
    "title": "Struct optimal_learning::ThreadSchedule | qiotoolkit",
    "keywords": "Struct optimal_learning::ThreadSchedule \\rst Overview** When we ask openmp to parallelize a for loop, we can give it additional information on how to distribute the work. In particular, the overall work (N iterations) needs to be divided up amongst the threads. We have two major ways to affect how openmp structures the loop: schedule and chunk_size. chunk_size changes meaning depending on schedule. Here we list out the options for schedule as name (ENV_NAME, enum_name) where ENV_NAME is the corresponding value of OMP_SCHEDULE (not used by optimal_learning) and enum_name is the corresponding type from opm_sched_t in omp.h. Below, \"work\" refers to loop iterations (N total). Schedule Types** a. static (\"static\", omp_sched_static): Work is divided into N/chunk_size contiguous chunks (of chunk_size iterations) and distributed amongst the threads statically in a round-robin fashion. Use when you are confident all chunks will take the same amount of time. Low control overhead but high waste if one iteration is very slow (since the other threads will sit idle). Default chunk_size: N / number_of_threads. This schedule type is repeatable: repeated runs/calls (with the same work) will produce the same mapping of loop iterations to threads every time. b. dynamic (\"dynamic\", omp_sched_dynamic): Work is divided into N/chunk_size contiguous chunks (of chunk_size iterations) and distributed to threads as they complete their work, first-come first-serve. If there is a chunk that is very slow, the other threads can finish all remaining work instead of sitting idle. High control overhead, use when you have no idea how long each chunk will take. Default chunk_size: 1. This schedule type does not produce repeatable mappings of iterations to threads. c. guided (\"guided\", omp_sched_guided): Work is divided into progressively smaller chunks; chunk_size sets the minimum value. As with dynamic, chunks are assigned on a first-come, first-serve basis. Less overhead than dynamic (b/c chunk_size scale down). Useful when iteration times are similar but not identical. Less overhead than dynamic while guaranteeing the waste case of static doesn't arise. Default chunk_size: approximately N / number_of_threads. This schedule type does not produce repeatable mappings of iterations to threads. d. auto (\"auto\", omp_sched_auto): The compiler decides how to map iterations to threads; this mapping is not required to be one of the previous choices. chunk_size has no meaning when the schedule is auto. See: https://gcc.gnu.org/onlinedocs/libgomp/omp_005fset_005fschedule.html This schedule type is not guaranteed to be repeatable. Further documentation: http://openmp.org/mp-documents/OpenMP3.1-CCard.pdf https://software.intel.com/en-us/articles/openmp-loop-scheduling http://publib.boulder.ibm.com/infocenter/comphelp/v8v101/index.jsp?topic=%2Fcom.ibm.xlcpp8a.doc%2Fcompiler%2Fref%2Fruompfor.htm \\endrst Constructors ThreadSchedule() \\rst Construct a ThreadSchedule using the specified number of threads, schedule type, and chunk_size. :max_num_threads maximum number of threads for use by OpenMP (generally should be <= # cores) :schedule: static, dynamic, guided, or auto. See class comments for more details. :chunk_size: how to distribute work to threads; the precise meaning depends on schedule. Zero or negative chunk_size ask OpenMP to use its default behavior. See class comments for details. \\endrst Declaration optimal_learning::ThreadSchedule::ThreadSchedule(int max_num_threads_in, omp_sched_t schedule_in, int chunk_size_in) ThreadSchedule() \\rst Construct a ThreadSchedule using the specified number of threads and schedule type with default chunk_size. :max_num_threads maximum number of threads for use by OpenMP (generally should be <= # cores) :schedule: static, dynamic, guided, or auto. See class comments for more details. \\endrst Declaration optimal_learning::ThreadSchedule::ThreadSchedule(int max_num_threads_in, omp_sched_t schedule_in) ThreadSchedule() \\rst Construct a ThreadSchedule using the specified number of threads with default schedule type and chunk_size. :max_num_threads maximum number of threads for use by OpenMP (generally should be <= # cores) \\endrst Declaration optimal_learning::ThreadSchedule::ThreadSchedule(int max_num_threads_in) ThreadSchedule() \\rst Construct a ThreadSchedule using the default number of threads, schedule type, and chunk_size. \\endrst Declaration optimal_learning::ThreadSchedule::ThreadSchedule()"
  },
  "api/optimal/learning/uniform-random-generator.html": {
    "href": "api/optimal/learning/uniform-random-generator.html",
    "title": "Struct optimal_learning::UniformRandomGenerator | qiotoolkit",
    "keywords": "Struct optimal_learning::UniformRandomGenerator \\rst Container for an uniform random generator (e.g., mersenne twister). Member functions are for easy manipulation of seeds and have signatures matching corresponding members of NormalRNG. .. Note:: seed values take type EngineType::result_type. Do not pass in a wider integer type! .. WARNING:: this class is NOT THREAD-SAFE. You must construct one object per thread (and ensure that the seeds are different for practical computations). \\endrst Constructors UniformRandomGenerator() \\rst Default-constructs a UniformRandomGenerator, seeding with kDefaultSeed. \\endrst Declaration optimal_learning::UniformRandomGenerator::UniformRandomGenerator() noexcept UniformRandomGenerator() \\rst Construct a UniformRandomGenerator, seeding with the specified seed. See UniformRandomGenerator::SetExplicitSeed for details. :seed new seed to set \\endrst Declaration optimal_learning::UniformRandomGenerator::UniformRandomGenerator(EngineType::result_type seed) noexcept UniformRandomGenerator() \\rst Construct a UniformRandomGenerator, seeding with an automatically selected seed based on time, thread_id, etc. See UniformRandomGenerator::SetRandomizedSeed for details. :base_seed base value for the new seed :thread_id: id of the thread using this object \\endrst Declaration optimal_learning::UniformRandomGenerator::UniformRandomGenerator(EngineType::result_type seed, int thread_id) noexcept Methods GetEngine() \\rst Get a reference to the RNG engine used by this class. Not necessary for this class since engine is public but we expose this to maintain a uniform interface with NormalRNG. reference to the underlying RNG engine \\endrst Declaration EngineType&optimal_learning::UniformRandomGenerator::GetEngine() noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT last_seed() Declaration EngineType::result_type optimal_learning::UniformRandomGenerator::last_seed() const noexcept OL_PURE_FUNCTION OL_WARN_UNUSED_RESULT SetExplicitSeed() \\rst Seed the random number generator with the input value. The main purpose of this function is for testingto allow seeding the RNG with a known value for repeatability. :seed new seed to set \\endrst Declaration void optimal_learning::UniformRandomGenerator::SetExplicitSeed(EngineType::result_type seed) noexcept SetRandomizedSeed() \\rst Set a new seed for the random number generator. A \"random\" seed is selected based on the input seed value, the current time, and the thread_id. This function is meant to initialize unique UniformRandomGenerator objects for each computation thread:: std::vector uniform_generator_vector(num_threads); for (int i = 0; i < num_threads; ++i) { uniform_generator_vector.SetRandomizedSeed(base_seed, i); } This function is meant to generate seeds so that: this function can be called multiple times successively (e.g., in the above loop) with different thread_ids to initialize RNGs for multiple threads multiple runs of this code are unlikely to generate the same seed values Item 2. is important for minimizing the probability that we run EI computations (see gpp_math.hpp) with the \"same\" randomness. :base_seed base value for the new seed :thread_id: id of the thread using this object \\endrst Declaration void optimal_learning::UniformRandomGenerator::SetRandomizedSeed(EngineType::result_type base_seed, int thread_id) noexcept ResetToMostRecentSeed() \\rst Reseeds the generator with its most recently specified seed value. Useful for testinge.g., can conduct multiple runs with the same initial conditions \\endrst Declaration void optimal_learning::UniformRandomGenerator::ResetToMostRecentSeed() noexcept PrintState() \\rst Prints the state of the generator to specified ostream. For testing. :out_stream[1] a std::ostream object ready for operator<<`` use \\output :out_stream[1]: std::ostreamwith the engine state \"written\" to itsoperator<<` \\endrst Declaration void optimal_learning::UniformRandomGenerator::PrintState(std::ostream *out_stream) const OL_NONNULL_POINTERS operator==() Declaration bool optimal_learning::UniformRandomGenerator::operator==(const UniformRandomGenerator&other) const operator!=() Declaration bool optimal_learning::UniformRandomGenerator::operator!=(const UniformRandomGenerator&other) const"
  },
  "api/optimal/learning/upper-bound-exception.html": {
    "href": "api/optimal/learning/upper-bound-exception.html",
    "title": "Class optimal_learning::UpperBoundException | qiotoolkit",
    "keywords": "Class optimal_learning::UpperBoundException \\rst Exception to capture value > max_value. Simple subclass of BoundsException that sets the min argument to std::numeric_limits ::lowest() See BoundsException for what() message format. \\endrst Inheritance optimal_learning::BoundsException optimal_learning::UpperBoundException Inherited Members max value OL_DISALLOW_DEFAULT_AND_ASSIGN BoundsException min BoundsException OptimalLearningException OptimalLearningException what AppendCustomMessageAndDebugInfo OptimalLearningException Constructors UpperBoundException() \\rst Constructs an UpperBoundException object with extra fields to flesh out the what() message. :line_info[] ptr to char array containing FILE and LINE info; e.g., from OL_STRINGIFY_FILE_AND_LINE :func_info[]: optional ptr to char array from OL_CURRENT_FUNCTION_NAME or similar :custom_message[]: optional ptr to char array with any additional text/info to print/log :value: the value that violates its min or max bound :max: the maximum bound for value \\endrst Declaration optimal_learning::UpperBoundException<ValueType>::UpperBoundException(char const *line_info, char const *func_info, char const *custom_message, ValueType value_in, ValueType max_in) Methods OL_DISALLOW_DEFAULT_AND_ASSIGN() Declaration optimal_learning::UpperBoundException<ValueType>::OL_DISALLOW_DEFAULT_AND_ASSIGN(UpperBoundException)"
  },
  "api/schedule/constant.html": {
    "href": "api/schedule/constant.html",
    "title": "Class schedule::Constant | qiotoolkit",
    "keywords": "Class schedule::Constant Constant Schedule. The schedule (or schedule segment) has a constant value over the input range. If queried for values outside the input range, the same constant value is returned as well. Configuration Examples: Basic definition: \"schedule\": { \"type\": \"constant\", \"value\": 42.0 } Short-hand notation: \"schedule\": 42.0 With input range: \"schedule\": { \"type\": \"constant\", \"value\": 42.0, \"start\": 0, \"stop\": 13 } With count: \"schedule\": { \"type\": \"constant\", \"value\": 42.0, \"count\": 100 } Note Since the value of this generator is constant, the properties start, stop and count only affect how many times the value is repeated when used as a Segment or generating a discretized set. Inheritance schedule::ScheduleGenerator schedule::Constant Inherited Members get_count ScheduleGenerator get_stop ~ScheduleGenerator get_initial_value get_start set_stop has_count get_final_value has_stop has_start is_repeated ~Component Component get_status param get_class_name Constructors Constant() Declaration schedule::Constant::Constant(double value) Constant() Declaration schedule::Constant::Constant(const Constant&other) Constant() Declaration schedule::Constant::Constant()=default Methods get_progress_value() Get the value at a relative position in the input interval. Arguments: progress relative input position \\in [0..1] Returns: the (constant) value of the generator. Declaration double schedule::Constant::get_progress_value(double) const override configure() Configure the Constant generator from input. This expects either the long (object) or short (scalar) input format, as described in the class description. Declaration void schedule::Constant::configure(const utils::Json&json) override render() Return the config value for schedule. Declaration utils::Structure schedule::Constant::render() const override"
  },
  "api/schedule/geometric.html": {
    "href": "api/schedule/geometric.html",
    "title": "Class schedule::Geometric | qiotoolkit",
    "keywords": "Class schedule::Geometric Geometric Schedule. The schedule (or schedule segment) interpolates geometrically from initial to final value over the input interval. Configuration: Basic definition: \"schedule\": { \"type\": \"geometric\", \"initial\": 1.0, \"final\": 0.3 } With input range: \"schedule\": { \"type\": \"geometric\", \"initial\": 1.0, \"final\": 0.3 \"start\": 0, \"stop\": 10 } With count: \"schedule\": { \"type\": \"geometric\", \"initial\": 1.0, \"final\": 0.3 \"count\": 11 } Inheritance schedule::RangedGenerator schedule::Geometric Inherited Members RangedGenerator get_count ScheduleGenerator get_stop ~ScheduleGenerator get_initial_value get_start set_stop has_count get_final_value has_stop has_start is_repeated ~Component Component get_status param get_class_name Constructors Geometric() Declaration schedule::Geometric::Geometric() Geometric() Declaration schedule::Geometric::Geometric(double initial_value, double final_value) Methods get_progress_value() Get the value at a relative position in the input interval. Arguments: progress relative input position \\in [0..1] Returns: the interpolated value Declaration double schedule::Geometric::get_progress_value(double progress) const override configure() Configure the Geometric generator from input. This expects the initial and final value to interpolate. Declaration void schedule::Geometric::configure(const utils::Json&json) override render() Return the config value for schedule. Declaration utils::Structure schedule::Geometric::render() const override"
  },
  "api/schedule/linear.html": {
    "href": "api/schedule/linear.html",
    "title": "Class schedule::Linear | qiotoolkit",
    "keywords": "Class schedule::Linear Linear Schedule. The schedule (or schedule segment) interpolates linearly from initial to final value over the input interval. Configuration: Basic definition: \"schedule\": { \"type\": \"linear\", \"initial\": 1.0, \"final\": 0.3 } With input range: \"schedule\": { \"type\": \"linear\", \"initial\": 1.0, \"final\": 0.3 \"start\": 0, \"stop\": 10 } With count: \"schedule\": { \"type\": \"linear\", \"initial\": 1.0, \"final\": 0.3 \"count\": 11 } Inheritance schedule::RangedGenerator schedule::Linear Inherited Members RangedGenerator get_count ScheduleGenerator get_stop ~ScheduleGenerator get_initial_value get_start set_stop has_count get_final_value has_stop has_start is_repeated ~Component Component get_status param get_class_name Constructors Linear() Declaration schedule::Linear::Linear() Linear() Explicitly create an inverse linear schedule. This allows algorithms to explicitly create an inverse linear schedule from other input variants (i.e., beta_start, beta_stop). Declaration schedule::Linear::Linear(double initial_value, double final_value) Methods get_progress_value() Get the value at a relative position in the input interval. Arguments: progress relative input position \\in [0..1] Returns: the interpolated value Declaration double schedule::Linear::get_progress_value(double progress) const override configure() Configure the Linear generator from input. This expects the initial and final value to interpolate. Declaration void schedule::Linear::configure(const utils::Json&json) override render() Return the config value for schedule. Declaration utils::Structure schedule::Linear::render() const override"
  },
  "api/schedule/ranged-generator.html": {
    "href": "api/schedule/ranged-generator.html",
    "title": "Class schedule::RangedGenerator | qiotoolkit",
    "keywords": "Class schedule::RangedGenerator RangedGenerator base class. This is a shared interface for generators returning values in a configured range initial..final. Inheritance schedule::ScheduleGenerator schedule::RangedGenerator schedule::Geometric schedule::Linear Inherited Members get_count ScheduleGenerator get_stop ~ScheduleGenerator get_initial_value get_start set_stop has_count get_final_value get_progress_value has_stop has_start is_repeated render ~Component Component get_status param get_class_name Constructors RangedGenerator() Declaration schedule::RangedGenerator::RangedGenerator() Methods configure() Shared input format: Two numbers defining the range initial..final. Declaration void schedule::RangedGenerator::configure(const utils::Json&json) override"
  },
  "api/schedule/schedule-generator.html": {
    "href": "api/schedule/schedule-generator.html",
    "title": "Class schedule::ScheduleGenerator | qiotoolkit",
    "keywords": "Class schedule::ScheduleGenerator ScheduleGenerator interface. A schedule generator maps any input value in the progress interval [0..1] to an output value. Note The input is always in the range [0..1], because the mapping from the input interval to this progress interval is handled by the Schedule class. This ensures the input mapping logic is not duplicated in every generator. Inheritance utils::Component schedule::ScheduleGenerator schedule::Constant schedule::RangedGenerator schedule::Segments Inherited Members render ~Component Component get_status param get_class_name Constructors ScheduleGenerator() Declaration schedule::ScheduleGenerator::ScheduleGenerator() Methods ~ScheduleGenerator() Declaration virtual schedule::ScheduleGenerator::~ScheduleGenerator() get_progress_value() Return the value that progress is mapped to. Declaration virtual double schedule::ScheduleGenerator::get_progress_value(double progress) const =0 get_initial_value() Return the value at the beginning of the input interval. Declaration double schedule::ScheduleGenerator::get_initial_value() const get_final_value() Return the value at the end of the input interval. Declaration double schedule::ScheduleGenerator::get_final_value() const get_start() Get the start input value (first in the input interval). [This default to start=0 if not specified]. Declaration double schedule::ScheduleGenerator::get_start() const get_stop() Get the stop input value (last in the input interval). [This defaults to stop=1 if not specified]. Declaration double schedule::ScheduleGenerator::get_stop() const get_count() Get the number of equidistributed input values to use when generating a discretized set from this generator (without requesting a specific set size). The default value of count=1 means pick just one (the initial value) Declaration int schedule::ScheduleGenerator::get_count() const set_stop() Force the number of equidistributed values to pick. Declaration void schedule::ScheduleGenerator::set_stop(int stop) has_start() Query if the start value was explicitly configured. Declaration bool schedule::ScheduleGenerator::has_start() const has_stop() Query if the stop value was explicitly configured. Declaration bool schedule::ScheduleGenerator::has_stop() const has_count() Query if the count value was explicitly configured. Declaration bool schedule::ScheduleGenerator::has_count() const is_repeated() Should this schedule be repeated indefinitly outside the input interval? Declaration bool schedule::ScheduleGenerator::is_repeated() const configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void schedule::ScheduleGenerator::configure(const utils::Json&json) override"
  },
  "api/schedule/schedule.html": {
    "href": "api/schedule/schedule.html",
    "title": "Class schedule::Schedule | qiotoolkit",
    "keywords": "Class schedule::Schedule Schedule. A schedule maps an input interval to values for parametrization. This can be employed to, e.g., specify a temperature set or schedule (tempering, annealing) define how a parameter should evolve over time (substochastic MC) The schedule can be specified in different forms: constant value linear or geometric interpolation for a range initial..final By explicitly listing the temperatures as an array Examples: Geometric interpolation of the range inital to final: \"schedule\": { \"type\": \"geometric\", \"initial\": 1.0 \"final\": 0.3, } 20 steps with equal spacing from initial to final: \"schedule\": { \"type\": \"linear\", \"initial\": 1.0, \"final\": 3.0 \"count\": 20 } An explicit set of values: \"schedule\": [0.1, 0.2, 0.3, 0.4] Note Without a prior call to configure, an object of this class is uninitialized and will throw an access to nullptr exception. Inheritance utils::Component schedule::Schedule Inherited Members ~Component Component get_status param get_class_name Constructors Schedule() Declaration schedule::Schedule::Schedule() Schedule() Declaration schedule::Schedule::Schedule(const Schedule&copy) Methods ~Schedule() Declaration virtual schedule::Schedule::~Schedule() operator=() Declaration Schedule&schedule::Schedule::operator=(const Schedule&)=delete get_value() Assuming the simulation runs from [0 .. max_steps] (inclusive), returns the temperature to be used at step. Declaration double schedule::Schedule::get_value(double input) const get_discretized_values() Return a set of values for equidistributed inputs over the schedule. Declaration std::vector<double>schedule::Schedule::get_discretized_values(int count=-1) const get_start() Get the starting input value. Declaration double schedule::Schedule::get_start() const get_stop() Get the stopping input value. Declaration double schedule::Schedule::get_stop() const get_initial() Get the initial value (at start) Declaration double schedule::Schedule::get_initial() const get_final() Get the final value (at stop) Declaration double schedule::Schedule::get_final() const get_count() Query how many equidistributed inputs this schedule would \"naturally\" have (for an explicit set of values, this is the length of the array, for a generator it is derived from the (rounded) input range. Declaration int schedule::Schedule::get_count() const set_stop() Force a specific number of steps to be queried from the schedule. Declaration void schedule::Schedule::set_stop(int stop) configure() Serialization will instantiate the correct schedule_generator_ according to / the configuration it is passed. Declaration void schedule::Schedule::configure(const utils::Json&json) override make_linear() Manually create an linear schedule. Declaration void schedule::Schedule::make_linear(double initial_value, double final_value) make_geometric() Manually create an geometric schedule. Declaration void schedule::Schedule::make_geometric(double beta_start, double beta_stop) make_list() Declaration void schedule::Schedule::make_list(const std::vector<double>&values) make_constant() Declaration void schedule::Schedule::make_constant(double value) render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure schedule::Schedule::render() const override"
  },
  "api/schedule/segments.html": {
    "href": "api/schedule/segments.html",
    "title": "Class schedule::Segments | qiotoolkit",
    "keywords": "Class schedule::Segments Segments Schedule. Concatenate multiple schedules to a single schedule. Configuration: Basic definition: \"schedule\": { \"type\": \"segments\", \"segments\": [ { \"type\": \"constant\", \"value\": 3, \"count\": 100, }, { \"type\": \"linear\", \"initial\": 3, \"final\": 0.3, \"count\": 1000 }, { \"type\": \"constant\", \"value\": 0.3, \"count\": 500 } ] } From an array: \"schedule\": { \"type\": \"segments\", \"segments\": [1,2,3] } From an array, shorthand \"schedule\": [1,2,3] Note Unless otherwise specified, each segment is assumed to have a width of 1, with the start point coinciding with the stop of the previous segment. You can modify this default behavior by specifying the (absolute) stop point of the segment or by adjusting the width via count. Note Segments specified as an array of scalar values are presumed to be points of zero input range width spaced by 1. As a result, the total width of the array is mapped to 0..(size-1) and discretization with default count yields back the array. Input values between these points are interpolated linearly. Inheritance schedule::ScheduleGenerator schedule::Segments Inherited Members get_count ScheduleGenerator get_stop ~ScheduleGenerator get_initial_value get_start set_stop has_count get_final_value has_stop has_start is_repeated ~Component Component get_status param get_class_name Constructors Segments() Declaration schedule::Segments::Segments()=default Segments() Declaration schedule::Segments::Segments(const std::vector<double>&values) Methods get_progress_value() Get the value at a relative position in the input interval. Arguments: progress relative input position \\in [0..1] Returns: the value of the segment covering progress. Declaration double schedule::Segments::get_progress_value(double progress) const override configure() Configure the Segments generator from input. This expects the segments to be concatenated. Declaration void schedule::Segments::configure(const utils::Json&json) override adjust() Declaration void schedule::Segments::adjust() render() Return the config value for schedule. Declaration utils::Structure schedule::Segments::render() const override"
  },
  "api/solver/empty-model.html": {
    "href": "api/solver/empty-model.html",
    "title": "Class solver::EmptyModel | qiotoolkit",
    "keywords": "Class solver::EmptyModel Inheritance markov::Model solver::EmptyModel Inherited Members get_benchmark_properties estimate_max_cost_diff has_initial_configuration Model configure state_only_memory_estimate get_term_count get_sweep_size init get_initial_configuration_state render_state calculate_cost apply_transition estimate_min_cost_diff get_scale_factor is_empty set_step_limit is_rescaled state_memory_estimate rescale render calculate_cost_difference get_random_transition get_const_cost configure get_random_state match_version get_version ~BaseModel configure BaseModel get_identifier render ~Component Component get_status param get_class_name Methods create_state() Declaration EmptyState solver::EmptyModel::create_state(const std::vector<int>&) const get_spin_overlap() Declaration double solver::EmptyModel::get_spin_overlap(const EmptyState&, const EmptyState&) const get_term_overlap() Declaration double solver::EmptyModel::get_term_overlap(const EmptyState&, const EmptyState&) const create_parameter_change() Declaration int solver::EmptyModel::create_parameter_change(std::vector<double>&) const get_parameters() Declaration std::vector<std::string>solver::EmptyModel::get_parameters() const"
  },
  "api/solver/empty-state.html": {
    "href": "api/solver/empty-state.html",
    "title": "Class solver::EmptyState | qiotoolkit",
    "keywords": "Class solver::EmptyState Inheritance solver::EmptyState Methods copy_state_only() Declaration int solver::EmptyState::copy_state_only(EmptyState) const render() Declaration utils::Structure solver::EmptyState::render() const"
  },
  "api/solver/estimator.html": {
    "href": "api/solver/estimator.html",
    "title": "Class solver::Estimator | qiotoolkit",
    "keywords": "Class solver::Estimator Energy scale estimator. This estimator attempts to estimate the relevant energy range of the model it analyzes by collecting a histogram of transition costs in the random and quenched regime. The random regime is sampled from random initial positions. It is used as a proxy for the high temperature regime. The quenched regime is sampled from quenched states obtained by peforming several metropolis sweeps at T=0. From these histograms we can tune the temperature for expected boltzmann acceptance rates in either regime. This is expressed in terms of the portion of energy increasing transitions that would be accepted (transitions which don't change the energy are always accepted and the potion of energy-lowering transitions in the random regime depends on the model). Inheritance solver::Estimator Methods gather_statistics() Measure the current cost and samples of the cost of a random transition for the walker at its current state. For models derived from model::FacedGraphModel, an extra parameter for a cost object is necessary to communicate cached calculations. Declaration std::enable_if<std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, void>::type solver::Estimator<Model>::gather_statistics(const Model&model, Walker&walker, double *cost, std::vector<double>*deltas) gather_statistics() Measure the current cost and samples of the cost of a random transition for the walker at its current state. For all other models, no extra cost parameter is needed. Declaration std::enable_if<!std::is_base_of<model::FacedGraphModel<TS, TT>, TM>::value, void>::type solver::Estimator<Model>::gather_statistics(const Model&model, Walker&walker, double *cost, std::vector<double>*deltas) quench() Declaration void solver::Estimator<Model>::quench(Walker&walker) estimate_accepted() Declaration double solver::Estimator<Model>::estimate_accepted(const std::vector<double>&deltas, double T) find_temperature() Declaration double solver::Estimator<Model>::find_temperature(const std::vector<double>&deltas, double target_acceptance) analyze() Declaration utils::Structure solver::Estimator<Model>::analyze(const Model&model, utils::RandomGenerator&rng)"
  },
  "api/solver/evaluation-counter.html": {
    "href": "api/solver/evaluation-counter.html",
    "title": "Class solver::EvaluationCounter | qiotoolkit",
    "keywords": "Class solver::EvaluationCounter Inheritance utils::Component solver::EvaluationCounter Inherited Members configure ~Component Component get_status param get_class_name Constructors EvaluationCounter() Declaration solver::EvaluationCounter::EvaluationCounter() EvaluationCounter() Declaration solver::EvaluationCounter::EvaluationCounter(const EvaluationCounter&other) Methods reset() Declaration void solver::EvaluationCounter::reset() operator+=() Declaration const EvaluationCounter&solver::EvaluationCounter::operator+=(const EvaluationCounter&other) operator-=() Declaration const EvaluationCounter&solver::EvaluationCounter::operator-=(const EvaluationCounter&other) operator=() Declaration EvaluationCounter&solver::EvaluationCounter::operator=(const EvaluationCounter&other)=default render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure solver::EvaluationCounter::render() const get_function_evaluation_count() Declaration uint64_t solver::EvaluationCounter::get_function_evaluation_count() const get_difference_evaluation_count() Declaration uint64_t solver::EvaluationCounter::get_difference_evaluation_count() const get_accepted_transition_count() Declaration uint64_t solver::EvaluationCounter::get_accepted_transition_count() const"
  },
  "api/solver/model-solver.html": {
    "href": "api/solver/model-solver.html",
    "title": "Class solver::ModelSolver | qiotoolkit",
    "keywords": "Class solver::ModelSolver Inheritance solver::Solver solver::ModelSolver solver::SteppingSolver Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label set_time_limit get_result run copy_limits get_solver_properties finalize get_identifier ~Solver get_benchmark get_max_threads get_thread_count init Solver set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors ModelSolver() Declaration solver::ModelSolver<Model_T>::ModelSolver() Methods ~ModelSolver() Declaration solver::ModelSolver<Model_T>::~ModelSolver() override configure() Check the identifier and version against the configuraiton. Declaration void solver::ModelSolver<Model_T>::configure(const utils::Json&json) override set_model() Declaration virtual void solver::ModelSolver<Model_T>::set_model(const Model_T *model) get_model_properties() Declaration utils::Structure solver::ModelSolver<Model_T>::get_model_properties() const override get_model_sweep_size() Declaration size_t solver::ModelSolver<Model_T>::get_model_sweep_size() const override get_model_term_size() Declaration size_t solver::ModelSolver<Model_T>::get_model_term_size() const get_lowest_cost() Declaration Cost_T solver::ModelSolver<Model_T>::get_lowest_cost() const get_solutions() Get the structured result description. Declaration utils::Structure solver::ModelSolver<Model_T>::get_solutions() const override init_memory_check_error_message() Declaration virtual std::string solver::ModelSolver<Model_T>::init_memory_check_error_message() const =0 target_number_of_states() Declaration virtual size_t solver::ModelSolver<Model_T>::target_number_of_states() const =0 init_memory_check() Declaration virtual void solver::ModelSolver<Model_T>::init_memory_check() copy_solutions_other() Declaration void solver::ModelSolver<Model_T>::copy_solutions_other(ModelSolver *other, unsigned solutions_to_return) const copy_lowest_state() Declaration void solver::ModelSolver<Model_T>::copy_lowest_state(ModelSolver *other) const count_solutions() Declaration size_t solver::ModelSolver<Model_T>::count_solutions() const is_empty() Declaration virtual bool solver::ModelSolver<Model_T>::is_empty() const get_model() Declaration virtual const Model_T&solver::ModelSolver<Model_T>::get_model() const max_replicas_of_state() Declaration virtual size_t solver::ModelSolver<Model_T>::max_replicas_of_state() const max_replicas_adjusted_state() Declaration virtual size_t solver::ModelSolver<Model_T>::max_replicas_adjusted_state(uint32_t max_replicas_mult) const adjust_states() Declaration virtual size_t solver::ModelSolver<Model_T>::adjust_states(double input_states, uint32_t max_replicas_mult) const update_lowest_cost() Declaration bool solver::ModelSolver<Model_T>::update_lowest_cost(Cost_T cost, const State_T&state) copy_solutions() Declaration void solver::ModelSolver<Model_T>::copy_solutions(std::vector<Walker_T>&walkers, unsigned solutions_to_return) copy_solutions() Declaration void solver::ModelSolver<Model_T>::copy_solutions(Population<Walker_T>&population, unsigned solutions_to_return) populate_solutions() Declaration void solver::ModelSolver<Model_T>::populate_solutions(Solutions_T&solutions) get_model_unconst() Declaration Model_T* solver::ModelSolver<Model_T>::get_model_unconst()"
  },
  "api/solver/murex.html": {
    "href": "api/solver/murex.html",
    "title": "Class solver::Murex | qiotoolkit",
    "keywords": "Class solver::Murex Multi-Objective Replica Exchange \"murex\". This solver implements a replica exchange approach whereby, in addition to the variables (\"spins\"), the parameters are at different quenched values at every node. As a result, the method explores the lowest cost value that can be achieved at different \"mixing rates\" of the parameters. The (currently hard-coded) arrangement of the nodes is in a triangle such that the temperature of nodes in each diagonal agrees, while the parameters x and y are swept from (1,0) to (0,1) in equal steps. position weights 5 j ( 0, 1) 2 4  ( 0, 1) (.5,.5) 0 1 3 (.5,.5) ( 1, 0) ( 1, 0)  i This setup is intended to analyze the effect of a multi-objective model with different weights assigned to each objective: \\mathcal{H} = -x\\mathcal{H}_x(\\vec{s}) -y\\mathcal{H}_y(\\vec{s}) Replica exchanges happen between neighboring nodes (vertically and horizontally) and take place at a rate which take into consideration that the cost of the swapped states are modified by virtue of being placed at a new node (and, thus, new set of parameters). Note The solver requires the model being simulated to support parameters and have exactly two parameters (which are selected as x and y). They do not necessarily need to split the cost function as described. [At the time of this writing, only \"poly\" supports this]. Inheritance solver::SteppingSolver solver::Murex Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits finalize ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors Murex() Declaration solver::Murex<Model_T>::Murex() Murex() Declaration solver::Murex<Model_T>::Murex(const Murex&)=delete Methods operator=() Declaration Murex&solver::Murex<Model_T>::operator=(const Murex&)=delete get_identifier() Get the identifier of this solver. This identifier is denoted as the target in the request. Declaration std::string solver::Murex<Model_T>::get_identifier() const override init_memory_check_error_message() Declaration std::string solver::Murex<Model_T>::init_memory_check_error_message() const override target_number_of_states() Declaration size_t solver::Murex<Model_T>::target_number_of_states() const override init() Initialize the solver. Initialization entails creating the graph of nodes and initializing a replica (a metropolis walker containing a state) at each node. The set of nodes is currently hard coded do consist of nT diagonals with 1..nT nodes, respectively. Each of them is assigned a set of mixing weights such that the starting point (on the x axis) is [1,0] and the end point on the y axis is [0,1], with linear interpolation of the mixing weights in between: position weights 5 j ( 0, 1) 2 4  ( 0, 1) (.5,.5) 0 1 3 (.5,.5) ( 1, 0) ( 1, 0)  i The temperature is highest in the lower left corner and decreases by one temperature step until the lowest on the furthest diagonal. We use the i index to refer to the diagonal and the j index for the position on that diagnoal. As such, the results for the pareto front are found on the last nT nodes in the node list, which represent the diagonal at i=nT-1, j=[0..nT]. Declaration void solver::Murex<Model_T>::init() override make_step() Make a step. A step constitutes performing individual metropolis updates on the nodes, followed by measurements and replica exchange moves. We update exchange moves from even (odd) diagonals in even (odd) steps and switch the direction every 2 steps. Declaration void solver::Murex<Model_T>::make_step(uint64_t step) override get_solutions() Return the solutions found. As the solution we return the lowest_cost found in the furthest diagonal (i.e., at lowest temperature) and the states corresponding to this lowest cost. Declaration utils::Structure solver::Murex<Model_T>::get_solutions() const override configure() Check the identifier and version against the configuraiton. Declaration void solver::Murex<Model_T>::configure(const utils::Json&json) override"
  },
  "api/solver/murex/node.html": {
    "href": "api/solver/murex/node.html",
    "title": "Class solver::Murex::Node | qiotoolkit",
    "keywords": "Class solver::Murex::Node Representation of a node in the replica exchange grid. Inheritance solver::Murex::Node Methods memory_estimate() Declaration static size_t solver::Murex<Model_T>::Node::memory_estimate(const Model_T&model)"
  },
  "api/solver/paparameter-free.html": {
    "href": "api/solver/paparameter-free.html",
    "title": "Class solver::PAParameterFree | qiotoolkit",
    "keywords": "Class solver::PAParameterFree PAParameterFree is the class which implements ParameterFreeAdapterInterface to fit in parameter free solver framework. And it is a child of PopulationAnnealing so that it immediately has the functionalities of PA. Inheritance solver::PopulationAnnealing solver::ParameterFreeAdapterInterface solver::PAParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver get_solutions copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state is_friction_tensor resample finalize is_constant_culling target_number_of_states make_step is_energy_variance is_linear_schedule PopulationAnnealing PopulationAnnealing operator= init_population init find_delta_beta is_geometric_schedule expand_population get_identifier init_memory_check_error_message set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors PAParameterFree() Declaration solver::PAParameterFree<Model_T>::PAParameterFree() PAParameterFree() Declaration solver::PAParameterFree<Model_T>::PAParameterFree(const PAParameterFree&)=delete Methods operator=() Declaration PAParameterFree&solver::PAParameterFree<Model_T>::operator=(const PAParameterFree&)=delete configure() Check the identifier and version against the configuraiton. Declaration void solver::PAParameterFree<Model_T>::configure(const utils::Json&json) override update_parameters_linearly() Declaration void solver::PAParameterFree<Model_T>::update_parameters_linearly(std::vector<double>&parameters) const override parameter_dimensions() Declaration size_t solver::PAParameterFree<Model_T>::parameter_dimensions() const override parameter_ranges() Declaration std::vector<std::pair<double, double>>solver::PAParameterFree<Model_T>::parameter_ranges() const override get_initial_parameter_values() Declaration void solver::PAParameterFree<Model_T>::get_initial_parameter_values(std::vector<double>&initials) override update_parameters() Declaration void solver::PAParameterFree<Model_T>::update_parameters(const std::vector<double>&parameters, double left_over_time) override estimate_execution_cost() Declaration double solver::PAParameterFree<Model_T>::estimate_execution_cost() const override"
  },
  "api/solver/parallel-tempering.html": {
    "href": "api/solver/parallel-tempering.html",
    "title": "Class solver::ParallelTempering | qiotoolkit",
    "keywords": "Class solver::ParallelTempering Parallel Tempering algorithm. Run multiple metropolis simulating at separate temperatures, allowing exchange of configuration between neighboring replicas between sweeps. Note Use of the Metropolis Algorithm is currently hard-coded, but this could in principle be a template argument, provided that the alternative method implements the corresponding interfaces for exchange moves. Inheritance solver::SteppingSolver solver::ParallelTempering solver::PTParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver get_solutions copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors ParallelTempering() Create an uninitialized Parallel Tempering instance. Declaration solver::ParallelTempering<Model_T>::ParallelTempering() ParallelTempering() Declaration solver::ParallelTempering<Model_T>::ParallelTempering(const ParallelTempering&)=delete Methods operator=() Declaration ParallelTempering&solver::ParallelTempering<Model_T>::operator=(const ParallelTempering&)=delete get_temperatures() Declaration std::vector<double>solver::ParallelTempering<Model_T>::get_temperatures() use_inverse_temperatures() Declaration bool solver::ParallelTempering<Model_T>::use_inverse_temperatures() get_identifier() Identifier of this solver (target in the request) Declaration std::string solver::ParallelTempering<Model_T>::get_identifier() const override init_memory_check_error_message() Declaration std::string solver::ParallelTempering<Model_T>::init_memory_check_error_message() const override target_number_of_states() Declaration size_t solver::ParallelTempering<Model_T>::target_number_of_states() const override init() Initialize each replica. Declaration void solver::ParallelTempering<Model_T>::init() override accept() Decide whether to accept an exchange move. Declaration double solver::ParallelTempering<Model_T>::accept(const markov::Metropolis<Model_T>&a, const markov::Metropolis<Model_T>&b) make_step() Perform discrete time step t. Declaration void solver::ParallelTempering<Model_T>::make_step(uint64_t step) override configure() Configure. Declaration void solver::ParallelTempering<Model_T>::configure(const utils::Json&json) override finalize() Declaration void solver::ParallelTempering<Model_T>::finalize() override"
  },
  "api/solver/parameter-free-adapter-interface-base.html": {
    "href": "api/solver/parameter-free-adapter-interface-base.html",
    "title": "Class solver::ParameterFreeAdapterInterfaceBase | qiotoolkit",
    "keywords": "Class solver::ParameterFreeAdapterInterfaceBase Interface classes for solver to fit in the parameter free framework. To be added in parameter freeEach solver must implement one of interface: ParameterFreeAdapterInterface or ParameterFreeLinearSearchAdapterInterface. Common interfaces Inheritance solver::ParameterFreeAdapterInterfaceBase solver::ParameterFreeAdapterInterface solver::ParameterFreeLinearSearchAdapterInterface Methods parameter_dimensions() Declaration virtual size_t solver::ParameterFreeAdapterInterfaceBase::parameter_dimensions() const =0 get_initial_parameter_values() Declaration virtual void solver::ParameterFreeAdapterInterfaceBase::get_initial_parameter_values(std::vector<double>&initials)=0 update_parameters() Declaration virtual void solver::ParameterFreeAdapterInterfaceBase::update_parameters(const std::vector<double>&parameters, double start_time)=0 update_parameters_linearly() Declaration virtual void solver::ParameterFreeAdapterInterfaceBase::update_parameters_linearly(std::vector<double>&parameters) const =0 estimate_execution_cost() Declaration virtual double solver::ParameterFreeAdapterInterfaceBase::estimate_execution_cost() const =0"
  },
  "api/solver/parameter-free-adapter-interface.html": {
    "href": "api/solver/parameter-free-adapter-interface.html",
    "title": "Class solver::ParameterFreeAdapterInterface | qiotoolkit",
    "keywords": "Class solver::ParameterFreeAdapterInterface Interface class for solver to use with Bayesian Global Optimization stategy. Inheritance solver::ParameterFreeAdapterInterfaceBase solver::ParameterFreeAdapterInterface solver::PAParameterFree solver::SSMCParameterFree Inherited Members update_parameters_linearly update_parameters parameter_dimensions estimate_execution_cost get_initial_parameter_values Methods parameter_ranges() Declaration virtual std::vector<std::pair<double, double>>solver::ParameterFreeAdapterInterface::parameter_ranges() const =0"
  },
  "api/solver/parameter-free-linear-search-adapter-interface.html": {
    "href": "api/solver/parameter-free-linear-search-adapter-interface.html",
    "title": "Class solver::ParameterFreeLinearSearchAdapterInterface | qiotoolkit",
    "keywords": "Class solver::ParameterFreeLinearSearchAdapterInterface Interface class for solver to use with Linear Search Global Optimization stategy. Inheritance solver::ParameterFreeAdapterInterfaceBase solver::ParameterFreeLinearSearchAdapterInterface solver::PTParameterFree solver::SAParameterFree solver::TabuParameterFree Inherited Members update_parameters parameter_dimensions estimate_execution_cost Methods parameter_ranges() Declaration virtual std::vector<int>solver::ParameterFreeLinearSearchAdapterInterface::parameter_ranges() const =0 update_parameters_linearly() Declaration void solver::ParameterFreeLinearSearchAdapterInterface::update_parameters_linearly(std::vector<double>&) const override get_initial_parameter_values() Declaration void solver::ParameterFreeLinearSearchAdapterInterface::get_initial_parameter_values(std::vector<double>&initials) override"
  },
  "api/solver/parameter-free-linear-search-solver.html": {
    "href": "api/solver/parameter-free-linear-search-solver.html",
    "title": "Class solver::ParameterFreeLinearSearchSolver | qiotoolkit",
    "keywords": "Class solver::ParameterFreeLinearSearchSolver Inheritance solver::ParameterFreeSolver solver::ParameterFreeLinearSearchSolver Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver get_solutions copy_lowest_state target_number_of_states copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state get_model_sweep_size init_memory_check_error_message is_empty get_lowest_cost adjust_states max_replicas_of_state get_identifier operator= set_model get_solver_properties get_model get_model_properties get_output_parameters finalize target_number_of_states configure ~ParameterFreeSolver make_step init ParameterFreeSolver init_memory_check_error_message update_best ParameterFreeSolver set_time_limit get_result copy_limits get_identifier ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors ParameterFreeLinearSearchSolver() Declaration solver::ParameterFreeLinearSearchSolver<Model_T, SolverAdapter>::ParameterFreeLinearSearchSolver() ParameterFreeLinearSearchSolver() Declaration solver::ParameterFreeLinearSearchSolver<Model_T, SolverAdapter>::ParameterFreeLinearSearchSolver(const ParameterFreeLinearSearchSolver&)=delete Methods operator=() Declaration ParameterFreeLinearSearchSolver&solver::ParameterFreeLinearSearchSolver<Model_T, SolverAdapter>::operator=(const ParameterFreeLinearSearchSolver&)=delete ~ParameterFreeLinearSearchSolver() Declaration solver::ParameterFreeLinearSearchSolver<Model_T, SolverAdapter>::~ParameterFreeLinearSearchSolver() training_parameters() Declaration void solver::ParameterFreeLinearSearchSolver<Model_T, SolverAdapter>::training_parameters(const utils::Json&) override"
  },
  "api/solver/parameter-free-solver.html": {
    "href": "api/solver/parameter-free-solver.html",
    "title": "Class solver::ParameterFreeSolver | qiotoolkit",
    "keywords": "Class solver::ParameterFreeSolver Inheritance solver::SteppingSolver solver::ParameterFreeSolver Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver get_solutions copy_lowest_state copy_solutions update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param ~Component Component get_status get_class_name Constructors ParameterFreeSolver() Declaration solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::ParameterFreeSolver() ParameterFreeSolver() Declaration solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::ParameterFreeSolver(const ParameterFreeSolver&)=delete Methods operator=() Declaration ParameterFreeSolver&solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::operator=(const ParameterFreeSolver&)=delete ~ParameterFreeSolver() Declaration solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::~ParameterFreeSolver() get_identifier() Identifier of this solver (target in the request) Declaration std::string solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::get_identifier() const override init() Initialize the solver. Declaration void solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::init() override init_memory_check_error_message() Declaration std::string solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::init_memory_check_error_message() const override target_number_of_states() Declaration virtual size_t solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::target_number_of_states() const override configure() Read the maximum number of steps from configuration. Declaration void solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::configure(const utils::Json&json) override make_step() Declaration void solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::make_step(uint64_t step) override finalize() Declaration void solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::finalize() override get_model_properties() Declaration utils::Structure solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::get_model_properties() const override get_solver_properties() Declaration utils::Structure solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::get_solver_properties() const override get_output_parameters() Get a structure containing all output_parameters. Declaration utils::Structure solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::get_output_parameters() const override get_model() Declaration const Model_T&solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::get_model() const override set_model() Declaration void solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::set_model(const Model_T *model) override training_parameters() Declaration virtual void solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::training_parameters(const utils::Json&parameters) update_best() Declaration void solver::ParameterFreeSolver<Model_T, SolverAdapter, StrategyType>::update_best(double objective, std::vector<double>&parameters, double estimated_execution_cost)"
  },
  "api/solver/population-annealing.html": {
    "href": "api/solver/population-annealing.html",
    "title": "Class solver::PopulationAnnealing | qiotoolkit",
    "keywords": "Class solver::PopulationAnnealing Population Annealing. Population based optimizer with variable beta stepping. PopulationAnnealing explores the configuration space using multiple Metropolis walkers subject to a birth-death process (depending on their relative cost-fitness within the population). Stepping method Population annealing supports three different resampling_strategys for dynamically selecting the next step in temperature space (delta_beta): friction_tensor: The stepping size is calculated from \\Delta\\beta = c_{FT} / \\sqrt\\zeta { 'target': 'populationannealing.cpu', 'version': '1.0', 'resampling_strategy': 'friction_tensor', 'friction_tensor_constant': 1 } energy_variance: Scale temperature stepping with the variance in the energy (cost): \\Delta\\beta = f_{culling} / \\sqrt\\sigma for this, the initial value for the culling fraction needs to be specified: { 'target': 'populationannealing.cpu', 'version': '1.0', 'resampling_strategy': 'energy_variance', 'initial_culling_fraction': 0.5 } constant_culling: The stepping size is chosen to keep the culling rate constant at culling_fraction (by estimating the expected death ratio and solving for delta_beta): { 'target': 'populationannealing.cpu', 'version': '1.0', 'resampling_strategy': 'constant_culling', 'culling_fraction': 0.1 } Inheritance solver::SteppingSolver solver::PopulationAnnealing solver::PAParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver get_solutions copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors PopulationAnnealing() Declaration solver::PopulationAnnealing<Model_T>::PopulationAnnealing() PopulationAnnealing() Declaration solver::PopulationAnnealing<Model_T>::PopulationAnnealing(const PopulationAnnealing&)=delete Methods operator=() Declaration PopulationAnnealing&solver::PopulationAnnealing<Model_T>::operator=(const PopulationAnnealing&)=delete get_identifier() Identifier of this solver (target in the request) Declaration std::string solver::PopulationAnnealing<Model_T>::get_identifier() const override init_memory_check_error_message() Declaration std::string solver::PopulationAnnealing<Model_T>::init_memory_check_error_message() const override target_number_of_states() Declaration size_t solver::PopulationAnnealing<Model_T>::target_number_of_states() const override init() Initialize the solver. Declaration void solver::PopulationAnnealing<Model_T>::init() override init_population() Reset the population with size target_population Note This method is invoked both at the start of the simulation and whenever a restart/population increase is triggered. Declaration void solver::PopulationAnnealing<Model_T>::init_population() find_delta_beta() Adjust the stepping size delta_beta until the estimator returns an estimate of zero. NOTE: This is NOT a generic root finder, it makes several assumptions about the estimator being used. Declaration double solver::PopulationAnnealing<Model_T>::find_delta_beta(T&estimator) make_step() Declaration void solver::PopulationAnnealing<Model_T>::make_step(uint64_t step) override resample() Declaration void solver::PopulationAnnealing<Model_T>::resample(double delta_beta) configure() Check the identifier and version against the configuraiton. Declaration void solver::PopulationAnnealing<Model_T>::configure(const utils::Json&json) override finalize() Declaration void solver::PopulationAnnealing<Model_T>::finalize() override expand_population() expand the population to size target_population Note This method is called to expand population from current size to target_population Declaration void solver::PopulationAnnealing<Model_T>::expand_population() is_linear_schedule() Declaration bool solver::PopulationAnnealing<Model_T>::is_linear_schedule() const is_geometric_schedule() Declaration bool solver::PopulationAnnealing<Model_T>::is_geometric_schedule() const is_friction_tensor() Declaration bool solver::PopulationAnnealing<Model_T>::is_friction_tensor() const is_energy_variance() Declaration bool solver::PopulationAnnealing<Model_T>::is_energy_variance() const is_constant_culling() Declaration bool solver::PopulationAnnealing<Model_T>::is_constant_culling() const"
  },
  "api/solver/population-annealing/culling-estimator.html": {
    "href": "api/solver/population-annealing/culling-estimator.html",
    "title": "Class solver::PopulationAnnealing::CullingEstimator | qiotoolkit",
    "keywords": "Class solver::PopulationAnnealing::CullingEstimator The CullingEstimator is used to infer the expected culling ratio for a proposed temperature stepping delta_beta for the \"constant_culling\" stepping method. (It is used by the find_delta_beta method below to iteratively find the appropriate stepping to keep culling approximately constant). Inheritance solver::PopulationAnnealing::CullingEstimator Constructors CullingEstimator() Declaration solver::PopulationAnnealing<Model_T>::CullingEstimator::CullingEstimator(const std::vector<double>&costs, double epsilon0, size_t target_population, double scale_factor) CullingEstimator() Declaration solver::PopulationAnnealing<Model_T>::CullingEstimator::CullingEstimator(const CullingEstimator&)=delete Methods operator=() Declaration CullingEstimator&solver::PopulationAnnealing<Model_T>::CullingEstimator::operator=(const CullingEstimator&)=delete operator=() Declaration CullingEstimator&solver::PopulationAnnealing<Model_T>::CullingEstimator::operator=(CullingEstimator&&)=delete operator()() Declaration double solver::PopulationAnnealing<Model_T>::CullingEstimator::operator()(double delta_beta)"
  },
  "api/solver/population.html": {
    "href": "api/solver/population.html",
    "title": "Class solver::Population | qiotoolkit",
    "keywords": "Class solver::Population A population is a container for several instances of Content_T Each instance of Content_T is internally wrapped in a Citizen wrapper, which allows setting the desired duplicity count of the contained instance after resampling. These new counts are, however, only applied after a call to resample(). Example: Populationstd::string population; population.insert(\"foo\"); population.insert(\"bar\"); population.insert(\"baz\"); for (size_t i = 0; i < population.size(); i++) { auto& citizen = population[i]; if (citizen.content() == \"foo\") citizen.kill() if (citizen.content() == \"baz\") citizen.spawn(2) } // output before resampling: \"foobarbaz\" for (size_t i = 0; i < population.size(); i++) { std::cout << population[i].content(); } population.resample(); // output after resampling: \"bazbarbazbaz\" for (size_t i = 0; i < population.size(); i++) { std::cout << population[i].content(); } Inheritance utils::Component solver::Population Inherited Members configure render ~Component Component get_status param get_class_name Constructors Population() Declaration solver::Population<Content_T>::Population() Methods reserve() Declaration void solver::Population<Content_T>::reserve(size_t size) insert() Add content to the population (with a count of 1). Declaration void solver::Population<Content_T>::insert(Content_T content) resize() Declaration void solver::Population<Content_T>::resize(size_t new_count) size() Get the current number of citizens. This does not consider current counts set on the citizens, so it should be called after resample(), prior to any set_count() on the citizens. Declaration size_t solver::Population<Content_T>::size() const empty() Check if the population is empty. Declaration bool solver::Population<Content_T>::empty() operator[]() Positional access to citizens. They are in no particular order, but the order remains \"stable\" during resampling. Declaration const Citizen&solver::Population<Content_T>::operator[](size_t position) const operator[]() Declaration Citizen&solver::Population<Content_T>::operator[](size_t position) resample() Resample the population such that count_ copies of each citizen in the current population are present in the resampled one. Declaration void solver::Population<Content_T>::resample() get_families() Returns a map describing the sizes of the remaining families. Declaration const std::map<FamilyId, size_t>&solver::Population<Content_T>::get_families() partial_sort() Declaration void solver::Population<Content_T>::partial_sort(unsigned solutions_to_sort) clear() Declaration void solver::Population<Content_T>::clear() clear_cache_pool() Declaration void solver::Population<Content_T>::clear_cache_pool() do_census() Declaration void solver::Population<Content_T>::do_census() create_citizen() Declaration void solver::Population<Content_T>::create_citizen(std::unique_ptr<Citizen>&citizen, const Content_T&content) remove_citizen() Declaration void solver::Population<Content_T>::remove_citizen(std::unique_ptr<Citizen>&citizen)"
  },
  "api/solver/population/citizen.html": {
    "href": "api/solver/population/citizen.html",
    "title": "Class solver::Population::Citizen | qiotoolkit",
    "keywords": "Class solver::Population::Citizen Inheritance solver::Population::Citizen Constructors Citizen() Declaration solver::Population<Content_T>::Citizen::Citizen() Citizen() Declaration solver::Population<Content_T>::Citizen::Citizen(const Citizen&other)=delete Citizen() Declaration solver::Population<Content_T>::Citizen::Citizen(Content_T content) Methods get_count() Declaration size_t solver::Population<Content_T>::Citizen::get_count() const set_count() Declaration void solver::Population<Content_T>::Citizen::set_count(size_t count) kill() Declaration void solver::Population<Content_T>::Citizen::kill() spawn() Declaration void solver::Population<Content_T>::Citizen::spawn(size_t n) set_family() Declaration void solver::Population<Content_T>::Citizen::set_family(FamilyId family_id) get_family() Declaration FamilyId solver::Population<Content_T>::Citizen::get_family() const content() Declaration const Content_T&solver::Population<Content_T>::Citizen::content() const content() Declaration Content_T&solver::Population<Content_T>::Citizen::content() operator*() Declaration const Content_T&solver::Population<Content_T>::Citizen::operator*() const operator*() Declaration Content_T&solver::Population<Content_T>::Citizen::operator*() >() Declaration const Content_T* solver::Population<Content_T>::Citizen::operator->() const >() Declaration const Content_T* solver::Population<Content_T>::Citizen::operator->() const >() Declaration Content_T* solver::Population<Content_T>::Citizen::operator->() >() Declaration Content_T* solver::Population<Content_T>::Citizen::operator->() compare() Declaration static bool solver::Population<Content_T>::Citizen::compare(const std::unique_ptr<Citizen>&c1, const std::unique_ptr<Citizen>&c2) operator=() Declaration Citizen&solver::Population<Content_T>::Citizen::operator=(const Citizen&other)"
  },
  "api/solver/ptparameter-free.html": {
    "href": "api/solver/ptparameter-free.html",
    "title": "Class solver::PTParameterFree | qiotoolkit",
    "keywords": "Class solver::PTParameterFree PTParameterFree is the class which implements ParameterFreeAdapterInterface to fit in parameter free solver framework. And it is a child of ParallelTempering so that it immediately has the functionalities of PT. Inheritance solver::ParallelTempering solver::ParameterFreeLinearSearchAdapterInterface solver::PTParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver get_solutions copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state target_number_of_states init ParallelTempering get_temperatures get_identifier use_inverse_temperatures ParallelTempering finalize operator= accept init_memory_check_error_message make_step get_initial_parameter_values update_parameters_linearly set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors PTParameterFree() Declaration solver::PTParameterFree<Model_T>::PTParameterFree() PTParameterFree() Declaration solver::PTParameterFree<Model_T>::PTParameterFree(const PTParameterFree&)=delete Methods operator=() Declaration PTParameterFree&solver::PTParameterFree<Model_T>::operator=(const PTParameterFree&)=delete configure() Configure. Declaration void solver::PTParameterFree<Model_T>::configure(const utils::Json&json) override parameter_dimensions() Declaration size_t solver::PTParameterFree<Model_T>::parameter_dimensions() const override parameter_ranges() Declaration std::vector<int>solver::PTParameterFree<Model_T>::parameter_ranges() const override update_parameters() Declaration void solver::PTParameterFree<Model_T>::update_parameters(const std::vector<double>&parameters, double left_over_time) override estimate_execution_cost() Declaration double solver::PTParameterFree<Model_T>::estimate_execution_cost() const override determine_all_betas_() Declaration void solver::PTParameterFree<Model_T>::determine_all_betas_(double lowest_beta, double highest_beta, size_t num_betas, std::vector<double>&betas)"
  },
  "api/solver/quantum-monte-carlo.html": {
    "href": "api/solver/quantum-monte-carlo.html",
    "title": "Class solver::QuantumMonteCarlo | qiotoolkit",
    "keywords": "Class solver::QuantumMonteCarlo Inheritance solver::SteppingSolver solver::QuantumMonteCarlo Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors QuantumMonteCarlo() Declaration solver::QuantumMonteCarlo<Model_T>::QuantumMonteCarlo() QuantumMonteCarlo() Declaration solver::QuantumMonteCarlo<Model_T>::QuantumMonteCarlo(const QuantumMonteCarlo&)=delete Methods operator=() Declaration QuantumMonteCarlo&solver::QuantumMonteCarlo<Model_T>::operator=(const QuantumMonteCarlo&)=delete get_identifier() Identifier of this solver (target in the request) Declaration std::string solver::QuantumMonteCarlo<Model_T>::get_identifier() const override init_memory_check_error_message() Declaration std::string solver::QuantumMonteCarlo<Model_T>::init_memory_check_error_message() const override target_number_of_states() Declaration size_t solver::QuantumMonteCarlo<Model_T>::target_number_of_states() const override init() Initialize the solver. Declaration void solver::QuantumMonteCarlo<Model_T>::init() override init_replicas() Declaration void solver::QuantumMonteCarlo<Model_T>::init_replicas() make_step() Declaration void solver::QuantumMonteCarlo<Model_T>::make_step(uint64_t step) override make_sweeps() Declaration void solver::QuantumMonteCarlo<Model_T>::make_sweeps(double beta, double bond_prob) get_solutions() Get the structured result description. Declaration utils::Structure solver::QuantumMonteCarlo<Model_T>::get_solutions() const override configure() Check the identifier and version against the configuraiton. Declaration void solver::QuantumMonteCarlo<Model_T>::configure(const utils::Json&json) override calc_beta() Declaration double solver::QuantumMonteCarlo<Model_T>::calc_beta(double beta) calc_bond_prob() Declaration double solver::QuantumMonteCarlo<Model_T>::calc_bond_prob(double transverse_field, double beta) finalize() Declaration void solver::QuantumMonteCarlo<Model_T>::finalize() override"
  },
  "api/solver/saparameter-free.html": {
    "href": "api/solver/saparameter-free.html",
    "title": "Class solver::SAParameterFree | qiotoolkit",
    "keywords": "Class solver::SAParameterFree SAParameterFree is the class which implements ParameterFreeAdapterInterface to fit in parameter free solver framework. And it is a child of SimulatedAnnealing so that it immediately has the functionalities of SA. Inheritance solver::SimulatedAnnealing solver::ParameterFreeLinearSearchAdapterInterface solver::SAParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state get_initial_parameter_values update_parameters_linearly finalize target_number_of_states get_solutions make_step init get_identifier SimulatedAnnealing operator= init_memory_check_error_message use_inverse_temperature SimulatedAnnealing set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors SAParameterFree() Declaration solver::SAParameterFree<Model_T>::SAParameterFree() SAParameterFree() Declaration solver::SAParameterFree<Model_T>::SAParameterFree(const SAParameterFree&)=delete Methods operator=() Declaration SAParameterFree&solver::SAParameterFree<Model_T>::operator=(const SAParameterFree&)=delete configure() Check the identifier and version against the configuraiton. Declaration void solver::SAParameterFree<Model_T>::configure(const utils::Json&json) override parameter_dimensions() Declaration size_t solver::SAParameterFree<Model_T>::parameter_dimensions() const override parameter_ranges() Declaration std::vector<int>solver::SAParameterFree<Model_T>::parameter_ranges() const override update_parameters() Declaration void solver::SAParameterFree<Model_T>::update_parameters(const std::vector<double>&parameters, double left_over_time) override estimate_execution_cost() Declaration double solver::SAParameterFree<Model_T>::estimate_execution_cost() const override"
  },
  "api/solver/simulated-annealing.html": {
    "href": "api/solver/simulated-annealing.html",
    "title": "Class solver::SimulatedAnnealing | qiotoolkit",
    "keywords": "Class solver::SimulatedAnnealing Inheritance solver::SteppingSolver solver::SimulatedAnnealing solver::SAParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors SimulatedAnnealing() Declaration solver::SimulatedAnnealing<Model_T>::SimulatedAnnealing() SimulatedAnnealing() Declaration solver::SimulatedAnnealing<Model_T>::SimulatedAnnealing(const SimulatedAnnealing&)=delete Methods operator=() Declaration SimulatedAnnealing&solver::SimulatedAnnealing<Model_T>::operator=(const SimulatedAnnealing&)=delete get_identifier() Identifier of this solver (target in the request) Declaration std::string solver::SimulatedAnnealing<Model_T>::get_identifier() const override init_memory_check_error_message() Declaration std::string solver::SimulatedAnnealing<Model_T>::init_memory_check_error_message() const override target_number_of_states() Declaration size_t solver::SimulatedAnnealing<Model_T>::target_number_of_states() const override init() Initialize the solver. Declaration void solver::SimulatedAnnealing<Model_T>::init() override make_step() Declaration void solver::SimulatedAnnealing<Model_T>::make_step(uint64_t step) override get_solutions() Get the structured result description. Declaration utils::Structure solver::SimulatedAnnealing<Model_T>::get_solutions() const override configure() Check the identifier and version against the configuraiton. Declaration void solver::SimulatedAnnealing<Model_T>::configure(const utils::Json&json) override use_inverse_temperature() Declaration bool solver::SimulatedAnnealing<Model_T>::use_inverse_temperature() const finalize() Declaration void solver::SimulatedAnnealing<Model_T>::finalize() override"
  },
  "api/solver/solver-registrar.html": {
    "href": "api/solver/solver-registrar.html",
    "title": "Struct solver::SolverRegistrar | qiotoolkit",
    "keywords": "Struct solver::SolverRegistrar Helper struct to statically fill the registry. Constructors SolverRegistrar() Declaration solver::SolverRegistrar<Solver_T>::SolverRegistrar(int)"
  },
  "api/solver/solver-registration.html": {
    "href": "api/solver/solver-registration.html",
    "title": "Class solver::SolverRegistration | qiotoolkit",
    "keywords": "Class solver::SolverRegistration Registration of a templateable solver. The registration carries only the identifier of the solver for now; the actual instantiation is handled by the templated create_solver method declared above. Inheritance solver::SolverRegistration Constructors SolverRegistration() Declaration solver::SolverRegistration::SolverRegistration(const std::string&identifier) Methods create_for_model() Method to forward solver instantiation to create_solver Declaration Solver* solver::SolverRegistration::create_for_model() const"
  },
  "api/solver/solver-registry.html": {
    "href": "api/solver/solver-registry.html",
    "title": "Class solver::SolverRegistry | qiotoolkit",
    "keywords": "Class solver::SolverRegistry Registry of available solvers. The solver registry allows the instantiation of each registered solver for a given model. Usage: Registering a new model: class MySolver : public [solver::Solver](xref:classsolver_1_1Solver) { public: // Must have a constructor without arguments MySolver() = default; // This is the identifier the solver is register with. std::string get_identifier() const override { return \"my_solver\"; } ... } [REGISTER_SOLVER(MySolver)](xref:solver__registry_8h_1a7f2e334b3ffcbd91a8d8c1275ff11ec5); ``` * Make sure the solver is included in `all_solver.h` and handled in the `create_solver<Model_T>` method. * Instantiating a solver for a specific model: ```c++ auto* my_solver = [solver::SolverRegistry::create_for_model<MyModel>](xref:classsolver_1_1SolverRegistry_1a7a4df54052a7e300382bec91d072d409)(\"my_solver\"); ``` Inheritance solver::SolverRegistry Constructors SolverRegistry() This is a singleton class. Declaration solver::SolverRegistry::SolverRegistry() Methods has() Check whether a solver with that identifier is registered. Declaration static bool solver::SolverRegistry::has(const std::string&identifier) get() Find and return the solver registration. Declaration static const SolverRegistration* solver::SolverRegistry::get(const std::string&identifier) create_for_model() Find the solver registration and create an instance for Model_T. Declaration static Solver* solver::SolverRegistry::create_for_model(const std::string&identifier) add() Add an entry to the solver registry. This is used by the SolverRegistrar in the REGISTER_SOLVER macro, you should not need to invoke it directly. Declaration static void solver::SolverRegistry::add(const std::string&identifier, std::unique_ptr<SolverRegistration>&&registration) instance() Access the singleton. Declaration static SolverRegistry&solver::SolverRegistry::instance()"
  },
  "api/solver/solver.html": {
    "href": "api/solver/solver.html",
    "title": "Class solver::Solver | qiotoolkit",
    "keywords": "Class solver::Solver Interface for a Solver. A solver is initialized, run and then asked to print a result. Inheritance observe::Observer solver::Solver solver::ModelSolver Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors Solver() Intanstiate an uninitialized Solver. Declaration solver::Solver::Solver() Methods ~Solver() Declaration virtual solver::Solver::~Solver() get_identifier() Get the identifier of this solver. This identifier is denoted as the target in the request. Declaration virtual std::string solver::Solver::get_identifier() const =0 init() Initialize the solver. Declaration virtual void solver::Solver::init()=0 run() Run the solver. SteppingSolver Declaration virtual void solver::Solver::run()=0 get_solutions() Get the structured result description. Declaration virtual utils::Structure solver::Solver::get_solutions() const =0 get_model_properties() Declaration virtual utils::Structure solver::Solver::get_model_properties() const get_solver_properties() Declaration virtual utils::Structure solver::Solver::get_solver_properties() const get_model_sweep_size() Declaration virtual size_t solver::Solver::get_model_sweep_size() const =0 get_benchmark() Declaration utils::Structure solver::Solver::get_benchmark() const get_result() Declaration utils::Structure solver::Solver::get_result() const configure() Check the identifier and version against the configuraiton. Declaration virtual void solver::Solver::configure(const utils::Json&json) get_thread_count() Return the number of threads which should be used in parallel sections of the solver. Declaration int solver::Solver::get_thread_count() const finalize() Declaration virtual void solver::Solver::finalize() copy_limits() Declaration void solver::Solver::copy_limits(Solver *other) const set_time_limit() Declaration void solver::Solver::set_time_limit(double value) get_max_threads() Return the maximum number of threads this solver can use. Declaration int solver::Solver::get_max_threads() const"
  },
  "api/solver/ssmcparameter-free.html": {
    "href": "api/solver/ssmcparameter-free.html",
    "title": "Class solver::SSMCParameterFree | qiotoolkit",
    "keywords": "Class solver::SSMCParameterFree SSMCParameterFree is the class which implements ParameterFreeAdapterInterface to fit in parameter free solver framework. And it is a child of SubstochasticMonteCarlo so that it immediately has the functionalities of SSMC. Inheritance solver::SubstochasticMonteCarlo solver::ParameterFreeAdapterInterface solver::SSMCParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step make_step init_memory_check_error_message update_population_statistics finalize prepare_population operator= resample_population SubstochasticMonteCarlo get_identifier make_walker_steps SubstochasticMonteCarlo init get_solutions target_number_of_states set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors SSMCParameterFree() Declaration solver::SSMCParameterFree<Model_T>::SSMCParameterFree() SSMCParameterFree() Declaration solver::SSMCParameterFree<Model_T>::SSMCParameterFree(const SSMCParameterFree&)=delete Methods operator=() Declaration SSMCParameterFree&solver::SSMCParameterFree<Model_T>::operator=(const SSMCParameterFree&)=delete configure() Check the identifier and version against the configuraiton. Declaration void solver::SSMCParameterFree<Model_T>::configure(const utils::Json&json) override parameter_dimensions() Declaration size_t solver::SSMCParameterFree<Model_T>::parameter_dimensions() const override parameter_ranges() Declaration std::vector<std::pair<double, double>>solver::SSMCParameterFree<Model_T>::parameter_ranges() const override get_initial_parameter_values() Declaration void solver::SSMCParameterFree<Model_T>::get_initial_parameter_values(std::vector<double>&initials) override update_parameters() Declaration void solver::SSMCParameterFree<Model_T>::update_parameters(const std::vector<double>&parameters, double left_over_time) override update_parameters_linearly() Declaration void solver::SSMCParameterFree<Model_T>::update_parameters_linearly(std::vector<double>&parameters) const override estimate_execution_cost() Declaration double solver::SSMCParameterFree<Model_T>::estimate_execution_cost() const override"
  },
  "api/solver/stepping-solver.html": {
    "href": "api/solver/stepping-solver.html",
    "title": "Class solver::SteppingSolver | qiotoolkit",
    "keywords": "Class solver::SteppingSolver Optimization in discretized steps. This base class assumes the solver runs in discretized steps t and handles the configuration of step_limit, increments t and calls make_step() for every integer value [0, step_limit]. Inheritance solver::ModelSolver solver::SteppingSolver examples::Descent solver::Murex solver::ParallelTempering solver::ParameterFreeSolver solver::PopulationAnnealing solver::QuantumMonteCarlo solver::SimulatedAnnealing solver::SubstochasticMonteCarlo solver::Tabu Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver get_solutions copy_lowest_state target_number_of_states copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size init_memory_check_error_message is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits finalize get_identifier ~Solver get_benchmark get_max_threads get_thread_count Solver set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors SteppingSolver() Declaration solver::SteppingSolver<Model_T>::SteppingSolver() Methods ~SteppingSolver() Declaration solver::SteppingSolver<Model_T>::~SteppingSolver() override configure() Read the maximum number of steps from configuration. Declaration void solver::SteppingSolver<Model_T>::configure(const utils::Json&json) override init() Initialize the solver. Declaration virtual void solver::SteppingSolver<Model_T>::init() override=0 make_step() Declaration virtual void solver::SteppingSolver<Model_T>::make_step(uint64_t step)=0 get_cost_function_evaluation_count() Declaration uint64_t solver::SteppingSolver<Model_T>::get_cost_function_evaluation_count() const status() Declaration virtual void solver::SteppingSolver<Model_T>::status() const run() A stepping solver is run by invoking / make_step(t) for every value of t. Declaration void solver::SteppingSolver<Model_T>::run() override get_solver_properties() Declaration utils::Structure solver::SteppingSolver<Model_T>::get_solver_properties() const override get_runtime() Declaration double solver::SteppingSolver<Model_T>::get_runtime() const get_seed() Declaration unsigned int solver::SteppingSolver<Model_T>::get_seed() const current_steps() Declaration virtual uint64_t solver::SteppingSolver<Model_T>::current_steps() const update_accumulated_info() Declaration void solver::SteppingSolver<Model_T>::update_accumulated_info() fixed_step_per_tick() Declaration void solver::SteppingSolver<Model_T>::fixed_step_per_tick(uint64_t value) reset() Declaration void solver::SteppingSolver<Model_T>::reset(double left_over_time) handle_signals() Declaration bool solver::SteppingSolver<Model_T>::handle_signals() seconds_per_step() Declaration double solver::SteppingSolver<Model_T>::seconds_per_step() const"
  },
  "api/solver/substochastic-monte-carlo.html": {
    "href": "api/solver/substochastic-monte-carlo.html",
    "title": "Class solver::SubstochasticMonteCarlo | qiotoolkit",
    "keywords": "Class solver::SubstochasticMonteCarlo Inheritance solver::SteppingSolver solver::SubstochasticMonteCarlo solver::SSMCParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors SubstochasticMonteCarlo() Declaration solver::SubstochasticMonteCarlo<Model_T>::SubstochasticMonteCarlo() SubstochasticMonteCarlo() Declaration solver::SubstochasticMonteCarlo<Model_T>::SubstochasticMonteCarlo(const SubstochasticMonteCarlo&)=delete Methods operator=() Declaration SubstochasticMonteCarlo&solver::SubstochasticMonteCarlo<Model_T>::operator=(const SubstochasticMonteCarlo&)=delete get_identifier() Identifier of this solver (target in the request) Declaration std::string solver::SubstochasticMonteCarlo<Model_T>::get_identifier() const override init_memory_check_error_message() Declaration std::string solver::SubstochasticMonteCarlo<Model_T>::init_memory_check_error_message() const override target_number_of_states() Declaration size_t solver::SubstochasticMonteCarlo<Model_T>::target_number_of_states() const override init() Initialize the solver. Declaration void solver::SubstochasticMonteCarlo<Model_T>::init() override make_step() Declaration void solver::SubstochasticMonteCarlo<Model_T>::make_step(uint64_t step) override make_walker_steps() Declaration void solver::SubstochasticMonteCarlo<Model_T>::make_walker_steps(double alpha) update_population_statistics() Declaration void solver::SubstochasticMonteCarlo<Model_T>::update_population_statistics() resample_population() Declaration void solver::SubstochasticMonteCarlo<Model_T>::resample_population(double beta) get_solutions() Get the structured result description. Declaration utils::Structure solver::SubstochasticMonteCarlo<Model_T>::get_solutions() const override configure() Read the maximum number of steps from configuration. Declaration void solver::SubstochasticMonteCarlo<Model_T>::configure(const utils::Json&json) override finalize() Declaration void solver::SubstochasticMonteCarlo<Model_T>::finalize() override prepare_population() prepare the population to size target_population Note This method is called to prepare population from current size to target_population Declaration void solver::SubstochasticMonteCarlo<Model_T>::prepare_population()"
  },
  "api/solver/tabu-parameter-free.html": {
    "href": "api/solver/tabu-parameter-free.html",
    "title": "Class solver::TabuParameterFree | qiotoolkit",
    "keywords": "Class solver::TabuParameterFree Defines a parameter-free tabu search solver. The parameter free solver uses the TabuSolver class and tunes the two parameters: tabu tenure and iterations (sweeps). Inheritance solver::Tabu solver::ParameterFreeLinearSearchAdapterInterface solver::TabuParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state get_initial_parameter_values update_parameters_linearly set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step Tabu make_step finalize target_number_of_states get_solutions operator= get_identifier init_memory_check_error_message Tabu init set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors TabuParameterFree() Declaration solver::TabuParameterFree<Model_T>::TabuParameterFree() TabuParameterFree() Declaration solver::TabuParameterFree<Model_T>::TabuParameterFree(const TabuParameterFree&)=delete Methods operator=() Declaration TabuParameterFree&solver::TabuParameterFree<Model_T>::operator=(const TabuParameterFree&)=delete configure() Read the maximum number of steps from configuration. Declaration void solver::TabuParameterFree<Model_T>::configure(const utils::Json&json) override parameter_dimensions() Declaration size_t solver::TabuParameterFree<Model_T>::parameter_dimensions() const override parameter_ranges() Declaration std::vector<int>solver::TabuParameterFree<Model_T>::parameter_ranges() const override update_parameters() Declaration void solver::TabuParameterFree<Model_T>::update_parameters(const std::vector<double>&parameters, double left_over_time) override estimate_execution_cost() Declaration double solver::TabuParameterFree<Model_T>::estimate_execution_cost() const override get_optimal_tenures_number() Declaration size_t solver::TabuParameterFree<Model_T>::get_optimal_tenures_number() const determine_all_tenures_() Declaration std::vector<unsigned int>solver::TabuParameterFree<Model_T>::determine_all_tenures_(size_t n_tenures)"
  },
  "api/solver/tabu.html": {
    "href": "api/solver/tabu.html",
    "title": "Class solver::Tabu | qiotoolkit",
    "keywords": "Class solver::Tabu Defines a tabu search solver. We are implementing the simple tabu search found in the following papers: J.E Beasley's 1998 paper \"Heuristic algorithms for the unconstrained binary quadratic programming problem\" GINTARAS PALUBECKIS 2004 paper \"Multistart Tabu Search Strategies for the Unconstrained Binary Quadratic Optimization Problem\" with the following adjustments: local delta calculations (as opposed to full re-calculation of the cost function) different starting configurations running in parallel Inheritance solver::SteppingSolver solver::Tabu solver::TabuParameterFree Inherited Members clear_observable_label restart is_watching scoped_observable_label observe Observer render set_observable_label ~ModelSolver copy_lowest_state copy_solutions get_model_properties update_lowest_cost get_model_term_size init_memory_check count_solutions copy_solutions get_model get_model_unconst copy_solutions_other ModelSolver populate_solutions max_replicas_adjusted_state set_model get_model_sweep_size is_empty get_lowest_cost adjust_states max_replicas_of_state set_time_limit get_result copy_limits ~Solver get_benchmark get_max_threads get_thread_count Solver fixed_step_per_tick update_accumulated_info handle_signals run current_steps reset get_cost_function_evaluation_count SteppingSolver get_solver_properties status get_seed ~SteppingSolver get_runtime seconds_per_step set_output_parameter set_output_parameter param get_output_parameters ~Component Component get_status get_class_name Constructors Tabu() Declaration solver::Tabu<Model_T>::Tabu() Tabu() Declaration solver::Tabu<Model_T>::Tabu(const Tabu&)=delete Methods operator=() Declaration Tabu&solver::Tabu<Model_T>::operator=(const Tabu&)=delete get_identifier() Identifier of this solver (target in the request) Declaration std::string solver::Tabu<Model_T>::get_identifier() const override init_memory_check_error_message() Declaration std::string solver::Tabu<Model_T>::init_memory_check_error_message() const override target_number_of_states() Declaration size_t solver::Tabu<Model_T>::target_number_of_states() const override init() Initialize the solver. Declaration void solver::Tabu<Model_T>::init() override make_step() Declaration void solver::Tabu<Model_T>::make_step(uint64_t) override get_solutions() Get the structured result description. Declaration utils::Structure solver::Tabu<Model_T>::get_solutions() const override configure() Read the maximum number of steps from configuration. Declaration void solver::Tabu<Model_T>::configure(const utils::Json&json) override finalize() Declaration void solver::Tabu<Model_T>::finalize() override"
  },
  "api/std/hash/3/01examples/soft-spin-transition/01/4.html": {
    "href": "api/std/hash/3/01examples/soft-spin-transition/01/4.html",
    "title": "Struct > | qiotoolkit",
    "keywords": "Struct > Methods operator()() Declaration std::size_t std::hash<examples::SoftSpinTransition>::operator()(const examples::SoftSpinTransition&trans) const noexcept"
  },
  "api/std/hash/3/01model/blume-capel-transition/01/4.html": {
    "href": "api/std/hash/3/01model/blume-capel-transition/01/4.html",
    "title": "Struct > | qiotoolkit",
    "keywords": "Struct > Methods operator()() Declaration std::size_t std::hash<model::BlumeCapelTransition>::operator()(const model::BlumeCapelTransition&trans) const noexcept"
  },
  "api/std/hash/3/01model/clock-transition/01/4.html": {
    "href": "api/std/hash/3/01model/clock-transition/01/4.html",
    "title": "Struct > | qiotoolkit",
    "keywords": "Struct > Methods operator()() Declaration std::size_t std::hash<model::ClockTransition>::operator()(const model::ClockTransition&trans) const noexcept"
  },
  "api/std/hash/3/01model/tsp-transition/01/4.html": {
    "href": "api/std/hash/3/01model/tsp-transition/01/4.html",
    "title": "Struct > | qiotoolkit",
    "keywords": "Struct > Methods operator()() Declaration std::size_t std::hash<model::TspTransition>::operator()(const model::TspTransition&trans) const noexcept"
  },
  "api/strategy/base-opt.html": {
    "href": "api/strategy/base-opt.html",
    "title": "Class strategy::BaseOpt | qiotoolkit",
    "keywords": "Class strategy::BaseOpt Global Optimization stategy searching implementation. Inheritance utils::Component strategy::BaseOpt strategy::BayesianOpt strategy::LinearSearchOpt Inherited Members configure render ~Component Component get_status param get_class_name Constructors BaseOpt() Declaration strategy::BaseOpt::BaseOpt() BaseOpt() Declaration strategy::BaseOpt::BaseOpt(const BaseOpt&)=delete Methods operator=() Declaration BaseOpt&strategy::BaseOpt::operator=(const BaseOpt&)=delete recommend_parameter_values() Declaration virtual bool strategy::BaseOpt::recommend_parameter_values(std::vector<double>&parameters_new)=0 add_new_sample() Declaration virtual void strategy::BaseOpt::add_new_sample(std::vector<double>&parameters, double objective)=0 get_perf_metrics() Declaration void strategy::BaseOpt::get_perf_metrics(utils::Structure&perf) const log_parameters() Declaration void strategy::BaseOpt::log_parameters(const std::string&header, const std::vector<double>&parameters)"
  },
  "api/strategy/bayesian-opt.html": {
    "href": "api/strategy/bayesian-opt.html",
    "title": "Class strategy::BayesianOpt | qiotoolkit",
    "keywords": "Class strategy::BayesianOpt Bayesian Global Optimization stategy searching implementation. The BayesianOpt is a wrapper class for GPP library https://github.com/wujian16/Cornell-MOE The paper \"A Tutorial on Bayesian Optimization\" is a good introduction of bayesian optimization. Inheritance strategy::BaseOpt strategy::BayesianOpt Inherited Members BaseOpt operator= BaseOpt get_perf_metrics log_parameters configure render ~Component Component get_status param get_class_name Constructors BayesianOpt() Declaration strategy::BayesianOpt::BayesianOpt() BayesianOpt() Declaration strategy::BayesianOpt::BayesianOpt(const BayesianOpt&)=delete Methods operator=() Declaration BayesianOpt&strategy::BayesianOpt::operator=(const BayesianOpt&)=delete configure() Declaration void strategy::BayesianOpt::configure(const utils::Json&params, int thread_count) init() Declaration void strategy::BayesianOpt::init(size_t dimensions, uint32_t seed, size_t reserved_samples=128) init() Declaration void strategy::BayesianOpt::init(size_t dimensions, uint32_t seed, const GDParameters&model_parameters, const GDParameters&search_parameters, size_t reserved_samples=128) recommend_parameter_values() Declaration bool strategy::BayesianOpt::recommend_parameter_values(std::vector<double>&parameters_new) override add_new_sample() Declaration void strategy::BayesianOpt::add_new_sample(std::vector<double>&parameters, double objective) override set_ranges() Declaration void strategy::BayesianOpt::set_ranges(const std::vector<std::pair<double, double>>&ranges) num_of_saved_samples() Declaration size_t strategy::BayesianOpt::num_of_saved_samples() const get_sample() Declaration double strategy::BayesianOpt::get_sample(int indx, std::vector<double>&sample_point) const get_reserved_samples() Declaration uint32_t strategy::BayesianOpt::get_reserved_samples() copyout_winner() Declaration void strategy::BayesianOpt::copyout_winner(std::vector<double>&parameters) const log_hyper_parameters() Declaration void strategy::BayesianOpt::log_hyper_parameters() const"
  },
  "api/strategy/gdparameters.html": {
    "href": "api/strategy/gdparameters.html",
    "title": "Class strategy::GDParameters | qiotoolkit",
    "keywords": "Class strategy::GDParameters Inheritance utils::Component strategy::GDParameters Inherited Members render ~Component Component get_status param get_class_name Methods load() Declaration void strategy::GDParameters::load(const utils::Structure&params) configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void strategy::GDParameters::configure(const utils::Json&params) override to_string() Declaration std::string strategy::GDParameters::to_string() const"
  },
  "api/strategy/linear-search-opt.html": {
    "href": "api/strategy/linear-search-opt.html",
    "title": "Class strategy::LinearSearchOpt | qiotoolkit",
    "keywords": "Class strategy::LinearSearchOpt Linear Search Global Optimization stategy searching implementation. Implemented 1 dimensional search and refine algorithm. Solution is computed at sample points iteratively and search interval defined by sample points will be moved toward the end where the best solution is observed. Inheritance strategy::BaseOpt strategy::LinearSearchOpt Inherited Members BaseOpt operator= BaseOpt get_perf_metrics log_parameters configure render ~Component Component get_status param get_class_name Constructors LinearSearchOpt() Declaration strategy::LinearSearchOpt::LinearSearchOpt() LinearSearchOpt() Declaration strategy::LinearSearchOpt::LinearSearchOpt(const LinearSearchOpt&)=delete Methods operator=() Declaration LinearSearchOpt&strategy::LinearSearchOpt::operator=(const LinearSearchOpt&)=delete configure() Declaration void strategy::LinearSearchOpt::configure(const utils::Json&params, int thread_count) init() Declaration void strategy::LinearSearchOpt::init(size_t dimensions, uint32_t seed, size_t reserved_samples=128) recommend_parameter_values() Declaration bool strategy::LinearSearchOpt::recommend_parameter_values(std::vector<double>&parameters_new) add_new_sample() Declaration void strategy::LinearSearchOpt::add_new_sample(std::vector<double>&parameters, double objective) set_ranges() Declaration void strategy::LinearSearchOpt::set_ranges(const std::vector<int>&intial_sample_points) fit_sample_points() Search interval defined by sample points will be stretched if the best solution is produced at the last sample point. Search interval defined by sample points will be shrinked if the best solution is produced at the first sample point. Declaration void strategy::LinearSearchOpt::fit_sample_points()"
  },
  "api/utils/any-key-object-member-stream-handler.html": {
    "href": "api/utils/any-key-object-member-stream-handler.html",
    "title": "Class utils::AnyKeyObjectMemberStreamHandler | qiotoolkit",
    "keywords": "Class utils::AnyKeyObjectMemberStreamHandler AnyKeyMemberStreamHandler. Stream handler for handling extra fields, which are not to be used for configure. These fields could have any key value. Inheritance utils::BaseStreamHandler utils::AnyKeyObjectMemberStreamHandler Inherited Members complete started failed reset error_message error_code BaseStreamHandler Constructors AnyKeyObjectMemberStreamHandler() Declaration utils::AnyKeyObjectMemberStreamHandler<OuterClass>::AnyKeyObjectMemberStreamHandler() Methods Key() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::Key(const std::string&key) Value() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::Value(SignalType signal_value, OuterClass&) StartObject() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::StartObject() EndObject() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::EndObject(size_t elementCount, OuterClass&) StartArray() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::StartArray() EndArray() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::EndArray(size_t elementCount, OuterClass&) complete() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::complete() const reset() Declaration void utils::AnyKeyObjectMemberStreamHandler<OuterClass>::reset(OuterClass&) fail_at_missing_inputs() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::fail_at_missing_inputs() missing_inputs() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::missing_inputs() started() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::started() const fail() Declaration bool utils::AnyKeyObjectMemberStreamHandler<OuterClass>::fail(const std::string&meassage=\"parsing error\", Error error_code=Error::ParsingError)"
  },
  "api/utils/base-stream-handler.html": {
    "href": "api/utils/base-stream-handler.html",
    "title": "Class utils::BaseStreamHandler | qiotoolkit",
    "keywords": "Class utils::BaseStreamHandler Classes for implementation of handlers, which consume the events (via function calls) from the steam readers. These handles configure objects directly from events information. The reader publishes events to the handler sequentially. Example: for JSON string { \"c\" = 2.0 } the following events will be called: StartObject(), Key(\"c\"), Double(2), EndObject(1) from which object of type class { double c; } will be configured. BaseStreamHandler This is a base class for stream handlers. It has default failure handling implementation for unexpected events due to incorrect format of data. Boolean result of operations indicates that parsing can continue. Inheritance utils::BaseStreamHandler utils::BasicTypeStreamHandler utils::ObserverStreamHandler Constructors BaseStreamHandler() Declaration utils::BaseStreamHandler<T>::BaseStreamHandler() Methods Value() Declaration bool utils::BaseStreamHandler<T>::Value(SignalType, T&) StartObject() Declaration bool utils::BaseStreamHandler<T>::StartObject() Key() Declaration bool utils::BaseStreamHandler<T>::Key(const std::string&) EndObject() Declaration bool utils::BaseStreamHandler<T>::EndObject(size_t, T&) StartArray() Declaration bool utils::BaseStreamHandler<T>::StartArray() EndArray() Declaration bool utils::BaseStreamHandler<T>::EndArray(size_t, T&) reset() Declaration void utils::BaseStreamHandler<T>::reset() reset() Declaration void utils::BaseStreamHandler<T>::reset(T&) started() Declaration bool utils::BaseStreamHandler<T>::started() const complete() Declaration bool utils::BaseStreamHandler<T>::complete() const missing_inputs() Declaration bool utils::BaseStreamHandler<T>::missing_inputs() const fail_at_missing_inputs() Declaration bool utils::BaseStreamHandler<T>::fail_at_missing_inputs() const failed() Declaration bool utils::BaseStreamHandler<T>::failed() const error_code() Declaration Error utils::BaseStreamHandler<T>::error_code() const error_message() Declaration std::string utils::BaseStreamHandler<T>::error_message() const fail() Declaration bool utils::BaseStreamHandler<T>::fail(const std::string&meassage=\"parsing error\", Error error_code=Error::ParsingError)"
  },
  "api/utils/basic-type-stream-handler.html": {
    "href": "api/utils/basic-type-stream-handler.html",
    "title": "Class utils::BasicTypeStreamHandler | qiotoolkit",
    "keywords": "Class utils::BasicTypeStreamHandler BasicTypeStreamHandler<..> Particular implementations of stream handlers for values of simple, non class, types like int, double, string... Inheritance utils::BaseStreamHandler utils::BasicTypeStreamHandler Inherited Members StartObject StartArray complete started fail_at_missing_inputs Key missing_inputs EndArray failed reset error_message EndObject Value reset error_code BaseStreamHandler fail"
  },
  "api/utils/basic-type-stream-handler/3/01bool/01/4.html": {
    "href": "api/utils/basic-type-stream-handler/3/01bool/01/4.html",
    "title": "Class > | qiotoolkit",
    "keywords": "Class > Inheritance utils::BaseStreamHandler > Inherited Members StartObject StartArray complete started fail_at_missing_inputs Key missing_inputs EndArray failed reset error_message EndObject reset error_code BaseStreamHandler fail Methods Value() Declaration bool utils::BasicTypeStreamHandler<bool>::Value(SignalType signal_val, bool&) Value() Declaration bool utils::BasicTypeStreamHandler<bool>::Value(bool b, bool&val)"
  },
  "api/utils/basic-type-stream-handler/3/01double/01/4.html": {
    "href": "api/utils/basic-type-stream-handler/3/01double/01/4.html",
    "title": "Class > | qiotoolkit",
    "keywords": "Class > Inheritance utils::BaseStreamHandler > Inherited Members StartObject StartArray complete started fail_at_missing_inputs Key missing_inputs EndArray failed reset error_message EndObject reset error_code BaseStreamHandler fail Methods Value() Declaration bool utils::BasicTypeStreamHandler<double>::Value(SignalType signal_val, double&) Value() Declaration bool utils::BasicTypeStreamHandler<double>::Value(int64_t i, double&val) Value() Declaration bool utils::BasicTypeStreamHandler<double>::Value(double d, double&val)"
  },
  "api/utils/basic-type-stream-handler/3/01int/01/4.html": {
    "href": "api/utils/basic-type-stream-handler/3/01int/01/4.html",
    "title": "Class > | qiotoolkit",
    "keywords": "Class > Inheritance utils::BaseStreamHandler > Inherited Members StartObject StartArray complete started fail_at_missing_inputs Key missing_inputs EndArray failed reset error_message EndObject reset error_code BaseStreamHandler fail Methods Value() Declaration bool utils::BasicTypeStreamHandler<int>::Value(SignalType signal_val, int&) Value() Declaration bool utils::BasicTypeStreamHandler<int>::Value(int64_t i, int&val)"
  },
  "api/utils/basic-type-stream-handler/3/01size/t/01/4.html": {
    "href": "api/utils/basic-type-stream-handler/3/01size/t/01/4.html",
    "title": "Class > | qiotoolkit",
    "keywords": "Class > Inheritance utils::BaseStreamHandler > Inherited Members StartObject StartArray complete started fail_at_missing_inputs Key missing_inputs EndArray failed reset error_message EndObject reset error_code BaseStreamHandler fail Methods Value() Declaration bool utils::BasicTypeStreamHandler<size_t>::Value(SignalType signal_val, size_t&) Value() Declaration bool utils::BasicTypeStreamHandler<size_t>::Value(int64_t i, size_t&val)"
  },
  "api/utils/basic-type-stream-handler/3/01std/string/01/4.html": {
    "href": "api/utils/basic-type-stream-handler/3/01std/string/01/4.html",
    "title": "Class > | qiotoolkit",
    "keywords": "Class > Inheritance utils::BaseStreamHandler > Inherited Members StartObject StartArray complete started fail_at_missing_inputs Key missing_inputs EndArray failed reset error_message EndObject reset error_code BaseStreamHandler fail Methods Value() Declaration bool utils::BasicTypeStreamHandler<std::string>::Value(SignalType signal_val, std::string&) Value() Declaration bool utils::BasicTypeStreamHandler<std::string>::Value(std::string str, std::string&val)"
  },
  "api/utils/bit-stream-visitor.html": {
    "href": "api/utils/bit-stream-visitor.html",
    "title": "Class utils::BitStreamVisitor | qiotoolkit",
    "keywords": "Class utils::BitStreamVisitor Class to keep the position information about data to be read/write in BitStream. Inheritance utils::BitStreamVisitor Constructors BitStreamVisitor() Declaration utils::BitStreamVisitor::BitStreamVisitor() Methods reset() Declaration void utils::BitStreamVisitor::reset(uint64_t value)"
  },
  "api/utils/bit-stream.html": {
    "href": "api/utils/bit-stream.html",
    "title": "Class utils::BitStream | qiotoolkit",
    "keywords": "Class utils::BitStream BitStream is used for saving data in a compact format, data are saved in fix sized bits continuouly. To read and write data to and from BitStream, a BitStreamVisitor is required. Currently one sequentially visiting is supported, unless you know how to manually modify position information inside BitStreamVisitor. Inheritance utils::BitStream Constructors BitStream() Declaration utils::BitStream::BitStream() Methods init() Declaration void utils::BitStream::init(size_t num_units, uint32_t unit_bits) read_next() Declaration uint64_t utils::BitStream::read_next(BitStreamVisitor&visitor) const write_next() Declaration void utils::BitStream::write_next(uint64_t value, BitStreamVisitor&visitor) init_reader() Declaration void utils::BitStream::init_reader(BitStreamVisitor&visitor) const update() Declaration void utils::BitStream::update(BitStreamVisitor&position)"
  },
  "api/utils/component-with-output.html": {
    "href": "api/utils/component-with-output.html",
    "title": "Class utils::ComponentWithOutput | qiotoolkit",
    "keywords": "Class utils::ComponentWithOutput Inheritance utils::Component utils::ComponentWithOutput observe::Observer Inherited Members configure render ~Component Component get_status get_class_name Methods param() Assign a parameter from a json field. Declaration ParameterBuilder<utils::ComponentWithOutput, T>utils::ComponentWithOutput::param(const utils::Json&json, const std::string&name, T&parameter) set_output_parameter() Store a key, value pair in the output_parameters (to be returned to the user). Declaration void utils::ComponentWithOutput::set_output_parameter(const std::string&key, const T&value) set_output_parameter() Store the value of an optional in the output_parameters if it is set. Declaration void utils::ComponentWithOutput::set_output_parameter(const std::string&key, const std::optional<T>&value) get_output_parameters() Get a structure containing all output_parameters. Declaration virtual utils::Structure utils::ComponentWithOutput::get_output_parameters() const"
  },
  "api/utils/component.html": {
    "href": "api/utils/component.html",
    "title": "Class utils::Component | qiotoolkit",
    "keywords": "Class utils::Component Component base class. The component base class provides quality-of-life functionality for any class derived from it: It can be initialized from json via its configure() method It is rendered in \"structured\" form via render() when assigned to an utils::Structure. Its class name is accessible via get_class_name() When passed to an ostream (or LOG), it is represented as '<ClassName: status>' where the status is given by get_status() While get_class_name() works out-of-the-box, the appropriate functionality must be implemented by overloading the others accordingly. Inheritance utils::Component graph::CompactGraph graph::Edge graph::Face graph::Graph graph::Node markov::ClusterWalker markov::State markov::Transition markov::Walker model::BaseModel model::Terms model::Terms::Term model::Terms::Variable observe::Observable observe::Windowed::ValueAndWeight schedule::Schedule schedule::ScheduleGenerator solver::EvaluationCounter solver::Population strategy::BaseOpt strategy::GDParameters utils::ComponentWithOutput utils::Dimacs::Clause utils::Range Constructors Component() Declaration utils::Component::Component() Methods ~Component() Declaration virtual utils::Component::~Component() configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void utils::Component::configure(const utils::Json&json) param() Assign a parameter from a json field. Declaration ParameterBuilder<utils::Component, T>utils::Component::param(const utils::Json&json, const std::string&name, T&parameter) render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration utils::Structure utils::Component::render() const get_status() get_status shows a simplified state representation Like render, this produces a structured representation of the object's state. However, it is intended to be simpler in nature with the purpose of rendering the object during stream-output and logging. By default, it will fall back to the full render, but overloading this allows distinguising how the object looks during LOG vs full output. Declaration utils::Structure utils::Component::get_status() const get_class_name() get_class_name shows an identifier of the (derived) class name Its primary use is for type identification during stream output and logging, where a component is rendered as <ClassName: status>. Example: {c++} MyClass : public Component {} // This will render as '<MyClass>' MyClass my_object; std::cout << my_object << std::endl; Note You do not need to overload this method unless you want to change the output of the default implementation. Declaration std::string utils::Component::get_class_name() const"
  },
  "api/utils/configuration-error.html": {
    "href": "api/utils/configuration-error.html",
    "title": "Class utils::ConfigurationError | qiotoolkit",
    "keywords": "Class utils::ConfigurationError Inheritance utils::ConfigurationException utils::ConfigurationError Inherited Members get_error_code get_error_code_message get_error_message ConfigurationException Constructors ConfigurationError() Declaration utils::ConfigurationError<err>::ConfigurationError(std::string message)"
  },
  "api/utils/configuration-exception.html": {
    "href": "api/utils/configuration-exception.html",
    "title": "Class utils::ConfigurationException | qiotoolkit",
    "keywords": "Class utils::ConfigurationException ConfigurationException. A configuration exception is thrown when the input configuration is not valid (either syntactically or according to the requirements of the components which need to be configured). Inheritance utils::ConfigurationException utils::ConfigurationError Constructors ConfigurationException() Declaration utils::ConfigurationException::ConfigurationException(std::string message, Error error_code) Methods get_error_message() Declaration std::string utils::ConfigurationException::get_error_message() const get_error_code_message() Declaration std::string utils::ConfigurationException::get_error_code_message() const get_error_code() Declaration Error utils::ConfigurationException::get_error_code() const"
  },
  "api/utils/dimacs.html": {
    "href": "api/utils/dimacs.html",
    "title": "Class utils::Dimacs | qiotoolkit",
    "keywords": "Class utils::Dimacs Handler for the DIMACS format. Inheritance utils::Dimacs Constructors Dimacs() Constructor: Create an empty Dimacs handler. Declaration utils::Dimacs::Dimacs() Methods read() Parse a string in dimacs format. Declaration void utils::Dimacs::read(const std::string&content) read_file() Read the contents from filename and parse them. Declaration void utils::Dimacs::read_file(const std::string&filename) get_type() Get the type of the parsed problem ('cnf' or 'wcnf') (or empty string if nothing was parsed). Declaration std::string utils::Dimacs::get_type() const get_nvar() Get the number of variables in the problem. Declaration size_t utils::Dimacs::get_nvar() const get_ncl() Get the number of clauses in the problem. Declaration size_t utils::Dimacs::get_ncl() const get_top() Get the value specified as top (minimum weight for a clause to be 'hard') in the input (return 0 if not specified). [!NOTE]: Only type type='wcnfallowstop as an optional 5th element in the problem description line (starting with a 'p). Declaration double utils::Dimacs::get_top() const get_variables() Get the list of variables actually encountered in the description. These are all in their positive (non-negated) form, but they don't need to form a contiguous set of numbers. Declaration const std::vector<int>&utils::Dimacs::get_variables() const get_clauses() Get the list of clauses parsed. Declaration const std::vector<Dimacs::Clause>&utils::Dimacs::get_clauses() const clear() Return the Dimacs parser to its initial state. Declaration void utils::Dimacs::clear() parse() Parse the string pointed to by content_ Declaration void utils::Dimacs::parse() ignore_line() Ignore a full line from the current position p_ Declaration void utils::Dimacs::ignore_line() parse_problem() Parse a problem description line. Declaration void utils::Dimacs::parse_problem() parse_clause() Parse a clause (terminated by 0) Declaration void utils::Dimacs::parse_clause() expect() Check that the next characters are equivalent to s and advance the position p_ past them. Declaration void utils::Dimacs::expect(const std::string&s) to_next_ws() Count the number of characters to the next whitespace. Declaration size_t utils::Dimacs::to_next_ws() read_string() Read the set of characters to the next whitespace and store them in target. Declaration void utils::Dimacs::read_string(std::string *target) read_number() Parse the characters up to the next whitespace as an integer. Declaration void utils::Dimacs::read_number(int *target) read_number() Parse the characters up to the next whitespace as a double. Declaration void utils::Dimacs::read_number(double *target) read_positive_number() Parse the characters up to the next whitespace as an integer and ensure that the number is greater than 0. Declaration void utils::Dimacs::read_positive_number(uint32_t *target) eat_whitespace() Ignore all whitespace from the current position. Declaration void utils::Dimacs::eat_whitespace() on_line() Return a string describing the current line being parsed. Declaration std::string utils::Dimacs::on_line() const at_position() Return a string describing the current line and column (adding offset to the current column). [!NOTE] This does not fix the line if col+offset is on a different line; it is meant solely to get correct positioning after a number has been read successfully but does not meet conditions. Declaration std::string utils::Dimacs::at_position(int offset=0) const"
  },
  "api/utils/dimacs/clause.html": {
    "href": "api/utils/dimacs/clause.html",
    "title": "Class utils::Dimacs::Clause | qiotoolkit",
    "keywords": "Class utils::Dimacs::Clause Representation of a satisfiability clause For unweighted satisfiability problems, the weight will always be 1. Inheritance utils::Component utils::Dimacs::Clause Inherited Members render ~Component Component get_status param get_class_name Methods configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void utils::Dimacs::Clause::configure(const utils::Json&json) check_variable_names() Declaration void utils::Dimacs::Clause::check_variable_names()"
  },
  "api/utils/dimacs/clause/get/variables.html": {
    "href": "api/utils/dimacs/clause/get/variables.html",
    "title": "Struct utils::Dimacs::Clause::Get_Variables | qiotoolkit",
    "keywords": "Struct utils::Dimacs::Clause::Get_Variables Methods get() Declaration static std::vector<int>&utils::Dimacs::Clause::Get_Variables::get(Clause&config) get_key() Declaration static std::string utils::Dimacs::Clause::Get_Variables::get_key()"
  },
  "api/utils/dimacs/clause/get/weight.html": {
    "href": "api/utils/dimacs/clause/get/weight.html",
    "title": "Struct utils::Dimacs::Clause::Get_Weight | qiotoolkit",
    "keywords": "Struct utils::Dimacs::Clause::Get_Weight Methods get() Declaration static double&utils::Dimacs::Clause::Get_Weight::get(Clause&config) get_key() Declaration static std::string utils::Dimacs::Clause::Get_Weight::get_key()"
  },
  "api/utils/enum-stream-handler.html": {
    "href": "api/utils/enum-stream-handler.html",
    "title": "Class utils::EnumStreamHandler | qiotoolkit",
    "keywords": "Class utils::EnumStreamHandler EnumStreamHandler. Implementations of stream handlers for enum expressed as fixed string values Inheritance utils::BaseStreamHandler utils::EnumStreamHandler Inherited Members StartObject StartArray complete started fail_at_missing_inputs Key missing_inputs EndArray failed reset error_message EndObject reset error_code BaseStreamHandler fail Methods Value() Declaration bool utils::EnumStreamHandler<EnumType, StringEnumMap>::Value(SignalType signal_val, EnumType&) Value() Declaration bool utils::EnumStreamHandler<EnumType, StringEnumMap>::Value(std::string str, EnumType&val)"
  },
  "api/utils/feature-configs.html": {
    "href": "api/utils/feature-configs.html",
    "title": "Class utils::FeatureConfigs | qiotoolkit",
    "keywords": "Class utils::FeatureConfigs Inheritance utils::FeatureConfigs Constructors FeatureConfigs() Declaration utils::FeatureConfigs::FeatureConfigs() Methods set_disabled_feature() Declaration void utils::FeatureConfigs::set_disabled_feature(int feature_id) set_enabled_feature() Declaration void utils::FeatureConfigs::set_enabled_feature(int feature_id) feature_disabled() Declaration bool utils::FeatureConfigs::feature_disabled(int feature_id) feature_enabled() Declaration bool utils::FeatureConfigs::feature_enabled(int feature_id)"
  },
  "api/utils/file.html": {
    "href": "api/utils/file.html",
    "title": "Class utils::File | qiotoolkit",
    "keywords": "Class utils::File File Wrapper for using FILE* exception safe way. Inheritance utils::File Constructors File() Declaration utils::File::File() File() Declaration utils::File::File(const std::string&file_name, const std::string&mode) Methods open() Declaration void utils::File::open(const std::string&file_name, const std::string&mode) get() Declaration FILE* utils::File::get() ~File() Declaration utils::File::~File()"
  },
  "api/utils/final-object-member-stream-handler.html": {
    "href": "api/utils/final-object-member-stream-handler.html",
    "title": "Class utils::FinalObjectMemberStreamHandler | qiotoolkit",
    "keywords": "Class utils::FinalObjectMemberStreamHandler Stream handler for object is implemented as nested classes of key value and utility stream handlers: ObjectStreamHandler< ObjectMemberStreamHandler<.., ObjectMemberStreamHandler<.., FinalObjectMemberStreamHandler Each ObjectMemberStreamHandler represents key value entry of object. FinalObjectMemberStreamHandler is the last one in the chain. FinalObjectMemberStreamHandler FinalObjectMemberStreamHandler is the last one in the members stream chain. It is used to simplify ObjectMemberStreamHandler logic. Inheritance utils::BaseStreamHandler utils::FinalObjectMemberStreamHandler utils::ObjectMemberStreamHandler Inherited Members StartObject StartArray complete started Key EndArray failed reset error_message EndObject Value reset error_code BaseStreamHandler fail Methods complete() Declaration bool utils::FinalObjectMemberStreamHandler<OuterClass>::complete() const started() Declaration bool utils::FinalObjectMemberStreamHandler<OuterClass>::started() const fail_at_missing_inputs() Declaration bool utils::FinalObjectMemberStreamHandler<OuterClass>::fail_at_missing_inputs() missing_inputs() Declaration bool utils::FinalObjectMemberStreamHandler<OuterClass>::missing_inputs()"
  },
  "api/utils/index.html": {
    "href": "api/utils/index.html",
    "title": "| qiotoolkit",
    "keywords": ""
  },
  "api/utils/jsonhandler-proxy.html": {
    "href": "api/utils/jsonhandler-proxy.html",
    "title": "Class utils::JSONHandlerProxy | qiotoolkit",
    "keywords": "Class utils::JSONHandlerProxy JSONHandlerProxy. Convert and redirect JSON reader signals into qiotoolkit handler. Inheritance utils::JSONHandlerProxy Constructors JSONHandlerProxy() Declaration utils::JSONHandlerProxy<StreamHandler>::JSONHandlerProxy() Methods Null() Declaration bool utils::JSONHandlerProxy<StreamHandler>::Null() Bool() Declaration bool utils::JSONHandlerProxy<StreamHandler>::Bool(bool b) Int() Declaration bool utils::JSONHandlerProxy<StreamHandler>::Int(int i) Uint() Declaration bool utils::JSONHandlerProxy<StreamHandler>::Uint(unsigned i) Int64() Declaration bool utils::JSONHandlerProxy<StreamHandler>::Int64(int64_t i) Uint64() Declaration bool utils::JSONHandlerProxy<StreamHandler>::Uint64(uint64_t i) Double() Declaration bool utils::JSONHandlerProxy<StreamHandler>::Double(double d) RawNumber() Declaration bool utils::JSONHandlerProxy<StreamHandler>::RawNumber(const char *, int, bool) String() Declaration bool utils::JSONHandlerProxy<StreamHandler>::String(const char *str, int len, bool) StartObject() Declaration bool utils::JSONHandlerProxy<StreamHandler>::StartObject() Key() Declaration bool utils::JSONHandlerProxy<StreamHandler>::Key(const char *str, int len, bool) EndObject() Declaration bool utils::JSONHandlerProxy<StreamHandler>::EndObject(int elementCount) StartArray() Declaration bool utils::JSONHandlerProxy<StreamHandler>::StartArray() EndArray() Declaration bool utils::JSONHandlerProxy<StreamHandler>::EndArray(int elementCount) get_value() Declaration StreamHandler::Type_T&utils::JSONHandlerProxy<StreamHandler>::get_value() complete() Declaration bool utils::JSONHandlerProxy<StreamHandler>::complete() error_code() Declaration Error utils::JSONHandlerProxy<StreamHandler>::error_code() const error_message() Declaration std::string utils::JSONHandlerProxy<StreamHandler>::error_message() const fail() Declaration bool utils::JSONHandlerProxy<StreamHandler>::fail() const"
  },
  "api/utils/logger.html": {
    "href": "api/utils/logger.html",
    "title": "Class utils::Logger | qiotoolkit",
    "keywords": "Class utils::Logger Logging facilities. Apart from adjusting logging attributes (level, color, output stream), users should use the LOG(LEVEL, ...) macro to produce log output. The macro takes care of including the source context of where information was logged, while passing the content to Logger::log. Inheritance utils::Logger Methods log() Log information to the Logger Declaration static void utils::Logger::log(LogLevel level, Context context, const Args&... args) debug() Declaration static void utils::Logger::debug(Context context, Args... args) set_level() Set the global log level (minimum level for logs to appear) Declaration static void utils::Logger::set_level(LogLevel level) is_default_level() Whether the level is still the default value for that build (used to decide whether to override from input). Declaration static bool utils::Logger::is_default_level() set_level() Set the global log level from string (minimum level for logs to appear) Declaration bool utils::Logger::set_level(std::string str_level) get_level() Get the currently set log level. Declaration static LogLevel utils::Logger::get_level() to_string() Turn a log level into its string representation. Declaration std::string utils::Logger::to_string(LogLevel level) set_color() Set whether logging should use ansi colors. Declaration static void utils::Logger::set_color(bool color) set_stream() Set where logging should be streamed to (default=std::cerr) Declaration static void utils::Logger::set_stream(std::ostream *stream) get_stream() Declaration static std::ostream&utils::Logger::get_stream() use_color() Declaration static bool utils::Logger::use_color() color() Declaration std::string utils::Logger::color(LogLevel level) context_color() Declaration std::string utils::Logger::context_color() normal_color() Declaration std::string utils::Logger::normal_color() get_prefix() Return the prefix that should be at the beginning of each log. Declaration std::string utils::Logger::get_prefix(LogLevel level, const Context&context) get_debug_prefix() Return the prefix that should be at the beginning of each debug output. Declaration std::string utils::Logger::get_debug_prefix(const Context&context) should_log() Decide whether a given call should pass to the output. Declaration bool utils::Logger::should_log(LogLevel level, const Context&_)"
  },
  "api/utils/logger/context.html": {
    "href": "api/utils/logger/context.html",
    "title": "Class utils::Logger::Context | qiotoolkit",
    "keywords": "Class utils::Logger::Context Store the (code) context of where a LOG was invoked (file, line, function). Inheritance utils::Logger::Context Constructors Context() Declaration utils::Logger::Context::Context(const std::string&file, int line, const std::string&function) Methods get_line() Return the source line where the LOG was made. Declaration int utils::Logger::Context::get_line() const get_file() Return the source file path where the LOG was made. Declaration const std::string&utils::Logger::Context::get_file() const get_function() Return the function or method within which the LOG was made. Declaration const std::string&utils::Logger::Context::get_function() const demangle() Translate gcc mangled function name to human-readable form. Declaration std::string utils::Logger::Context::demangle(const char *name)"
  },
  "api/utils/memory-usage-log.html": {
    "href": "api/utils/memory-usage-log.html",
    "title": "Class utils::MemoryUsageLog | qiotoolkit",
    "keywords": "Class utils::MemoryUsageLog Inheritance utils::MemoryUsageLog Methods clear() Declaration void utils::MemoryUsageLog::clear() add() Declaration void utils::MemoryUsageLog::add(const std::string&checkpoint) get() Declaration const std::vector<std::pair<std::string, size_t>>&utils::MemoryUsageLog::get()"
  },
  "api/utils/object-member-stream-handler.html": {
    "href": "api/utils/object-member-stream-handler.html",
    "title": "Class utils::ObjectMemberStreamHandler | qiotoolkit",
    "keywords": "Class utils::ObjectMemberStreamHandler ObjectMemberStreamHandler. Stream handler for key value entry. GetMember::get_key() is the key of object entry. GetMember::get(val) return reference to the member it is streaming to. Inheritance utils::FinalObjectMemberStreamHandler utils::ObjectMemberStreamHandler Inherited Members complete started failed reset error_message error_code BaseStreamHandler started Methods Key() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::Key(const std::string&key) Value() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::Value(SignalType signal_value, OuterClass&val) StartObject() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::StartObject() EndObject() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::EndObject(size_t elementCount, OuterClass&val) StartArray() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::StartArray() EndArray() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::EndArray(size_t elementCount, OuterClass&val) complete() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::complete() const reset() Declaration void utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::reset(OuterClass&val) fail_at_missing_inputs() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::fail_at_missing_inputs() missing_inputs() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::missing_inputs() fail() Declaration bool utils::ObjectMemberStreamHandler<StreamHandler, OuterClass, GetMember, required_, NextObjectMemberStreamHandler>::fail(const std::string&meassage=\"parsing error\", Error error_code=Error::ParsingError)"
  },
  "api/utils/object-stream-handler.html": {
    "href": "api/utils/object-stream-handler.html",
    "title": "Class utils::ObjectStreamHandler | qiotoolkit",
    "keywords": "Class utils::ObjectStreamHandler ObjectStreamHandler. Stream handler for object. .stop_early_ - stop streaming when all required parameters are loaded, to skip streaming of large members. Inheritance utils::ObjectStreamHandler Methods Key() Declaration bool utils::ObjectStreamHandler<MemberStreamHandler, stop_early_, Out_Type>::Key(const std::string&key) StartObject() Declaration bool utils::ObjectStreamHandler<MemberStreamHandler, stop_early_, Out_Type>::StartObject() EndObject() Declaration bool utils::ObjectStreamHandler<MemberStreamHandler, stop_early_, Out_Type>::EndObject(size_t elementCount, typename MemberStreamHandler::Out_Type_T&val) complete() Declaration bool utils::ObjectStreamHandler<MemberStreamHandler, stop_early_, Out_Type>::complete() const started() Declaration bool utils::ObjectStreamHandler<MemberStreamHandler, stop_early_, Out_Type>::started() const reset() Declaration void utils::ObjectStreamHandler<MemberStreamHandler, stop_early_, Out_Type>::reset(Type_T&val)"
  },
  "api/utils/observer-stream-handler.html": {
    "href": "api/utils/observer-stream-handler.html",
    "title": "Class utils::ObserverStreamHandler | qiotoolkit",
    "keywords": "Class utils::ObserverStreamHandler ObserverStreamHandler. This is a stream handler for skipping to load corresponding member, used for configure passes when only part of information is needed. Inheritance utils::BaseStreamHandler utils::ObserverStreamHandler Inherited Members complete fail_at_missing_inputs missing_inputs failed reset error_message error_code BaseStreamHandler fail Constructors ObserverStreamHandler() Declaration utils::ObserverStreamHandler<T>::ObserverStreamHandler() Methods Key() Declaration bool utils::ObserverStreamHandler<T>::Key(const std::string&) Value() Declaration bool utils::ObserverStreamHandler<T>::Value(SignalType, T&) StartObject() Declaration bool utils::ObserverStreamHandler<T>::StartObject() EndObject() Declaration bool utils::ObserverStreamHandler<T>::EndObject(size_t, T&) StartArray() Declaration bool utils::ObserverStreamHandler<T>::StartArray() EndArray() Declaration bool utils::ObserverStreamHandler<T>::EndArray(size_t, T&) reset() Declaration void utils::ObserverStreamHandler<T>::reset(T&val) started() Declaration bool utils::ObserverStreamHandler<T>::started() const state_check() Declaration bool utils::ObserverStreamHandler<T>::state_check()"
  },
  "api/utils/omp-catch.html": {
    "href": "api/utils/omp-catch.html",
    "title": "Class utils::OmpCatch | qiotoolkit",
    "keywords": "Class utils::OmpCatch Catch exceptions in parallel sections. Omp parallel sections must not leave the parallel section via exceptions This allows such exceptions to be caught and later re-thrown after the parallel section: utils::OmpCatch omp_catch; #pragma omp parallel for (int i = 0; i < N; i++) { omp_catch([=]{ // .. code which might throw }); } omp_catch.rethrow(); Inheritance utils::OmpCatch Constructors OmpCatch() Declaration utils::OmpCatch::OmpCatch() Methods ~OmpCatch() Declaration utils::OmpCatch::~OmpCatch() run() Declaration void utils::OmpCatch::run(Function f, Parameters... params) rethrow() Declaration void utils::OmpCatch::rethrow()"
  },
  "api/utils/owner-class-stream-handler.html": {
    "href": "api/utils/owner-class-stream-handler.html",
    "title": "Class utils::OwnerClassStreamHandler | qiotoolkit",
    "keywords": "Class utils::OwnerClassStreamHandler Inheritance utils::OwnerClassStreamHandler Methods Value() Declaration bool utils::OwnerClassStreamHandler<StreamHandler>::Value(SignalType signal_value) EndObject() Declaration bool utils::OwnerClassStreamHandler<StreamHandler>::EndObject(size_t elementCount) EndArray() Declaration bool utils::OwnerClassStreamHandler<StreamHandler>::EndArray(size_t sz) get_value() Declaration StreamHandler::Type_T&utils::OwnerClassStreamHandler<StreamHandler>::get_value() reset() Declaration void utils::OwnerClassStreamHandler<StreamHandler>::reset()"
  },
  "api/utils/pair-vector-stream-handler.html": {
    "href": "api/utils/pair-vector-stream-handler.html",
    "title": "Class utils::PairVectorStreamHandler | qiotoolkit",
    "keywords": "Class utils::PairVectorStreamHandler PairVectorStreamHandler. Implementations of stream handlers for set of values of simple, non class type like bool, int... Inheritance utils::BaseStreamHandler utils::PairVectorStreamHandler Inherited Members StartArray complete started fail_at_missing_inputs missing_inputs EndArray failed reset error_message error_code BaseStreamHandler Methods StartObject() Declaration bool utils::PairVectorStreamHandler<Key_T, T>::StartObject() Key() Declaration bool utils::PairVectorStreamHandler<Key_T, T>::Key(const std::string&key) Value() Declaration bool utils::PairVectorStreamHandler<Key_T, T>::Value(SignalType signal_value, Type_T&vec) EndObject() Declaration bool utils::PairVectorStreamHandler<Key_T, T>::EndObject(size_t elementCount, Type_T&vec) started() Declaration bool utils::PairVectorStreamHandler<Key_T, T>::started() const complete() Declaration bool utils::PairVectorStreamHandler<Key_T, T>::complete() const reset() Declaration void utils::PairVectorStreamHandler<Key_T, T>::reset(Type_T&vec) fail() Declaration bool utils::PairVectorStreamHandler<Key_T, T>::fail(const std::string&meassage=\"parsing error\", Error error_code=Error::ParsingError)"
  },
  "api/utils/parameter-builder.html": {
    "href": "api/utils/parameter-builder.html",
    "title": "Class utils::ParameterBuilder | qiotoolkit",
    "keywords": "Class utils::ParameterBuilder ParameterBuilder. This is the builder object returned by Component::param(). Its initial setup contains the input (json) to read from, the name of the field to read and a reference to the parameter to be filled. Additional details on how to read the parameter can be chained onto the Parameter builder: this->param(json, \"number\", my_number) .alias(\"my_number\") .description(\"some description\") .matches(GreaterEquals(0)) .default_value(1) Note A parameter is considered optional unless required() is specified explicitly as part of the chain. An optional parameter with no default value will leave the parameter reference untouched (except if the parameter is an std::optional , in which case it will be reset). Output Parametes Some parameters are intended to be returned as part of the response (read back). Components with this property should be derived from utils::ComponentWithOutput, in which case the ParameterBuilder has the additional method [.with_output()](xref:classutils_1_1ParameterBuilder_1ae7291d09e04864c1126fff90516d470f) causing the parameter value to be stored in the component's output parameters: this->param(json, \"number\", my_number) .with_output() this->get_output_parameters(); // will contain {\"number\": my_number} Inheritance utils::ParameterBuilder Constructors ParameterBuilder() Assign the parameter from the specified field. Declaration utils::ParameterBuilder<Component_T, Param_T>::ParameterBuilder(Component_T *component, const utils::Json&json, const std::string&name, Param_T&parameter) Methods alias() If not yet assigned try assigning from the alias field. Declaration ParameterBuilder&utils::ParameterBuilder<Component_T, Param_T>::alias(const std::string&name) description() Store the description to be used in error messages. Declaration ParameterBuilder&utils::ParameterBuilder<Component_T, Param_T>::description(const std::string&desc) required() Throw an exception if the parameter has not been assigned (i.e., the field was not present). Declaration ParameterBuilder&utils::ParameterBuilder<Component_T, Param_T>::required() default_value() Assign a default value if the parameter has not ben assigned. Declaration ParameterBuilder&utils::ParameterBuilder<Component_T, Param_T>::default_value(const V&default_value_) with_output() Store the parameters value in the component's output_parameters (to be returned to the user). Declaration std::enable_if<std::is_same<C, utils::ComponentWithOutput>::value, ParameterBuilder&>::type utils::ParameterBuilder<Component_T, Param_T>::with_output() matches() If the parameter was assigned, match its value against the matcher. Declaration ParameterBuilder&utils::ParameterBuilder<Component_T, Param_T>::matches(const M&matcher) try_assign() Declaration bool utils::ParameterBuilder<Component_T, Param_T>::try_assign() reset_if_optional() Declaration static void utils::ParameterBuilder<Component_T, Param_T>::reset_if_optional(T&) reset_if_optional() Declaration static void utils::ParameterBuilder<Component_T, Param_T>::reset_if_optional(std::optional<T>&parameter)"
  },
  "api/utils/pcg.html": {
    "href": "api/utils/pcg.html",
    "title": "Class utils::PCG | qiotoolkit",
    "keywords": "Class utils::PCG Inheritance utils::RandomGenerator utils::PCG Inherited Members min ~RandomGenerator max seed RandomGenerator operator() Constructors PCG() Declaration utils::PCG::PCG() PCG() Declaration utils::PCG::PCG(uint32_t seed) Methods seed() Set the seed to the random_number_generator. Declaration void utils::PCG::seed(uint32_t) override fork() Declaration std::unique_ptr<RandomGenerator>utils::PCG::fork() override uint32() Return a random 32bit integer. Declaration uint32_t utils::PCG::uint32() override uniform() Return a uniformly distributed double in [0,1). Declaration double utils::PCG::uniform() override gaussian() Return a gaussian distributed double. Declaration double utils::PCG::gaussian() override poisson() Return a poisson distributed integer with mean mean. Declaration uint32_t utils::PCG::poisson(double mean) override memory_estimate() Return estimation of memory in bytes. Declaration size_t utils::PCG::memory_estimate() const override"
  },
  "api/utils/proto-reader.html": {
    "href": "api/utils/proto-reader.html",
    "title": "Class utils::ProtoReader | qiotoolkit",
    "keywords": "Class utils::ProtoReader Inheritance utils::ProtoReader Constructors ProtoReader() Declaration utils::ProtoReader::ProtoReader() Methods Parse() Declaration bool utils::ProtoReader::Parse(const std::string&folder_path, utils::PROTOHandlerProxy<StreamHandler>&handler_proxy) Parse() Declaration bool utils::ProtoReader::Parse(const google::protobuf::RepeatedPtrField<QuantumUtil::Problem_Term>&terms, utils::PROTOHandlerProxy<StreamHandler>&handler_proxy, unsigned&num_terms) Parse() Declaration bool utils::ProtoReader::Parse(const QuantumUtil::Problem_Term&term, utils::PROTOHandlerProxy<StreamHandler>&handler_proxy)"
  },
  "api/utils/protohandler-proxy.html": {
    "href": "api/utils/protohandler-proxy.html",
    "title": "Class utils::PROTOHandlerProxy | qiotoolkit",
    "keywords": "Class utils::PROTOHandlerProxy PROTOHandlerProxy. Convert and redirect PROTO reader signals into qiotoolkit handler. Inheritance utils::PROTOHandlerProxy Constructors PROTOHandlerProxy() Declaration utils::PROTOHandlerProxy<StreamHandler>::PROTOHandlerProxy() Methods Null() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::Null() Bool() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::Bool(bool b) Int() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::Int(int i) Uint() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::Uint(unsigned i) Int64() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::Int64(int64_t i) Uint64() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::Uint64(uint64_t i) Double() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::Double(double d) RawNumber() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::RawNumber(const char *, int, bool) String() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::String(std::string str) StartObject() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::StartObject() Key() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::Key(std::string str) EndObject() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::EndObject(int elementCount) StartArray() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::StartArray() EndArray() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::EndArray(int elementCount) get_value() Declaration StreamHandler::Type_T&utils::PROTOHandlerProxy<StreamHandler>::get_value() reset() Declaration void utils::PROTOHandlerProxy<StreamHandler>::reset() complete() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::complete() error_code() Declaration Error utils::PROTOHandlerProxy<StreamHandler>::error_code() const error_message() Declaration std::string utils::PROTOHandlerProxy<StreamHandler>::error_message() const fail() Declaration bool utils::PROTOHandlerProxy<StreamHandler>::fail() const"
  },
  "api/utils/random-generator.html": {
    "href": "api/utils/random-generator.html",
    "title": "Class utils::RandomGenerator | qiotoolkit",
    "keywords": "Class utils::RandomGenerator RandomGenerator. This is a thin wrapper around the STL mersenne twister which provides the convenience function for the uniform = rand \\in [0,1) and poisson(double mean) Inheritance utils::RandomGenerator utils::PCG utils::Twister Constructors RandomGenerator() Declaration utils::RandomGenerator::RandomGenerator() Methods max() Declaration static result_type utils::RandomGenerator::max() min() Declaration static result_type utils::RandomGenerator::min() ~RandomGenerator() Declaration virtual utils::RandomGenerator::~RandomGenerator() operator()() Declaration uint32_t utils::RandomGenerator::operator()() seed() Allow seeding from signed integer. Declaration void utils::RandomGenerator::seed(int32_t seed) fork() Declaration virtual std::unique_ptr<RandomGenerator>utils::RandomGenerator::fork()=0 seed() Set the seed to the random_number_generator. Declaration virtual void utils::RandomGenerator::seed(uint32_t seed)=0 uint32() Return a random 32bit integer. Declaration virtual uint32_t utils::RandomGenerator::uint32()=0 uniform() Return a uniformly distributed double in [0,1). Declaration virtual double utils::RandomGenerator::uniform()=0 gaussian() Return a gaussian distributed double. Declaration virtual double utils::RandomGenerator::gaussian()=0 poisson() Return a poisson distributed integer with mean mean. Declaration virtual uint32_t utils::RandomGenerator::poisson(double mean)=0 memory_estimate() Return estimation of memory in bytes. Declaration virtual size_t utils::RandomGenerator::memory_estimate() const =0"
  },
  "api/utils/random-selector.html": {
    "href": "api/utils/random-selector.html",
    "title": "Class utils::RandomSelector | qiotoolkit",
    "keywords": "Class utils::RandomSelector Inheritance utils::RandomSelector Constructors RandomSelector() Declaration utils::RandomSelector::RandomSelector() Methods reset() Declaration void utils::RandomSelector::reset() insert() Declaration void utils::RandomSelector::insert(size_t id, double weight) select() Declaration size_t utils::RandomSelector::select(double uniform) const select_and_remove() Declaration size_t utils::RandomSelector::select_and_remove(double uniform) get_weight() Declaration double utils::RandomSelector::get_weight(size_t position) const get_cumulative_weight() Declaration double utils::RandomSelector::get_cumulative_weight(size_t position) const select_position() Declaration size_t utils::RandomSelector::select_position(double uniform) const remove_position() Declaration void utils::RandomSelector::remove_position(size_t position) add_weight() Declaration void utils::RandomSelector::add_weight(size_t position, double weight)"
  },
  "api/utils/range.html": {
    "href": "api/utils/range.html",
    "title": "Class utils::Range | qiotoolkit",
    "keywords": "Class utils::Range Inheritance utils::Component utils::Range Inherited Members ~Component Component get_status param Constructors Range() Declaration utils::Range::Range() Range() Declaration utils::Range::Range(double in_initial, double in_final) Methods ~Range() Declaration utils::Range::~Range() configure() configure the object from input Initialize the object's state from the input utils::Config. This is done by declaring which required and optional parameters are associated with the fields of this object. During initialization, they are checked for their presence, type and any matchers. Example: MyClass : public Component { public: void configure(const utils::Json& json) override { this->param(json, \"number\", my_number) .description(\"some description\") .matches(GreaterEquals(0)) .required(); this->param(json, \"name\", my_name) .description(\"some description\") .matches(SizeIs(GreaterThan(0))) .default_value(\"no_name\"); } private: int my_number; std::string my_name; } MyClass my_object; my_object.configure(utils::json_from_string(R\"( { \"number\": 42, \"name\": \"hello\" } )\")); Note By default, nothing is configured from input. You need to overload this method if you want to use this functionality. Don't forget to call the configure method of the parent class if it also needs to be configured (this is not the case for utils::Component itself). HINT: Parameters are not limited to scalars and strings; Any component can be a parameter; in which case it is initialized using its own configure method. utils::Json Declaration void utils::Range::configure(const utils::Json&json) override render() render the object in structured form Return a structured representation of the object. This is intended for output purposes. For instance, the solution your solver finds should have a render method which allows it to be returned as part of the result. Example: {c++} MySolution : public Component { public: // Represent the internal bool vector as +-1 output. utils::Structure render() const override { utils::Structure rendered; for (bool item : solution_) rendered.push_back(item ? 1 : -1); return rendered; } private: std::vector<bool> solution_; } MySolution solution; std::cout << solution.render().to_string() << std::endl; utils::Structure Declaration Structure utils::Range::render() const override get_class_name() get_class_name shows an identifier of the (derived) class name Its primary use is for type identification during stream output and logging, where a component is rendered as <ClassName: status>. Example: {c++} MyClass : public Component {} // This will render as '<MyClass>' MyClass my_object; std::cout << my_object << std::endl; Note You do not need to overload this method unless you want to change the output of the default implementation. Declaration std::string utils::Range::get_class_name() const override"
  },
  "api/utils/runtime-error.html": {
    "href": "api/utils/runtime-error.html",
    "title": "Class utils::RuntimeError | qiotoolkit",
    "keywords": "Class utils::RuntimeError Inheritance utils::RuntimeException utils::RuntimeError Inherited Members RuntimeException get_error_code_message get_error_code Constructors RuntimeError() Declaration utils::RuntimeError<err>::RuntimeError(std::string message)"
  },
  "api/utils/runtime-exception.html": {
    "href": "api/utils/runtime-exception.html",
    "title": "Class utils::RuntimeException | qiotoolkit",
    "keywords": "Class utils::RuntimeException RuntimeException. A runtime exception is thrown when an unexpected state is encountered during the simulation, which cannot be handled gracefully (i.e., a bug). Inheritance utils::RuntimeException utils::RuntimeError Constructors RuntimeException() Declaration utils::RuntimeException::RuntimeException(std::string message, Error error_code) Methods get_error_code_message() Declaration std::string utils::RuntimeException::get_error_code_message() const get_error_code() Declaration Error utils::RuntimeException::get_error_code() const"
  },
  "api/utils/scoped-timer.html": {
    "href": "api/utils/scoped-timer.html",
    "title": "Class utils::ScopedTimer | qiotoolkit",
    "keywords": "Class utils::ScopedTimer Drop in timer (logs time used to INFO when the object leaves scope) Inheritance utils::ScopedTimer Constructors ScopedTimer() Declaration utils::ScopedTimer::ScopedTimer(std::string title) Methods ~ScopedTimer() Declaration utils::ScopedTimer::~ScopedTimer()"
  },
  "api/utils/set-stream-handler.html": {
    "href": "api/utils/set-stream-handler.html",
    "title": "Class utils::SetStreamHandler | qiotoolkit",
    "keywords": "Class utils::SetStreamHandler SetStreamHandler. Implementations of stream handlers for set of values of simple, non class type like bool, int... Inheritance utils::BaseStreamHandler utils::SetStreamHandler Inherited Members StartObject complete started fail_at_missing_inputs Key missing_inputs failed reset error_message EndObject error_code BaseStreamHandler Methods Value() Declaration bool utils::SetStreamHandler<T>::Value(SignalType signal_value, Type_T&vec) StartArray() Declaration bool utils::SetStreamHandler<T>::StartArray() EndArray() Declaration bool utils::SetStreamHandler<T>::EndArray(size_t elementCount, Type_T&vec) started() Declaration bool utils::SetStreamHandler<T>::started() const complete() Declaration bool utils::SetStreamHandler<T>::complete() const reset() Declaration void utils::SetStreamHandler<T>::reset(Type_T&vec) fail() Declaration bool utils::SetStreamHandler<T>::fail(const std::string&meassage=\"parsing error\", Error error_code=Error::ParsingError)"
  },
  "api/utils/signal.html": {
    "href": "api/utils/signal.html",
    "title": "Class utils::Signal | qiotoolkit",
    "keywords": "Class utils::Signal Inheritance utils::Signal Constructors Signal() Declaration utils::Signal::Signal(Type type) Methods get_type() Declaration Signal::Type utils::Signal::get_type() get_time() Declaration double utils::Signal::get_time() queue() Declaration std::queue<Signal>&utils::Signal::queue()"
  },
  "api/utils/structure.html": {
    "href": "api/utils/structure.html",
    "title": "Class utils::Structure | qiotoolkit",
    "keywords": "Class utils::Structure Inheritance utils::Structure Constructors Structure() Declaration utils::Structure::Structure() Structure() Declaration utils::Structure::Structure(Type type) Structure() Declaration utils::Structure::Structure(const Structure&other) Structure() Declaration utils::Structure::Structure(const T&value) Methods ~Structure() Declaration utils::Structure::~Structure() remove() Declaration void utils::Structure::remove(const std::string&key) operator=() Declaration Structure&utils::Structure::operator=(const Structure&other) check_reset() Declaration void utils::Structure::check_reset(Structure::Type type) reset() Declaration void utils::Structure::reset(Structure::Type type) operator=() Declaration Structure&utils::Structure::operator=(const T&value) operator[]() Declaration Structure&utils::Structure::operator[](size_t index) operator[]() Declaration Structure&utils::Structure::operator[](int32_t index) operator[]() Declaration const Structure&utils::Structure::operator[](size_t index) const check_array() Declaration void utils::Structure::check_array() const operator[]() Declaration const Structure&utils::Structure::operator[](int32_t index) const operator[]() Declaration Structure&utils::Structure::operator[](const std::string&key) check_object() Declaration void utils::Structure::check_object() const operator[]() Declaration const Structure&utils::Structure::operator[](const std::string&key) const get_extension() Declaration Structure&utils::Structure::get_extension(const std::string&name) get_type() Declaration Type utils::Structure::get_type() const is_empty() Declaration bool utils::Structure::is_empty() const has_key() Declaration bool utils::Structure::has_key(const std::string&key) const has_extension() Declaration bool utils::Structure::has_extension(const std::string&name) const get_array_size() Declaration size_t utils::Structure::get_array_size() const get_object_keys() Declaration std::vector<std::string>utils::Structure::get_object_keys() const get_extension_names() Declaration std::vector<std::string>utils::Structure::get_extension_names() const get() Declaration T utils::Structure::get() const clear() Declaration void utils::Structure::clear() push_back() Declaration void utils::Structure::push_back(const T&value) append() Declaration void utils::Structure::append(const std::vector<T>&values) prepend() Declaration void utils::Structure::prepend(const std::vector<T>&values) set_extension() Declaration void utils::Structure::set_extension(const std::string&name, const T&value) to_string() Declaration std::string utils::Structure::to_string(bool pretty=true) const <() Declaration bool utils::Structure::operator<(const Structure&rhs) const <() Declaration bool utils::Structure::operator<(const Structure&rhs) const get() Declaration bool utils::Structure::get() const get() Declaration int32_t utils::Structure::get() const get() Declaration uint32_t utils::Structure::get() const get() Declaration int64_t utils::Structure::get() const get() Declaration uint64_t utils::Structure::get() const get() Declaration double utils::Structure::get() const get() Declaration bool utils::Structure::get() const get() Declaration int32_t utils::Structure::get() const get() Declaration uint32_t utils::Structure::get() const get() Declaration int64_t utils::Structure::get() const get() Declaration uint64_t utils::Structure::get() const get() Declaration double utils::Structure::get() const get() Declaration std::string utils::Structure::get() const find_extension() Declaration const Structure* utils::Structure::find_extension(const std::string&name) const set_value() Declaration void utils::Structure::set_value(bool value) set_value() Declaration void utils::Structure::set_value(int32_t value) set_value() Declaration void utils::Structure::set_value(uint32_t value) set_value() Declaration void utils::Structure::set_value(int64_t value) set_value() Declaration void utils::Structure::set_value(uint64_t value) set_value() Declaration void utils::Structure::set_value(float value) set_value() Declaration void utils::Structure::set_value(double value) set_value() Declaration void utils::Structure::set_value(const char value[]) set_value() Declaration void utils::Structure::set_value(const std::string&value) set_value() Declaration void utils::Structure::set_value(const Structure&value) set_value() Declaration void utils::Structure::set_value(const std::vector<T>&array) set_value() Declaration void utils::Structure::set_value(const std::map<std::string, T>&object) set_value() Declaration void utils::Structure::set_value(const std::unordered_map<std::string, T>&object) set_value() Declaration void utils::Structure::set_value(const T&component) is_simple() Declaration bool utils::Structure::is_simple() const to_string() Declaration std::string utils::Structure::to_string(bool pretty, int level) const type2string() Declaration std::string utils::Structure::type2string(Type type) deep_copy() Declaration void utils::Structure::deep_copy(const Structure&other)"
  },
  "api/utils/twister.html": {
    "href": "api/utils/twister.html",
    "title": "Class utils::Twister | qiotoolkit",
    "keywords": "Class utils::Twister Inheritance utils::RandomGenerator utils::Twister Inherited Members min ~RandomGenerator max seed RandomGenerator operator() Constructors Twister() Declaration utils::Twister::Twister() Twister() Declaration utils::Twister::Twister(uint32_t seed) Methods seed() Set the seed to the random_number_generator. Declaration void utils::Twister::seed(uint32_t) override fork() Declaration std::unique_ptr<RandomGenerator>utils::Twister::fork() override uint32() Return a random 32bit integer. Declaration uint32_t utils::Twister::uint32() override uniform() Return a uniformly distributed double in [0,1). Declaration double utils::Twister::uniform() override gaussian() Return a gaussian distributed double. Declaration double utils::Twister::gaussian() override poisson() Return a poisson distributed integer with mean mean. Declaration uint32_t utils::Twister::poisson(double mean) override memory_estimate() Return estimation of memory in bytes. Declaration size_t utils::Twister::memory_estimate() const override"
  },
  "api/utils/vector-object-stream-handler.html": {
    "href": "api/utils/vector-object-stream-handler.html",
    "title": "Class utils::VectorObjectStreamHandler | qiotoolkit",
    "keywords": "Class utils::VectorObjectStreamHandler Inheritance utils::BaseStreamHandler utils::VectorObjectStreamHandler Inherited Members complete started fail_at_missing_inputs missing_inputs EndArray failed reset error_message EndObject Value reset error_code BaseStreamHandler Methods StartArray() Declaration bool utils::VectorObjectStreamHandler<StreamHandler>::StartArray() Value() Declaration bool utils::VectorObjectStreamHandler<StreamHandler>::Value(SignalType signal_value, Type_T&) StartObject() Declaration bool utils::VectorObjectStreamHandler<StreamHandler>::StartObject() Key() Declaration bool utils::VectorObjectStreamHandler<StreamHandler>::Key(const std::string&key) EndObject() Declaration bool utils::VectorObjectStreamHandler<StreamHandler>::EndObject(size_t elementCount, Type_T&vec) EndArray() Declaration bool utils::VectorObjectStreamHandler<StreamHandler>::EndArray(size_t elementCount, Type_T&vec) complete() Declaration virtual bool utils::VectorObjectStreamHandler<StreamHandler>::complete() const started() Declaration virtual bool utils::VectorObjectStreamHandler<StreamHandler>::started() const reset() Declaration void utils::VectorObjectStreamHandler<StreamHandler>::reset(Type_T&vec) push_back_value() Declaration void utils::VectorObjectStreamHandler<StreamHandler>::push_back_value(Type_T&vec) fail() Declaration bool utils::VectorObjectStreamHandler<StreamHandler>::fail(const std::string&meassage=\"parsing error\", Error error_code=Error::ParsingError)"
  },
  "api/utils/vector-stream-handler.html": {
    "href": "api/utils/vector-stream-handler.html",
    "title": "Class utils::VectorStreamHandler | qiotoolkit",
    "keywords": "Class utils::VectorStreamHandler VectorStreamHandler. Implementations of stream handlers for vector of values of simple, non class type like bool, int... Inheritance utils::BaseStreamHandler utils::VectorStreamHandler Inherited Members StartObject complete started fail_at_missing_inputs Key missing_inputs failed reset error_message EndObject error_code BaseStreamHandler Methods Value() Declaration bool utils::VectorStreamHandler<T>::Value(SignalType signal_value, Type_T&vec) StartArray() Declaration bool utils::VectorStreamHandler<T>::StartArray() EndArray() Declaration bool utils::VectorStreamHandler<T>::EndArray(size_t elementCount, Type_T&vec) complete() Declaration bool utils::VectorStreamHandler<T>::complete() const started() Declaration bool utils::VectorStreamHandler<T>::started() const reset() Declaration void utils::VectorStreamHandler<T>::reset(Type_T&vec) fail() Declaration bool utils::VectorStreamHandler<T>::fail(const std::string&meassage=\"parsing error\", Error error_code=Error::ParsingError)"
  },
  "CODE_OF_CONDUCT.html": {
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of Conduct | qiotoolkit",
    "keywords": "Code of Conduct Purpose The primary goal of this project is to foster an open and welcoming environment where everyone who wishes to contribute can do so, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion (or lack thereof). We encourage everyone with the necessary skills and a genuine interest in this project to contribute and be a part of our community. This Code of Conduct outlines our expectations for all community members and the principles we adhere to, ensuring a safe and positive experience for everyone. Our Standards We are committed to maintaining a community that acknowledges the value of every individual. We expect all members of our community to: Demonstrate empathy, kindness, and respect towards each other. Respect personal boundaries and personal identities. Give credit where credit is due and acknowledge the contributions of others. Welcome constructive criticism and be open to learning. Show commitment to the wellbeing and success of the community. Examples of unacceptable behavior include: Offensive or derogatory comments, personal or political attacks, or any form of public or private harassment. Publishing others' private information without their explicit permission. Other conduct that could be reasonably considered inappropriate in a professional setting. Responsibilities Project maintainers are responsible for clarifying and enforcing our standards of acceptable behavior and are expected to take appropriate action in response to any instances of unacceptable behavior. Maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the project maintainers responsible for enforcement at dennis.jensen@kpmg.com. All complaints will be reviewed, investigated and responded to. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html Conclusion By choosing to contribute to this project, you commit to upholding this Code of Conduct. Let us all strive to create a productive, welcoming, and inclusive environment where everyone feels valued and can freely contribute their skills and knowledge."
  },
  "CONTRIBUTING.html": {
    "href": "CONTRIBUTING.html",
    "title": "Quantum Inspired Optimization Toolkit Project - Contribution Guidelines | qiotoolkit",
    "keywords": "Quantum Inspired Optimization Toolkit Project - Contribution Guidelines Introduction Thank you for your interest in contributing to the Quantum Inspired Optimization Toolkit project! We're thrilled to welcome you on board. This project is deeply rooted in complex mathematical concepts, implemented in C++, and we greatly value your expertise and dedication. Getting Started Familiarize Yourself with the Project Before making your first contribution, please familiarize yourself with the project. Understand what the project is about, what its objectives are, and how it works. This project requires a strong understanding of quantum-inspired algorithms, mathematical optimization, and C++ programming. Understand the Code of Conduct We maintain a standard Code of Conduct that all contributors must adhere to. Make sure you read and understand this before contributing. A respectful and inclusive environment is critical to the project's success. How to Contribute Fork the Repository To start contributing, fork the repository and then clone it to your local machine. This will give you a copy of the entire codebase on your system, allowing you to make changes. Set Up the Development Environment Ensure your local setup is working perfectly and you can run the project without any issues. If you encounter any problems, reach out to the community for help. Pick an Issue Check out the open issues in the repository. Select one that you are interested in and fits your skills. We encourage newcomers to start with smaller, simpler issues and gradually move on to more complex ones as they get more comfortable. Write Your Code After selecting an issue, start writing your code. Please try and ensure you follow our coding standards and conventions. They help maintain the code's consistency and readability. Write code in modern, idiomatic C++ and adhere to the C++ Core Guidelines Include detailed comments with your code when the logic isn't immediately apparent. Strive for clean, efficient, and optimized code. Avoid unnecessary complexity or over-optimization. Testing Write tests for your code to verify that it behaves as expected. Your tests should cover edge cases and potential errors. This is necessary to actually contribute to the project as we do not currently allow ned code in with under 90% test coverage. Documentation Given the mathematical complexity of this project, robust documentation is critical. Include thorough explanations of your code and the mathematical concepts behind it when necessary. Remember that others in the community may not have the same level of understanding or expertise as you. Submitting Your Contribution Commit Your Changes Before committing your changes, ensure that your code builds correctly and all tests pass. Your commit message should be descriptive and clarify the purpose of the commit. Push to Your Fork Push your commits to your fork. Open a Pull Request From your forked repository, open a pull request against the main project. Include a descriptive title and detailed description of your changes. Link any related issues. Quality gate check We have several automated quality checks. Currently we do not allow any code change without 90% code coverage. We also have a static code analysis tool that checks for any code quality issues. The pull request will automatically run these checks and it won't be possible to contribute until these checks pass. Code Review Maintainers or project members will review your code. They may suggest changes or improvements. This is a normal part of the process, and these reviews ensure the quality and consistency of the code base. Incorporate Feedback Incorporate any feedback or changes requested during the code review. Once all feedback has been addressed, the maintainers will merge your pull request. Conclusion We hope these guidelines help you navigate your contribution journey. Don't hesitate to ask for help when you need it. And most importantly, have fun! Thank you for contributing to the Quantum Inspired Optimization Toolkit project!"
  },
  "design/exceptions.html": {
    "href": "design/exceptions.html",
    "title": "Design: Exceptions | qiotoolkit",
    "keywords": "Design: Exceptions Background: Concise and actionable feedback in exceptions is crucial to allow customers to amend their input. Goal: Provide guidelines on the content and wording of exceptions returned to customers. Non-goals: Design and discussion of logging levels for development. Exceptions vs Logging Exceptions are intended as an early-abort mechanism with feedback to customers (unless caught and handled) Logging is purely for development (we don't surface any log statements to customers). How to throw We provide a THROW macro which takes care of concatenating multiple stream-printable contents into an error message: if (!my_condition(variable)) { THROW(utils::ValueException, \"my_condition must hold for variable. Found variable=\", variable, \".\"); } which is equivalent to throw(utils::ValueException(msg.str())) with a manually crafted msg. Which Exception We recognize [four classes of exceptions]: Insufficient Resources (1-100) Invalid Input Data (101-200) Runtime Errors (201-300) File IO errors (301-400) The list of error codes can be expanded if necessary. Choice of Modal Verb To clarify whether a condition is enforced or merely suggested, we follow the Microsoft Style Guide: the word \"must\" indicates a required condition (the task will abort early if not followed and indicate this condition as the reason), the word \"should\" indicates a suggestion (the task will still run, but we expect results to be inferior). We currently don't have a mechanism to surface warnings to the user, so the second instance will silently accept inputs not following the suggestion. Positive Statement Exceptions should be formulated in the positive (i.e., describe the expected situation), followed by what was encountered (i.e., why the expectation is not met). Ideally with details on where to find the inconsistency. Example a must be greater-equals 0. Found a=-1 in the json input: cost_function.params.a. alpha must be decreasing (i.e., the initial value of alpha must be larger than the final value). Found alpha.initial = 0 < 1 = alpha.final. Note This is in contrast to describing what is wrong (i.e., \"Error: alpha.initial is smaller than alpha.final.\")."
  },
  "index.html": {
    "href": "index.html",
    "title": "qiotoolkit | qiotoolkit",
    "keywords": "Tools for rapid development, analysis and deployment of quantum inspired optimization algorithms. Content This documentation contains: API: Documentation of the C++ interfaces of qiotoolkit components, Specification: Listing of input parameters for models and solvers, Theory: Explanations of commonly used terminology, and a few Tutorials to get up and running."
  },
  "spec/index.html": {
    "href": "spec/index.html",
    "title": "I/O Specifications | qiotoolkit",
    "keywords": "Specifications The interfaces generated for qiotoolkit-based solvers use JSON for both input and output. This documentation section specifies their format as JSON-schemas."
  },
  "spec/model/blume-capel.html": {
    "href": "spec/model/blume-capel.html",
    "title": "Blume-Capel model | qiotoolkit",
    "keywords": "Blume-Capel Model The variables in the BlumeCapel model can take 3 values (spin-\\(`1`\\) as opposed to spin-\\(`\\frac12`\\) in the Ising case). \\mathcal{H} = -\\sum_{ij} J_{ij} s_{i}s_{j}\\,,\\quad s_i \\in \\{-1,0,+1\\} Hypergraph Generalization The generalization is analogous to the Ising spin glass: \\mathcal{H} = -\\sum_e J_e \\prod_{i\\in e} s_i NOTE: Contrary to the Ising model, squared spin variables can be meaningful in the BlumeCapel model: \\(`s_i\\in\\{-1,0,+1\\}`\\), \\(`s_i^2\\in\\{0,1\\}`\\). Example Input Data { \"type\": \"blume-capel\", \"version\": \"0.2\", \"terms\": [ {\"c\": 1.0, \"ids\": [0, 1]}, {\"c\": 1.0, \"ids\": [1, 2]}, {\"c\": 1.0, \"ids\": [2, 3]}, {\"c\": -2.0, \"ids\": [3, 0]} ] }"
  },
  "spec/model/clock.html": {
    "href": "spec/model/clock.html",
    "title": "Clock model | qiotoolkit",
    "keywords": "Clock Spin Glass The clock model (and corresponding clock spin glass) describes interacting spins which can take discrete values on the 2D planar unit circle. \\mathcal{H} = -\\sum_{ij} J_{ij}\\, s_i\\!\\cdot\\!s_j,\\quad s_i=(\\cos\\varphi_i, \\sin\\varphi_i) where \\(`s_i\\!\\cdot\\!s_j`\\) is a scalar product and each \\(`\\varphi_i`\\) can take one of \\(`q`\\) values: \\varphi_i = v_i \\frac{2\\pi}{q},\\quad v_i\\in\\{0,\\ldots,q-1\\} Or, as a visual representation for \\(`q=5`\\): \\begin{tikzpicture} \\draw (0,0) circle(2); \\draw[->,line width=1.5pt] (0,0) -- (30:2); \\draw[->,line width=1.5pt] (0,0) -- (102:2); \\draw[->,line width=1.5pt] (0,0) -- (174:2); \\draw[->,line width=1.5pt] (0,0) -- (246:2); \\draw[->,line width=1.5pt] (0,0) -- (318:2); \\end{tikzpicture} NOTE: For the case discretized to two values, \\(`q=2`\\), this model corresponds to the Ising spin glass. Hypergraph Generalization The scalar product in the standard clock Hamiltonian does not lend itself to a generalization for multi-spin interactions. Instead, the qiotoolkit implementation uses a Hamiltonian based on the normalized sum of vectors \\(`\\vec v_e`\\): \\mathcal{H} = -\\sum_e J_e (2v_e^2-1),\\quad \\vec v_e = \\frac{1}{|e|}\\sum_{i\\in e} \\vec{s_i}\\, where the term in brackets reduces to the scalar product for the case of 2-spin interactions and covers the range \\(`[-1,1]`\\) for any \\(`q`\\): The minimal value is realized when the clock spins \"cancel out\" (i.e., sum up to \\(`\\vec 0`\\)) The maximal value is realized when all spins are aligned (have the same value). Example Input Data { \"type\": \"clock\", \"version\": \"0.1\", \"q\": 5, \"terms\": [ {\"c\": 1.0, \"ids\": [0, 1]}, {\"c\": 1.0, \"ids\": [1, 2]}, {\"c\": 1.0, \"ids\": [2, 3]}, {\"c\": -2.0, \"ids\": [3, 0]} ] }"
  },
  "spec/model/index.html": {
    "href": "spec/model/index.html",
    "title": "Model Specifications | qiotoolkit",
    "keywords": "Built-in Models qiotoolkit has the following built-in models: Model Description ising The Ising model consists of binary variables \\(`\\in\\{\\pm1\\}`\\) with a cost function that is the sum of weighted variable-products. pubo, qubo The PUBO (\"polynomial unconstrained binary optimization\") and QUBO (\"quadratic ...\") models have binary variables \\(`\\in[0,1]`\\) and a cost function that sums terms of 2 (qubo) or more variables (pubo). blume-capel The Blume-Capel model extends the Ising model to Spin-1 (i.e, 3 states, \\(`[-1,0,1]`\\)). potts-model Each variable in the Potts model can have one of \\(`p`\\) values \\(`[0,1,\\ldots,p-1]`\\) and the cost function compares variables for equality. That is, there is no notion of neighboring values clock-model Each clock variable (or \"rotor\") can have on of \\(`p`\\) discrete values which can be interpreted as a discretized angle (periodic boundaries). The cost function compares how similarly aligned interacting variables are. tsp The travelling salesman problem asks to find the shortest tour visiting all nodes in a graph. poly Cost function constructed from nested polynomial terms with mutable parameters. Experimental State Spaces In addition to the built-in models, components for Partition and Permutation are provided as pre-defined state-spaces."
  },
  "spec/model/ising.html": {
    "href": "spec/model/ising.html",
    "title": "Ising model | qiotoolkit",
    "keywords": "Ising Spin Glass The Ising spin glass Hamiltonian has the form: \\mathcal{H} = \\sum_{ij} c_{ij} s_i s_j\\,, where the sum is over interacting pairs \\(`ij`\\) and the spins take the values \\(`s_i\\in\\{\\pm1\\}`\\). The \\(`c_{ij}`\\)'s are interaction constants which are specified as part of the problem description. Caution This definition differs from the canonical version used in statistical mechanics by a global minus sign! As a result, positive interaction constants \\(`c_{ij}\\gt0`\\) give rise to anti-ferromagnetic interaction. Graph Interpretation We consider each of the spins \\(`s_i`\\) to reside on the node \\(`i`\\) of a weighted graph and the interactions \\(`c_{ij}`\\) to represent edge weights between the respective nodes \\(`i`\\) and \\(`j`\\). For instance, the Hamiltonian \\mathcal{H}(\\vec s) = - (s_0s_1 + s_1s_2 + s_2s_3 - 2s_3s_0) would correspond to the graph \\begin{tikzpicture} \\draw[line width=1pt] (0,3) -- (3,3) -- (3,0) -- (0,0); \\draw[line width=3pt,color=red] (0,0) -- (0,3); \\draw[fill=white] (0,3) circle(0.5); \\draw[fill=white] (3,3) circle(0.5); \\draw[fill=white] (3,0) circle(0.5); \\draw[fill=white] (0,0) circle(0.5); \\node at (0,3) {\\(0\\)}; \\node at (3,3) {\\(1\\)}; \\node at (3,0) {\\(2\\)}; \\node at (0,0) {\\(3\\)}; \\node[color=red] at (0.5,1.5) {\\(-2\\)}; \\node at (1.5,2.5) {\\(+1\\)}; \\node at (2.5,1.5) {\\(+1\\)}; \\node at (1.5,0.5) {\\(+1\\)}; \\end{tikzpicture} Hypergraph Generalization The above interpretation holds for Hamiltonians with terms consisting of exactly two spins. For optimization problems, we are interested in a more general Hamiltonian of the form \\mathcal{H} = -\\sum_e c_e \\prod_{i\\in e} s_i where the sum is over all edges \\(`e`\\) in the list of terms and the product is over the ids \\(`i`\\) participating in this term. In this case, any number of spins can interact in a given term -- akin to a \"hyper edge\" in the graph. Example Input Data A configuration for the general Ising spin glass Hamiltonian must carry the label \"ising\" and corresponding version \"1.0\" specify a non-empty array of terms in the Hamiltonian (or, equally, the hyper edges of the graph): each edge must have a cost c (denoted \\(`c_e`\\) in the Hamiltonian). each edge must have an array ids of the nodes participating in the interaction. the ids in each term must be unique (no repetition within a given term) the ids mentioned in all terms must form a consecutive set [0..N-1]. Example: { \"type\": \"ising\", \"version\": \"1.0\", \"terms\": [ {\"c\": 1.0, \"ids\": [0, 1]}, {\"c\": 1.0, \"ids\": [1, 2]}, {\"c\": 1.0, \"ids\": [2, 3]}, {\"c\": -2.0, \"ids\": [3, 0]} ] } NOTES: The list of ids can be empty, which is interpreted as a constant term in the Hamiltonian. If there is only a single number in ids, this corresponds to a local field. Repetitions are not allowed since any odd number is equivalent to a single occurence (and any even number can be dropped). The number of spins \\(`N`\\) is inferred from the highest number found in ids of any term (which is presumed to represent \\(`N-1`\\))."
  },
  "spec/model/poly.html": {
    "href": "spec/model/poly.html",
    "title": "Polynomial Cost Function | qiotoolkit",
    "keywords": "Polynomial Cost Function (Poly) Example Input Data"
  },
  "spec/model/potts.html": {
    "href": "spec/model/potts.html",
    "title": "Potts Model | qiotoolkit",
    "keywords": "Potts Spin Glass The Potts Hamiltonian has the form \\mathcal{H} = -\\sum_{ij} \\delta_{s_i,s_j}\\,, where the sum is over interacting spin pairs \\(`i`\\), \\(`j`\\) and \\(`\\delta`\\) is the Kronecker delta: \\delta_{a,b} = \\begin{cases} 1 & a=b\\\\ 0 & a\\neq b \\end{cases} Example Input Data { \"type\": \"potts\", \"version\": \"0.1\", \"q\": 5, \"terms\": [ {\"c\": 1.0, \"ids\": [0, 1]}, {\"c\": 1.0, \"ids\": [1, 2]}, {\"c\": 1.0, \"ids\": [2, 3]}, {\"c\": -2.0, \"ids\": [3, 0]} ] }"
  },
  "spec/model/pubo.html": {
    "href": "spec/model/pubo.html",
    "title": "PUBO model | qiotoolkit",
    "keywords": "PUBO The partially unconstrained binary optimization problem consists of variables \\(`s_i`\\) which take the values \\(`\\in\\{0,1\\}`\\): \\mathcal{H} = \\sum_{ij} J_{ij} s_is_j Example Input Data"
  },
  "spec/model/tsp.html": {
    "href": "spec/model/tsp.html",
    "title": "Traveling Salesman Problem | qiotoolkit",
    "keywords": "Travelling Salesman Problem (TSP) \\mathcal{H} = \\sum_{j\\in\\Pi_i} \\text{dist}(j, j+1)\\,, where \\(`\\Pi_i`\\) is a permutation of the numbers \\(`\\{0,\\ldots,N-1\\}`\\) and \\(`\\text{dist}(j, j+1)`\\) is the distance between a consecutive pair of nodes in the parmutation (we equate the nodes 0 and N, i.e., the \"route\" forms a loop). Example InputData { \"type\": \"tsp\", \"version\": \"0.1\", \"q\": 5, \"terms\": [ {\"c\": 1.0, \"ids\": [0, 1]}, {\"c\": 1.0, \"ids\": [1, 2]}, {\"c\": 1.0, \"ids\": [2, 3]}, {\"c\": -2.0, \"ids\": [3, 0]} ] }"
  },
  "spec/schedule/constant.html": {
    "href": "spec/schedule/constant.html",
    "title": "Constant Schedule | qiotoolkit",
    "keywords": "Constant Schedule { \"$id\": \"schedule/constant.schema\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Constant Schedule Generator\", \"oneOf\": [ { \"type\": \"number\", \"description\": \"The constant value of this generator\", }, { \"type\": \"object\", \"description\": \"Generator for a sequence of values in the range [initial, final] (inclusive).\", \"required\": [\"type\", \"value\"], \"properties\": { \"type\": { \"type\": \"string\", \"description\": \"Constant generator identifier.\", \"const\": \"constant\" }, \"value\": { \"type\": \"number\", \"description\": \"The constant value of the generator.\" } } } ] }"
  },
  "spec/schedule/geometric.html": {
    "href": "spec/schedule/geometric.html",
    "title": "Constant Schedule | qiotoolkit",
    "keywords": "Constant Schedule { \"$id\": \"schedule/geometric.schema\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Geometric Schedule Generator\", \"type\": \"object\", \"description\": \"Generator for a geometric interpolation of values from `initial` to `final` (inclusive).\", \"required\": [\"type\", \"initial\", \"final\"], \"properties\": { \"type\": { \"type\": \"string\", \"description\": \"GeometricGenerator identifier.\", \"const\": \"geometric\" } \"initial\": { \"type\": \"number\", \"description\": \"The initial value in the schedule (e.g., initial annealing temperature).\", \"minimum\": 1e-9 }, \"final\": { \"type\": \"number\", \"description\": \"The final value in the schedule (e.g., final annealing temperature).\", \"minimum\": 1e-9 } } }"
  },
  "spec/schedule/index.html": {
    "href": "spec/schedule/index.html",
    "title": "Schedule Specifications | qiotoolkit",
    "keywords": "Schedule { \"$id\": \"schedule/schedule.schema\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Schedule\", \"allOf\": [ \"oneOf\": [ {\"$ref\": \"constant.schema#\"}, {\"$ref\": \"linear.schema#\"}, {\"$ref\": \"geometric.schema#\"}, {\"$ref\": \"segments.schema#\"}, ], \"oneOf\": [ { \"type\": \"number\", \"description\": \"Shorthand notation for a constant generator.\" }, { \"type\": \"array\", \"description\": \"Shorthand notation for a segments generator.\" \"items\": {\"$ref\": \"schedule.schema#\"}, }, { \"type\": \"object\", \"required\": [\"type\"], \"properties\": { \"type\": { \"type\": \"string\", \"description\": \"identifier for the type of schedule generator\", \"enum\": [\"constant\", \"linear\", \"geometric\", \"segments\"] }, \"start\": { \"type\": \"number\", \"description\": \"start of the input value interval\", \"defaut\": 0.0 }, \"stop\": { \"type\": \"number\" \"description\": \"stop of the input value interval\", \"default\": 1.0 }, \"count\": { \"type\": \"integer\", \"description\": \"number of values when discretizing this generator to a set\", \"default\": 1, \"minimum\": 1 }, \"repeat\": { \"type\": \"bool\", \"description\": \"whether to repeat the schedule outside of the input interval.\", \"default\": false } } } ] ] }"
  },
  "spec/schedule/linear.html": {
    "href": "spec/schedule/linear.html",
    "title": "Linear Schedule | qiotoolkit",
    "keywords": "Linear Schedule { \"$id\": \"schedule/linear.schema\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Linear Schedule Generator\", \"type\": \"object\", \"description\": \"Generator for a linear interpolation of values from `initial` to `final` (inclusive).\", \"required\": [\"type\", \"initial\", \"final\"], \"properties\": { \"type\": { \"type\": \"string\", \"description\": \"LinearGenerator identifier.\", \"const\": \"linear\" } \"initial\": { \"type\": \"number\", \"description\": \"The initial value in the schedule (e.g., initial annealing temperature).\" }, \"final\": { \"type\": \"number\", \"description\": \"The final value in the schedule (e.g., final annealing temperature).\" } } }"
  },
  "spec/schedule/segments.html": {
    "href": "spec/schedule/segments.html",
    "title": "Segments Schedule | qiotoolkit",
    "keywords": "Segments Schedule { \"$id\": \"schedule/segments.schema\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Segments Schedule Generator\", \"oneOf\": [ { \"type\": \"array\", \"description\": \"An array describing the segments.\", \"items\": {\"$ref\": \"schedule.schema#\"} }, { \"type\": \"object\", \"description\": \"Generator consisting of concatenated segments.\", \"required\": [\"type\", \"segments\"], \"properties\": { \"type\": { \"type\": \"string\", \"description\": \"SegmentsGenerator identifier\", \"const\": \"segments\" } \"segments\": { \"type\": \"array\", \"description\": \"An array describing the segments.\", \"items\": {\"$ref\": \"schedule.schema#\"} } } } ] }"
  },
  "spec/solver/index.html": {
    "href": "spec/solver/index.html",
    "title": "Solvers | qiotoolkit",
    "keywords": "Built-in Solvers Solver Description Simulated Annealing Simulates independent replicas with metropolis dynamics while gradually cooling the system. Parallel Tempering A chain of metropolis replicas from a low to high temperature regime with exchange dynamics. Population Annealing A population of metropolis walkers with resampling. Substochastic Monte Carlo A population of random walkers with resampling Multi Objective Replica Exchange (MUREX) A network of metropolis nodes in temperature + parameter space with exchange dynamics"
  },
  "spec/solver/multi-objective-replica-exchange.html": {
    "href": "spec/solver/multi-objective-replica-exchange.html",
    "title": "Multi-Objective Replica Exchange | qiotoolkit",
    "keywords": "Multiobjective Replica Exchange (MUREX) Caution This solver is experimental; it is still subject to development which may alter its API and performance attributes. MUREX simulates replicas at different temperature AND parameter values concurrently. This is currently only supported by the Poly model which has parameters which can be adjusted during the simulation. This is achieved by defining a graph of nodes, each associated with different parameter values and a temperature. Replica exchanges are performed between nearby nodes according to the graph's edges. Note The current implementation has a grid arrangement of nodes hard-coded into the setup routine. This will be adjusted to be more configurable in upcoming versions."
  },
  "spec/solver/parallel-tempering.html": {
    "href": "spec/solver/parallel-tempering.html",
    "title": "Parallel Tempering | qiotoolkit",
    "keywords": "Parallel Tempering qiotoolkit's parallel tempering implementation does not currently adjust the set of temperatures automatically to the model (upcoming feature). As of now, the set of temperatures needs to be specified as an input parameter. For this you can either use an explicit array or a schedule generator. Alternatively, you may specify the set of inverse temperatures via all_betas. Example { \"target\": \"parallel-tempering.qiotoolkit\", \"version\": \"1.0\", \"input_params\": { \"seed\": 42, \"temperatures\": { \"type\": \"geometric\", \"initial_value\": 3.0, \"final_value\": 0.2, \"count\": 32 } }, \"input_data_uri\": \"...\" } This example uses a generator to create a geometric series of temperatures ranging from 3.0 to 0.2 with 32 steps. This means that the simulation will use 32 replicas spaced such that a_i = c*a_{i+1} with a constant c selected to match the initial and final value requested. Parameters Specification List Schema param required/default description seed default: time-based temperatures required How the temperature should be changed over time. all_betas required How the inverse temperature should be changed over time. { \"$id\": \"solver/paralleltempering.qiotoolkit.schema\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Parallel Tempering with qiotoolkit\", \"type\": \"object\", \"properties\": { \"target\": { \"type\": \"string\", \"const\": \"paralleltempering.qiotoolkit\" }, \"input_params\": { \"type\": \"object\", \"oneOf\": [ { \"required\": [\"temperatures\"] }, { \"required\": [\"all_betas\"] } ], \"properties\": { \"temperatures\": { \"type\": \"object\", \"$ref\": \"schedule.schema#\", \"description\": \"How the temperature should be changed over time.\" }, \"all_betas\": { \"type\": \"object\", \"$ref\": \"schedule.schema#\", \"description\": \"How the inverse temperature should be changed over time.\" } } } } }"
  },
  "spec/solver/population-annealing.html": {
    "href": "spec/solver/population-annealing.html",
    "title": "Population Annealing | qiotoolkit",
    "keywords": "Population Annealing Simulates a population of metropolis walkers with birth-death process resampling. The qiotoolkit implementation uses variable setpping in temperatures according to one of 4 available resampling strategies: linear_schedule: Change the temperature according to a fixed schedule (non-adaptive). friction_tensor: Estimate the next temperature step such that the friction tensor remains stable with a constant of friction_tensor_constant energy_variance: Adjust culling such that the energy variance remains stable, using initial_culling_constant for the first step. constant_culling: Adjust stepping such that a constant portion culling_fraction of the population is replaced each resampling step. The initial number of replicas can be specified via initial_population_size. The solver will double the population size whenever There is no remaining cost variance (all replicas are in an equal-cost state) The ratio of families remaining in the population drops below alpha (indicating that one family dominates the population). Additionally, you may increase the number of sweeps between resamplings using the sweeps_per_replica parameter. Example { \"target\": \"population-annealing.cpu\", \"version\": \"1.0\", \"input_params\": { \"seed\": 42, \"population\": 100 }, \"input_data_uri\": \"...\" } Parameters Specification List Schema param type required/default description seed integer time-based population integer >1 number of threads population size. constant_population bool false whether to keep the population constant between restarts resampling_strategy string linear_schedule resample by linear_schedule, friction_tensor, energy_variance or constant_culling. beta Schedule linear 0..5 The schedule by which to anneal the system. beta_start float >0 0.0 initial beta for the linear_schedule. beta_stop float >beta_start 5.0 final beta for the linear_schedule. friction_tensor_constant float >0.0 1.0 friction tensor constant (required for friction_tensor initial_culling_fraction float [0,1] 0.5 initial culling rate (for energy_variance) culling_fraction float [0,1] 0.2 constant culling rate (for constant_culling) alpha float >1.0 2.0 ratio to trigger a restart { \"$id\": \"solver/populationannealing.qiotoolkit.schema\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"qiotoolkit Population Annealing\", \"type\": \"object\", \"required\": [\"target\", \"input_params\"], \"properties\": { \"target\": { \"type\": \"string\", \"const\": \"populationannealing.qiotoolkit\" }, \"input_params\": { \"type\": \"object\", \"required\": [\"resampling_strategy\"], \"properties\": { \"alpha\": { \"type\": \"number\", \"minimum\": 1.0, \"default\": 2.0, \"description\": \"ratio to trigger a restart\" }, \"population\": { \"type\": \"number\", \"minimum\": 2, \"description\": \"desired population size (defaults to the number of threads)\" }, \"constant_population\": { \"type\": \"boolean\", \"default\": \"false\", \"description\": \"whether to keep the population constant between restarts\" }, \"beta\": { \"type\": \"object\", \"$ref\": \"../schedule/schedule.schema\", \"description\": \"How the temperature should be changed over time.\" }, \"beta_start\": { \"type\": \"number\", \"description\": \"float specifying the starting inverse temperature.\" \"minimum\": 0, }, \"beta_stop\": { \"type\": \"number\", \"description\": \"float specifying the stopping inverse temperature.\" \"minimum\": 0, }, \"resampling_strategy\": { \"type\": \"string\", \"enum\": [\"linear_schedule\", \"friction_tensor\", \"energy_variance\", \"constant_culling\"], \"default\": 'linear_schedule', \"description\": \"resample by 'linear_schedule', 'friction_tensor', 'energy_variance' or 'constant_culling'.\" }, \"friction_tensor_constant\": { \"type\": \"number\", \"minimum\": 0.0, \"default\": 1.0, \"description\": \"friction tensor constant\" }, \"initial_culling_fraction\": { \"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0, \"default\": 0.5, \"description\": \"initial culling rate\" }, \"culling_fraction\": { \"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0, \"default\": 0.2, \"description\": \"constant culling rate\" }, \"sweeps_per_replica\" { \"type\": \"number\", \"minimum\": 1, \"multipleOf\": 1, \"default\": 1, \"sweeps per replica between resampling.\" } } } } }"
  },
  "spec/solver/simulated-annealing.html": {
    "href": "spec/solver/simulated-annealing.html",
    "title": "Simulated Annealing | qiotoolkit",
    "keywords": "Simulated Annealing Performs metropolis dynamics on the model while gratually lowering (\"annealing\") the simulation temperature. As a result, the system settles in a low-cost state while still being allowed to escape local minima as the simulation progresses. qiotoolkit's simulated annealing implementation allows the specification of an annealing schedule either as an explicit array of values or as a schedule-generator. This (explicit or schedule-generated) list of numbers can be interpreted as inverse temperatures by specifying use_inverse_temperature = true. Multiple restarts can be performed using the restarts parameter. These restarts are mutually independent. Note If you use the QIOtoolkit-comptaible parameters beta_start and beta_stop, the solver will attempt to infer the number of annealing steps from step_limit or its Qiotoolkit-alias sweeps. Example { \"target\": \"simulated-annealing.qiotoolkit\", \"version\": \"1.0\", \"input_params\": { \"seed\": 42, \"schedule\": { \"type\": \"geometric\", \"initial_value\": 3.0, \"final_value\": 0.2, \"count\": 128 } \"restarts\": 8 }, \"input_data_uri\": \"...\" } This will perform 8 restarts of cooling the system from T=3.0 to T=0.2 in 128 steps spaced geometrically. Parameters Specification List Schema param type required/default description seed integer default: time-based schedule Schedule required How the temperature should be changed over time. use_inverse_temperature boolean default: false Whether to interpret schedule as inverse temperature. beta_start float >0 (QIO compatibility) Float specifying the starting inverse temperature. beta_stop float >beta_start (QIO compatibility) Float specifying the stopping inverse temperature. restarts integer > 0 default: 1 How many restarts to perform. { \"$id\": \"solver/simulatedannealing.qiotoolkit.schema\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Simulated Annealing on CPU\", \"type\": \"object\", \"required\": [\"target\", \"input_params\"], \"properties\": { \"target\": { \"type\": \"string\", \"const\": \"simulatedannealing.qiotoolkit\" }, \"input_params\": { \"type\": \"object\", \"oneOf\": [ {\"requried\": [\"schedule\"]}, {\"required\": [\"beta_start\", \"beta_stop\"]}, ], \"properties\": { \"schedule\": { \"type\": \"object\", \"$ref\": \"../schedule/schedule.schema\", \"description\": \"How the temperature should be changed over time.\" }, \"use_inverse_temperature\": { \"type\": \"bool\", \"description\": \"Whether to interpret `schedule` as inverse temperature\", \"default\": false }, \"beta_start\": { \"type\": \"number\", \"description\": \"float specifying the starting inverse temperature.\" \"minimum\": 0, }, \"beta_stop\": { \"type\": \"number\", \"description\": \"float specifying the stopping inverse temperature.\" \"minimum\": 0, }, \"restarts\": { \"type\": \"number\", \"description\": \"how many restarts to perform\", \"minimum\": 1, \"multipleOf\": 1, \"default\": 1 } }, } } }"
  },
  "spec/solver/substochastic-monte-carlo.html": {
    "href": "spec/solver/substochastic-monte-carlo.html",
    "title": "Substochastic Monte Carlo | qiotoolkit",
    "keywords": "Substochastic Monte Carlo Simulates a population of random walkers with birth-death resampling. qiotoolkit's implementation allows both the stepping probability alpha and the resampling factor beta to be specified as a schedule (over simulation steps). Additionaly, you may specify the target_population and the number of steps_per_walker as input parametes. Example { \"target\": \"substochastic-monte-carlo.qiotoolkit\", \"version\": \"1.0\", \"input_params\": { \"seed\": 42, \"alpha\": { \"initial_value\": 0.8, \"final_value\": 0.2, \"count\": 1e3 } \"beta\": { \"initial_value\": 0.2, \"final_value\": 0.8, \"count\": 1e3 } \"target_population\": 800 }, \"model\": {...} } This simulates a population of 800 random walkers over the course of 200 steps. It starts out with an emphasis on stepping (alpha=0.8) and gradually changes to a resampling regime. Note Substochastic Monte-Carlo does currently NOT adjust the energy scale of your model. Therefore the magnitude of beta may need to be adjusted for your needs. Parameters Specification List Schema param type required/default description seed integer default: time-based target_population integer required The desired population size (throughout the simulation). alpha Schedule required How the stepping probability alpha should be changed over time. beta Schedule required How the resampling factor beta should be changed over time. steps_per_walker float >0 Number of steps to attempt for each walker. { \"$id\": \"solver/substochasticmontecarlo.qiotoolkit.schema\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"qiotoolkit Substochastic Monte Carlo\", \"type\": \"object\", \"required\": [\"target\", \"input_params\"], \"properties\": { \"target\": { \"type\": \"string\", \"const\": \"substochasticmontecarlo.qiotoolkit\" }, \"input_params\": { \"type\": \"object\", \"required\": [\"target_population\", \"alpha\", \"beta\"], \"properties\": { \"target_population\": { \"type\": \"number\", \"minimum\": 0, \"multipleOf\": 1 \"description\": \"The desired population size (throughout the simulation).\" }, \"alpha\": { \"$ref\": \"../schedule/schedule.schema\", \"description\": \"How the stepping probability `alpha` should be changed over time.\" }, \"beta\": { \"$ref\": \"../schedule/schedule.schema\", \"description\": \"How the resampling factor `beta` should be changed over time.\" }, \"steps_per_walker\" { \"type\": \"integer\", \"description\": \"Number of steps to attempt for each walker.\", \"minimum\": 1 } } } } }"
  },
  "theory/graph-cost.html": {
    "href": "theory/graph-cost.html",
    "title": "Graph Correspondency | qiotoolkit",
    "keywords": "Graph Cost Functions For models with a polynomial cost function of the form \\mathcal{H} = \\sum_i c_i \\prod_{j\\in i} s_j such as the Ising or PUBO models, we define the following hyper-graph correspondency: Each variable of the model is associated with a node of the graph. Each term in the cost function gives rise to an interaction between the variables in this term. This is interpreted as a \"hyper-edge\" connecting these nodes. The interaction constant \\(`c_i`\\) is taken to be the weight of this edge. Note For the QUBO model (or an Ising model where no term has more then two variables interacting), this correspondency resolves to a regular graph where nodes are connected in pairs. As such we can describe each cost function of the above form as a hyper-graph. Implementation qiotoolkit provies a model::GraphModel base class which implements the input and access of a graph in the edge-list representation used by the QIO solvers in Qiotoolkit. Namely: \"terms\": [ {\"c\": 2, \"ids\": [0,1,2]}, {\"c\": -3, \"ids\": [3,4,5]} ... ] Where the listed terms describe An edge with weight \\(`+2`\\) connecting spins 0..2, and An edge with weight \\(`-3`\\) connecting spins 3,4,5 This would result in a cost function of the form \\mathcal{H} = +2s_0s_1s_2 -3s_3s_4s_5 + \\cdots The corresponding interfaces to access these nodes() and edges() can be found in the API section."
  },
  "theory/index.html": {
    "href": "theory/index.html",
    "title": "Theory | qiotoolkit",
    "keywords": "Theory The following articles aim to define some of the commonly used termininology: Graphs graph-cost.md correspondency for Ising, PUBO models Markov Dynamics Metropolis Walker Random Walker"
  },
  "theory/metropolis-walker.html": {
    "href": "theory/metropolis-walker.html",
    "title": "Metropolis Walker | qiotoolkit",
    "keywords": "Metropolis Walker The Boltzmann weight describes the likelyhood of finding a physical system in a state \\(`s`\\) at a given temperature \\(`T`\\): p(s) \\propto e^{-E(s)/k_B T} where \\(`E(s)`\\) is the energy of the system in state \\(`s`\\) (in the context of optimization, this is often referred to as the \"cost function\"). The actual value of \\(`p(s)`\\) could be obtained via normalization: p(s) = \\frac{e^{-E(s)/k_BT}}{Z},\\quad Z = \\sum_{s'} e^{-E(s')/k_B T} where the normalizing denominator \\(`Z`\\) is called the \"partition function\" and the sum is over all possible states of the system. For configuration spaces of interesting optimization problems, \\(`Z`\\) is typically intractable (that is, the number of possible configurations is too large to compute it). NOTE: The lower the temperature \\(`T`\\), the highter the weight-discrepancy between \"low energy\" and \"high energy\" states. In the limit \\(`T \\to 0`\\), the weight of all but the lowest energy state vanishes and we find the system in this \"ground state\" with probability \\(`1`\\). Randomized Sampling The metropolis algorithm describes a markov chain approach for randomized sampling of a system's states with their sampling probability converging to the Boltzmann weight. To extend the markov chain, a random next state is proposed and accepted with probability p(s\\to s') = \\begin{cases} e^{-(E(s')-E(s))/k_BT},& \\text{if } E(s') > E(s)\\\\ 1, & \\text{otherwise.} \\end{cases} That is, we always accept a move to a lower-energy state, but only allow increases depending on the energy difference \\(`\\Delta_E`\\) and temperature \\(`T`\\). This sampling approach can be proven to converge to the Boltzmann distribution in the long-term limit. Implementation Metropolis sampling is implemented as a specialization of the Walker base class: :::cpp linenums=False template <class Model> class Metropolis : public Walker<Model> { public: /// Decide whether to accept a given cost increase. bool accept(double cost_diff) override { return cost_diff <= 0 || Walker<Model>::random_number_generator_->uniform() < exp(-cost_diff * beta_); } /// Set the sampling temperature (also updates beta_). void set_temperature(double temperature); }; where the selection of a random next state and conditional application of the proposed transition is implemented in the Walker base class. NOTE: For optimization problems it is customary to set the Boltzmann constant to \\(`k_B\\equiv 1`\\) and work with the inverse temperature \\(`\\beta=1/T`\\). Usage The Metropolis class is used by solvers which rely on Boltzmann sampling, such as ParallelTempering, SimulatedAnnealing and PopulationAnnealing. Instantiation: The class of the model to be sampled is specified as a template argument to the Metropolis class. This defines both the model used for cost function evaluation and the type of state (Model::State_T) in the markov chain. Preparation: Metropolis instance initialization requires the following steps prior to calls to the make_step() method (inherited from Walker): set_model(&your_model) must be called with a pointer to an instance of the model to use (not owned). set_random_number_generator(&your_rng) must be called with a pointer to the random number generator to use (now owned). set_temperature(double) must be called with the desired sampling temperature (the default value is \\(`T=1`\\)). init() must be invoked after the model and rng have been assigned (and before make_step() is called). Both your_model and your_rng are \"not owned\", meaning they must be instantiated by the caller and must outlive the last call to make_step() NOTE: This delegation of ownership for the Model and RandomNumberGenerator allows the caller to manage the number of instances for each (e.g., avoid multiple models; rng's as needed for multi-threading). Introspection: As a Walker, each metropolis instance keeps track of the last state in the markov chain and caches its current energy. In particular, the calls to state() and cost() are \\(`O(1)`\\). Example #!cpp #include \"random/generator.h\" #include \"markov/metropolis.h #include \"solver/test/test_model.h\" int main() { // Prepare objects not owned by metropolis. TestModel model; common::RandomNumberGenerator rng; rng.seed(42); // Initialize the metropolis instance. Metropolis<TestModel> metropolis; metropolis.set_model(&model); metropolis.set_rng(&rng) metropolis.set_temperature(0.1); metropolis.init(); // Attempt 50 metropolis steps for (int i = 0; i < 50; i++) { metropolis.make_step(); std::cout << metropolis.state() << \" \" << metropolis.cost() << std::endl; } return 0; }"
  },
  "theory/random-walker.html": {
    "href": "theory/random-walker.html",
    "title": "Random Walker | qiotoolkit",
    "keywords": "Random Walker"
  },
  "tutorial/dev/configuration.html": {
    "href": "tutorial/dev/configuration.html",
    "title": "Configuration | qiotoolkit",
    "keywords": "Configuration qiotoolkit includes boiler plate to initialize C++ objects from json. For this, the class should be derived from utils::Component and override the virtual void configure(const utils::Json& json) {...} method inherited from the base class. Required Parameters A Parameter can be used to read a variable value from configuration. The chained call to required() signals that an exception should be thrown if the specified field name is not found: #include \"utils/component.h\" class MyModel : public utils::Component { public: void configure(const utils::Json& json) { this->param(json, \"my_param\", my_param_).required(); } private: int my_param_; }; This will read an integer from the JSON field my_param (see sample input below) and store its value in the class field my_param_. It throws a ConfigurationException if the field is not set or cannot be parsed as an integer: { \"my_param\": 42 } Matchers Depending on the meaning of my_param, not all integer values might make sense as input. For this reason, you maybe specify additional requirements to be checked on a ParameterBuilder using matches(): this->param(json, \"my_param\", my_param_) .matches(::matchers::GreaterThan(0)) .required(); This will throw a configuration exception if my_param is not specified, not an integer OR if its value is not greater than 0. Optional Parameters Parameters are implicitly optional (if required() is not specifeid). In this case, you can decide how to handle the absence of the parameter in the input: this->param(json, \"my_param\", my_param_); This does not throw a ConfigurationException if my_param is not set and does not change value of my_param_ (unless my_param_ is of std::optional<T> type, in which case it is reset). Specifying a default value: this->param(json, \"my_param\", my_param_) .default_value(3); assigns a specific default value (3) instead. For optional Parameters, you can also combine a matches() and default_value(): this->param(json, \"my_param\", my_param_) .matches(::matcher::GreaterThan(0)) .default_value(3); which tells the program to read from the configuration if the value is set (and check that it is greater than 0), or assign 3 otherwise. Note The default value is only checked against the matcher if it is chained before. Vector Parameters Configuration works for certain container data types in the same way as it does for primitive types. For instance, you can denote a std::vector<int> as an expected input parameter: #include \"../utils/component.h\" class MyComponent : public utils::Component { public: void configure(const utils::Json& json) { this->param(json, \"numbers\", numbers).required(); } private: std::vector<int> numbers_; }; Expects the input to have an array-type field numbers, the contents of which must all be integers. During configure(...), these numbers are filled into the std::vector: { \"numbers\": [1,2,3] } Complex Parameters Configuration works for any utils::Component (by invoking its configure method). This means that you can use components as parameter types: class MyParamType : public utils::Component { public: void configure(const utils::Json& json) override { this->param(json, \"a\", a_).required(); this->param(json, \"b\", b_).required(); } private: int a_, b_; }; class MyModel : public utils::Component { public: void configure(utils::Config& config) override { this->param(json, \"my_param\", my_param_).required(); } private: MyParamType my_param_; } The json input for MyModel is expected to have a field which contains the input for a MyParamType: { \"my_param\": { \"a\": 13, \"b\": 42 } }"
  },
  "tutorial/dev/dynamics.html": {
    "href": "tutorial/dev/dynamics.html",
    "title": "Model Dynamics | qiotoolkit",
    "keywords": "Model Dynamics"
  },
  "tutorial/dev/index.html": {
    "href": "tutorial/dev/index.html",
    "title": "Development | qiotoolkit",
    "keywords": "Development"
  },
  "tutorial/dev/log.html": {
    "href": "tutorial/dev/log.html",
    "title": "Logging | qiotoolkit",
    "keywords": "Logging The utils/log.h header file provides logging functionality in the form of the LOG() macro: ... LOG(INFO, \"The value of x is \", x) ... You can log any number of elements as an argument to LOG. Each of them must support printing to a stream (operator<<). Note All qiotoolkit classes which are derived from utils::Component have this capability. Levels Level Description DEBUG Surfaced in Debug builds only. INFO Hidden in Release unless -v is specified. WARN Unexpected state with predefined recovery. ERROR Problematic state, continuation possible. FATAL Recovery not possible. Debug builds will surface all log levels, while Release builds only show WARN level or higher (unless verbosity is set with -v). DEBUG logs are never shown in a Release build."
  },
  "tutorial/dev/model.html": {
    "href": "tutorial/dev/model.html",
    "title": "Model Development | qiotoolkit",
    "keywords": "Model Development A new model is created by inheriting from the markov::Model class. This base class defines the virtual pure methods which the new model must implement: method purpose get_identifier() string identifying the model -- this will be the type='...' value in the input. get_version() string identifying the version -- this will be the version='...' value in the input. configure(...) method telling qiotoolkit how to read model configuration from input. calculate_cost(state) Implements the (parametrized) cost function calculate_cost_difference(state, transition) Calculates how much the cost will change if transition is applied. get_random_state(rng) Creates a new random starting state get_random_transition(state, rng) Proposes a random change to the state apply_transition(transition, state) Modifies the state as dictated by the transition These methods entail the essence of the model: They define the state space and how one can move within it, as well as the association of a cost with each sate. The dynamics of whether to accept or reject a transition are left to the user. Self consistency The implementation of calculate_cost_difference() and calculate_cost() must be consistent. That is, \\text{costdiff}(\\text{before}, \\text{transition}) \\equiv \\text{cost}(\\text{transition}(\\text{before})) - \\text{cost}(\\text{before}) You can use the SelfConsistency Build to verify this holds. Note calculate_cost_difference() could be based on a call to calculate_cost() with a modified state, but this is less efficient for models where individual changes affect only a small number of terms. State and Transition In some situations it is sufficient to use a base type to represent the transition (or even the state); in others you need to define your own class to hold this information. You may extend markov::State and markov::Transition, respectively, to get the right interfaces. Whichever route you opt for, you need to template the base class of your new model with this two pieces of information: class MyModel : public ::markov::Model<MyState, MyTransition> { public: using State_T = MyState; using Transition_T = MyTransition; ... }; Note I find it convenient to typedef these two to State_T and Transition_T to homogenize the interfaces which need to be overloaded. Graph Model The special base class ::model::GraphModel makes use of the Graph-Cost correspondency and provides the appropriate interfaces to access the graph structure. If your cost function has the appropriate shape, this can simplify writing the cost function logic substantially. Example The following is an example implementation of the above interfaces for a soft-spin Model. It can be found in the cpp/examples/ directoy of the qiotoolkit codebase. #pragma once #include \"utils/config.h\" #include \"markov/state.h\" #include \"markov/transition.h\" #include \"model/graph_model.h\" namespace examples { class SoftSpinState : public ::markov::State { public: std::vector<double> spin; utils::Structure render() const override { return spin; } utils::Structure get_status() const override { return spin; } std::string get_class_name() const override { return \"SoftSpinState\"; } static size_t memory_estimate(size_t N) { return sizeof(SoftSpinState) + utils::vector_values_memory_estimate<double>(N); } static size_t state_only_memory_estimate(size_t N) { return memory_estimate(N); } }; class SoftSpinTransition : public ::markov::Transition { public: SoftSpinTransition() : spin_id(0), new_value(0) {} int spin_id; double new_value; bool operator<(const SoftSpinTransition& trans) const { if (spin_id == trans.spin_id) { return new_value < trans.new_value; } else { return spin_id < trans.spin_id; } } }; class SoftSpin : public ::model::GraphModel<SoftSpinState, SoftSpinTransition> { public: using State_T = SoftSpinState; using Transition_T = SoftSpinTransition; using Graph = ::model::GraphModel<State_T, Transition_T>; std::string get_identifier() const override { return \"softspin\"; } std::string get_version() const override { return \"0.1\"; } void configure(const utils::Json& json) override { Graph::configure(json); } void configure(Configuration_T& configuration) { Graph::configure(configuration); } double calculate_cost(const State_T& state) const override { double cost = 0; for (auto e : edges()) { double term = e.cost(); for (auto spin_id : e.node_ids()) { term *= state.spin[spin_id]; } cost += term; } return cost; } double calculate_cost_difference( const State_T& state, const Transition_T& transition) const override { double diff = 0; for (auto edge_id : node(transition.spin_id).edge_ids()) { const auto& e = edge(edge_id); double term_before = e.cost(); double term_after = e.cost(); for (auto spin_id : e.node_ids()) { term_before *= state.spin[spin_id]; if (spin_id == transition.spin_id) { term_after *= transition.new_value; } else { term_after *= state.spin[spin_id]; } } diff += term_after - term_before; } return diff; } State_T get_random_state(utils::RandomGenerator& rng) const override { State_T state; state.spin.resize(nodes().size()); for (size_t i = 0; i < nodes().size(); i++) { state.spin[i] = rng.uniform() * 2.0 - 1; } return state; } Transition_T get_random_transition(const State_T&, utils::RandomGenerator& rng) const override { Transition_T transition; transition.spin_id = (size_t)floor(rng.uniform() * static_cast<double>(nodes().size())); transition.new_value = rng.uniform() * 2.0 - 1; return transition; } void apply_transition(const Transition_T& transition, State_T& state) const override { state.spin[transition.spin_id] = transition.new_value; } size_t state_memory_estimate() const override { return State_T::memory_estimate(nodes().size()); } size_t state_only_memory_estimate() const override { return State_T::state_only_memory_estimate(nodes().size()); } }; } // namespace examples template <> struct std::hash<examples::SoftSpinTransition> { std::size_t operator()( const examples::SoftSpinTransition& trans) const noexcept { return utils::get_combined_hash(trans.spin_id, trans.new_value); } };"
  },
  "tutorial/dev/observer.html": {
    "href": "tutorial/dev/observer.html",
    "title": "Observer | qiotoolkit",
    "keywords": "Observer"
  },
  "tutorial/dev/solver.html": {
    "href": "tutorial/dev/solver.html",
    "title": "Solver Development | qiotoolkit",
    "keywords": "Solver Development A new solver is created by extending solver::AbstractSolver or solver::SteppingSolver. The latter has stepping logic already implemented and is currently used for all the built-in solvers. It is customary to template your solver for Model_T -- the class of the model to be simulated. The base class defines which methods must be implemented for the solver to work. For a SteppingSolver, they are: method purpose get_identifier() string identifying the solver -- this will be the target='...' value in the input. configure(...) method telling qiotoolkit how to read model configuration from input. init() called after configure(), allowing you to initialize based on configuration. make_step(step) perform the next step in the simulation. get_model_sweep_size() should return the sweep size of the model being simulated* get_lowest_cost() should return the lowest cost found so far (used for limit_cost condition) get_solutions() should render the best solution(s) found. Implementation The solver should rely on the interfaces provided by a markov::Model to perform its work. They are sufficient to build a markov chain, but might not cater to the need of more sophisticated solvers. For instance, the MUREX solver can currently only simulate the Poly model, because it relies on the additional parameter interfaces it provides. Example The following shows an implementation of a \"steep descent\" algorithm. Its deliberately not called \"steepest\" because the ::markov::Model interfaces do not allow us to enumerate all possible transitions from a given starting point. (And, depending on the model, this might be infeasible). Instead we settle for sampling a number of different candidates and picking the best choice (or none) at each step: #pragma once #include \"utils/random_generator.h\" #include \"solver/stepping_solver.h\" namespace examples { template <class Model_T> class Descent : public ::solver::SteppingSolver<Model_T> { public: using Base_T = ::solver::SteppingSolver<Model_T>; using State_T = typename Model_T::State_T; using Transition_T = typename Model_T::Transition_T; std::string get_identifier() const override { return \"descent.qiotoolkit\"; } void configure(const utils::Json& json) override { Base_T::configure(json); this->param(json[utils::kParams], \"samples\", samples_) .description(\"number of samples to take for the gradient\") .default_value(10) .matches(matcher::GreaterThan(0)) .with_output(); } std::string init_memory_check_error_message() const override { return ( \"Input problem is too large.\" \"Expected to exceed machine's current available memory.\"); } size_t target_number_of_states() const override { return 1; } void init() override { this->init_memory_check(); state_ = this->model_->get_random_state(*this->rng_); cost_ = this->model_->calculate_cost(state_); } void make_step(uint64_t) override { Transition_T transition; double best = 1.0; for (int i = 0; i < samples_; i++) { Transition_T candidate = this->model_->get_random_transition(state_, *this->rng_); double diff = this->model_->calculate_cost_difference(state_, candidate); if (diff < best) { best = diff; transition = candidate; } } if (best <= 0.0) { this->model_->apply_transition(transition, state_); cost_ += best; DEBUG(\"new best: \", cost_, \" \", state_); this->update_lowest_cost(cost_, state_); } } void finalize() override { this->update_lowest_cost(cost_, state_); } protected: int samples_; double cost_; State_T state_; }; } // namespace examples"
  },
  "tutorial/index.html": {
    "href": "tutorial/index.html",
    "title": "Tutorials | qiotoolkit",
    "keywords": "Tutorials Getting Started If this is your first time working with the qiotoolkit , here's a good place to start; build the binary and run your first simulation: Build the QIOToolkit Run your first simulation Usage Once you have a running binary (see previous section), this section explains how to run configure a request to run a specific solver on a particular model: Anatomy of a request Make it stop! Choosing a solver Choosing a model Multithreading Anatomy of a response Benchmarking Development Beyond the built-in functionality, you may also opt to modify a model to use custom simulation dynamics (types of steps), create a new model or even roll your own solver. Besides these tutorials, the API section provides machine-generated interface descriptions. Configuration Logging from qiotoolkit Writing a custom model Adding a solver"
  },
  "tutorial/setup/build.html": {
    "href": "tutorial/setup/build.html",
    "title": "Building the application | qiotoolkit",
    "keywords": "Building the QIOToolkit The QIOToolkit build environment supported is targeted for linux systems. The dockerfile located in the repo is the main build environment which the CI also runs in. If you want to build natively, you can follow the dockerfile for setting up your native environment. We do however recommend to stick with the dockerfile for the build process for building the x86-64 binary application. Running the two commands below should provide you with the build environment: qio-toolkit$ docker build -t qio-toolkit . qio-toolkit$ docker run -it --rm -v $(pwd):/qio-toolkit -w /qio-toolkit qio-toolkit Build and Test Once you have the docker environment up and running and have attached the repostiory inside of your docker environment you are ready to build the application and run tests. We have a single entrypoint for operating and building the application which is the root makefile. This file will provide you with all the necessary targets to do development on the QIOToolkit. The following targets are available: build - Builds the application in debug mode build-release - Builds the application in release mode build-coverage - Builds the application in debug mode with coverage test - Runs the unit tests test-coverage - Runs the unit tests with coverage clean - Cleans the build directory build-documentation - Builds the documentation static-code-analysis - Runs the static code analysis Building the application Building the application in debug mode can be done with the following command within your build environment: qio-toolkit$ make build Testing Testing the application can be done with the following command within your build environment: qio-toolkit$ make test Getting test coverage Getting the test coverage can be done with the following command within your build environment: qio-toolkit$ make test-coverage"
  },
  "tutorial/setup/cmake.html": {
    "href": "tutorial/setup/cmake.html",
    "title": "Build with CMake | qiotoolkit",
    "keywords": "Build with CMake qiotoolkit uses cmake as its build system. There are multiple build configurations available to choose from. Debug (default): During development, you may want to rely on a non-optimized build with debug symbols. Release: Uses optimization flags, strips debug symbols and all asserts from the code. SelfConsistency: Within walkers, check each calculate_cost_difference() against a full invocation of calculate_cost() (VERY SLOW). cmake is typically invoked in a new folder, e.g., path/to/qiotoolkit/cpp/YOUR_BUILD. The first time, cmake .. needs to be invoked with the selected build type. thereafter, you can simply make in that folder (even when modifying CMakeLists.txt files). Note The build type is also denoted in the build extensions found in each JSON response generated by the binary. Debug Build (default) $ cd path/to/qiotoolkit/cpp $ mkdir debug_build $ cd debug_build $ cmake .. $ make -j8 Release Build $ cd path/to/qiotoolkit/cpp $ mkdir release_build $ cd release_build $ cmake .. -DCMAKE_BUILD_TYPE=Release $ make -j8 SelfConsistency Build $ cd path/to/qiotoolkit/cpp $ mkdir consistency_build $ cd consistency_build $ cmake .. -DCMAKE_BUILD_TYPE=SelfConsistency $ make -j8 Once built, you can continue to running your first simulation."
  },
  "tutorial/setup/prerequisites.html": {
    "href": "tutorial/setup/prerequisites.html",
    "title": "Prerequisites | qiotoolkit",
    "keywords": "Prerequisites qiotoolkit uses the cmake build system and your compiler of choice (g++ being used in the example below). libomp-dev is required for multi-threading swig and python3-dev are used to generate python bindings (optional) In Linux, you can install these dependencies with sudo apt install cmake g++ libomp-dev swig python3-dev sudo apt-get install -y autoconf automake libtool curl make g++ unzip git clone https://github.com/protocolbuffers/protobuf.git cd protobuf git reset --hard 2514f0bd7da7e2af1bed4c5d1b84f031c4d12c10 cd cmake sudo cmake -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON . sudo make sudo make install sudo ldconfig You can find the script in src/Tools of this repo. After installing the prerequisites, you can build qiotoolkit with cmake. Troubleshooting A note about installing the latest cmake The command sudo apt install cmake You may not get the latest version. In such a scenario please follow the steps below. sudo apt-get purge cmake sudo apt-get update sudo apt-get install apt-transport-https ca-certificates gnupg software-properties-common wget wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | sudo apt-key add - sudo apt-add-repository 'deb https://apt.kitware.com/ubuntu/ bionic main' sudo apt-get update sudo apt-get install cmake You may get an error saying that boost is not available Please try the following steps sudo apt-get install libboost-all-dev"
  },
  "tutorial/setup/run.html": {
    "href": "tutorial/setup/run.html",
    "title": "First Simulation | qiotoolkit",
    "keywords": "First Simulation The first simulation we are going to simulate a simple Ising model. For this we need 2 inputs: The input parameters for the solver we want to run. The description of the Ising cost function (\"input data\"). A set of examples for these can be found in the cpp/examples/ directory: Input Params { \"type\": \"simulated-annealing.qiotoolkit\", \"version\": \"1.0\", \"params\": { \"step_limit\": 100, \"seed\": 42, \"restarts\": 4, \"schedule\": { \"type\": \"linear\", \"initial\": 2.0, \"final\": 1.0, \"count\": 100 } } } Input Data { \"cost_function\": { \"type\": \"ising\", \"version\": \"1.0\", \"terms\": [ {\"c\": 1, \"ids\": [0, 1]}, {\"c\": 1, \"ids\": [1, 2]}, {\"c\": 1, \"ids\": [2, 3]}, {\"c\": 1, \"ids\": [3, 4]}, {\"c\": 1, \"ids\": [4, 5]}, {\"c\": 1, \"ids\": [5, 6]}, {\"c\": 1, \"ids\": [6, 7]}, {\"c\": 1, \"ids\": [7, 8]}, {\"c\": 1, \"ids\": [8, 9]}, {\"c\": 1, \"ids\": [9, 0]} ] } } Running the Solver Additionally, the CLI command must specify the solver to use: $ cd path/to/qiotoolkit $ make build $ ./cpp/build_make/app/qiotoolkit --user --log_level=WARN \\ --solver simulatedannealing.qiotoolkit \\ --parameters ./data/params1.json \\ --input ./data/ising1.json Runtime Output [INFO] app/runner.cc:41: Reading file: ../data/params1.json [INFO] app/qiotoolkit.cc:191: Running solver simulatedannealing.qiotoolkit [DEBUG] solver/simulated_annealing.h:116: recorded cost:10.000000 [DEBUG] solver/simulated_annealing.h:117: calculated_cost:10.000000 [INFO] solver/stepping_solver.h:228: Stop due to step limit: 999, steps:999 [DEBUG] solver/simulated_annealing.h:116: recorded cost:-10.000000 [DEBUG] solver/simulated_annealing.h:117: calculated_cost:-10.000000 [INFO] app/qiotoolkit.cc:201: Writing Response to stdout Results { \"benchmark\": { \"accumulated_dependent_vars\": 0, \"avg_locality\": 1.000000, \"cost_difference_evaluation_count\": 0, \"cost_function_evaluation_count\": 99, \"cpu_time_ms\": 195.635000, \"disk_io_read_bytes\": 0, \"disk_io_write_bytes\": 2746, \"end2end_cputime_ms\": 241.409071, \"end2end_time_ms\": 39.701056, \"extensions\": [ { \"name\": \"counters\", \"value\": {\"accepted_transitions\": 0, \"difference_evaluations\": 0, \"function_evaluations\": 99} }, { \"name\": \"solver\", \"value\": { \"cost_milestones\": [ {\"cost\": 10.000000, \"step\": 1.000000} ], \"exit_reason\": \"Stop due to step limit: 999, steps:999\", \"last_step\": 999, \"step_limit\": 999 } }, { \"name\": \"model\", \"value\": { \"graph\": { \"size\": { \"accumulated_dependent_vars\": 0, \"avg_locality\": 1.000000, \"edges\": 3, \"locality\": 1, \"max_coupling_magnitude\": 4.000000, \"min_coupling_magnitude\": 2.000000, \"min_locality\": 1, \"nodes\": 3, \"sum_coefficient_degrees_total\": 3 } } } }, { \"name\": \"build\", \"value\": { \"branch\": \"\", \"build_type\": \"Debug\", \"commit_hash\": \"cc7d716\", \"compiler\": \"g++ 9.4\", \"qiotoolkit_version\": \"0.0.208\" } }, { \"name\": \"invocation\", \"value\": { \"datetime\": \"2023-05-10 11:44:04\", \"directory\": \"/QIOToolkitV1/cpp\", \"host\": \"\", \"user\": \"\" } } ], \"locality\": 1, \"max_coupling_magnitude\": 4.000000, \"max_mem_bytes\": 10448896, \"min_coupling_magnitude\": 2.000000, \"min_locality\": 1, \"num_terms\": 3, \"num_variables\": 3, \"postprocessing_time_ms\": 6.697893, \"preprocessing_time_ms\": 13.180017, \"solver_time_ms\": 5.856991, \"sum_coefficient_degrees_total\": 3, \"threads\": 20 }, \"solutions\": { \"version\": \"1.0\", \"configuration\": {\"0\": -1, \"2\": -1, \"3\": -1}, \"cost\": -10.000000, \"parameters\": { \"beta_start\": 0.990000, \"beta_stop\": 1.990000, \"restarts\": 99, \"seed\": 111, \"sweeps\": 999 }, \"solutions\": [ { \"configuration\": {\"0\": 1, \"2\": 1, \"3\": 1}, \"cost\": 10.000000, \"stage\": \"init\", \"time_in_stage_ms\": 13.180017 }, { \"configuration\": {\"0\": -1, \"2\": -1, \"3\": -1}, \"cost\": -10.000000, \"stage\": \"simulatedannealing.qiotoolkit\", \"time_in_stage_ms\": 5.856991 } ] } } Arguments --solver or -s (required): The name of the qiotoolkit solver target. --parameters or -p (required): The parameter file containing the input parameters for how the solver should be run. --input or -i (required): The data file containing the input data that specifies the problem. --output or -o (optional): The data file to which the result will be written. If this is not provided, the results will be written to stdout. --user (optional): Render output in human-readable form. -n (optional): No benchmarking output is displayed when this flag is set. -v (optional): Shows the qiotoolkit build version info."
  },
  "tutorial/usage/benchmark.html": {
    "href": "tutorial/usage/benchmark.html",
    "title": "Benchmark | qiotoolkit",
    "keywords": "Benchmark Caution Always ensure you are using a binary built with a Release configuration; there is a substantial difference in performance. If in doubt, you can check the build extension of the response.benchmark. Usage Details on how to run qiotoolkit can be found here. To display the benchmarking logging data, ensure that you run the qiotoolkit executable without the -n flag: $ cd path/to/qiotoolkit/cpp $ mkdir release_build $ cd release_build $ cmake .. -DCMAKE_BUILD_TYPE=Release $ make -j8 $ ./app/qiotoolkit --user --log_level=WARN --solver simulatedannealing.qiotoolkit \\ --parameters ../examples/params-sa.json \\ --input ../examples/ising.json Wall time of a run The wall clock duration of a simulation can be gathered from the benchmark.end2end_time_ms in the qiotoolkit Response. For this to be meaningful, the simulation should be run with a step_limit or (alternatively) eval_limit. When a timeout is set and reached, this will only show how precise qiotoolkit's stop timing was... Note For multi-threaded simulations, you can also compare this value to brenchmark.end2end_cputime_ms to evaluate how efficient the multi-threading was. Ideally the latter should approach threads times the former. Memory Usage Memory usage is returned as part of the response in the field benchmark.max_memory_usage_bytes. Time to Solution By setting a params.cost_limit you can tell qiotoolkit to stop the simulation once the expected lowest energy (or an even lower one...) is found. Use this limiting condition in conjunction with the wall clock duration mentioned above, keeping an eye on the number of benchmark.threads used when comparing results."
  },
  "tutorial/usage/limits.html": {
    "href": "tutorial/usage/limits.html",
    "title": "Limits | qiotoolkit",
    "keywords": "Make it Stop There are several options to define how long your simulation should run. Limit Parameters You should denote at least one of the following stopping conditions. If multiple are given, qiotoolkit will stop when the first one is reached. name description limit_time How many wall-clock seconds the simulation should run. qiotoolkit aims for a 1-s heartbeat (can be changed using tick_every) and will adjust the steps aiming to stop just before the limit_time is reached. This does not consider time spend in multiple vs single threads. step_limit How many steps the solver should take. The final step denoted is inclusive. eval_limit How many times the cost function should be evaluated. qiotoolkit will stop at the end of the step during which the eval_limit is surpassed. This treats full and partial cost function evaluations equivalently. cost_limit Stop when a cost equal or lower to cost_limit is reached. Will stop at the end of the step during which the requested cost is matched. This allows benchmarking time-to-solution. timeout Alias of time_limit for QIO compatibility sweeps Alias of step_limit for QIO compatibility Signals While the process is running, you can send the following signals to the process: signal effect SIG_USR1 show current status on stderr SIG_USR2 stop at the end of the current step SIG_TERM terminate immediately SIG_INT (Ctrl-C) 1st: show status, 2nd: stop after step, 3rd: terminate immediately Note To trigger the 2nd and 3rd option of SIG_INT, signals must be received within 2.0 wall clock seconds from each other (otherwise the counter resets to 0)."
  },
  "tutorial/usage/models.html": {
    "href": "tutorial/usage/models.html",
    "title": "Models | qiotoolkit",
    "keywords": "Built-in Models qiotoolkit has the following built-in models: Model Description ising The Ising model consists of binary variables \\(`\\in\\{\\pm1\\}`\\) with a cost function that is the sum of weighted variable-products. pubo, qubo The PUBO (\"polynomial unconstrained binary optimization\") and QUBO (\"quadratic ...\") models have binary variables \\(`\\in[0,1]`\\) and a cost function that sums terms of 2 (qubo) or more variables (pubo). blume-capel The Blume-Capel model extends the Ising model to Spin-1 (i.e, 3 states, \\(`[-1,0,1]`\\)). potts-model Each variable in the Potts model can have one of \\(`p`\\) values \\(`[0,1,\\ldots,p-1]`\\) and the cost function compares variables for equality. That is, there is no notion of neighboring values clock-model Each clock variable (or \"rotor\") can have on of \\(`p`\\) discrete values which can be interpreted as a discretized angle (periodic boundaries). The cost function compares how similarly aligned interacting variables are. tsp The travelling salesman problem asks to find the shortest tour visiting all nodes in a graph. poly Cost function constructed from nested polynomial terms with mutable parameters. Experimental State Spaces In addition to the built-in models, components for Partition and Permutation are provided as pre-defined state-spaces."
  },
  "tutorial/usage/multithread.html": {
    "href": "tutorial/usage/multithread.html",
    "title": "Multi-threading | qiotoolkit",
    "keywords": "Multi-threading Warning This feature is currently being reworked. Its parameters and functionality are subject to change. By default, solvers will try to use multiple threads to speed up the simulation (using OpenMP). You can override this behavior by setting a maximum number of threads in the input_params: { \"target\": \"paralleltempering.qiotoolkit\", \"input_params\": { \"threads\": 1, ... } } In any case, you can see the number of threads used denoted in the benchmark section of the output: { \"benchmark\": { ... \"threads\": 1, ... }, \"solutions\": [...] } If the \"threads\" is larger than the number of CPU cores on the machine or the value is not positive, the parameter \"threads\" will be ignored."
  },
  "tutorial/usage/request.html": {
    "href": "tutorial/usage/request.html",
    "title": "Anatomy of a Request | qiotoolkit",
    "keywords": "Anatomy of a Request The default qiotoolkit input format is JSON, although it would be possible to handle other formats with different input handlers. A minimal request for qiotoolkit has the following shape: { \"target\": \"SOLVER_IDENTIFIER\", \"input_params\": { ... } } \"cost_function\": { \"type\": \"MODEL_IDENTIFIER\", \"version\": \"MODEL_VERSION\", ... } } Where the SOLVER_IDENTIFIER must exactly match the value returned by AbstractSolver::get_identifier(). Likewise, MODEL_IDENTIFIER and MODEL_VERSION must match those return by markov::Model::get_identifier() and markov::Model::get_version(). Note These identifiers and version numbers not only tell qiotoolkit what to simulate but ensure there is no mismatch in versioning (i.e., you're trying to run old input with a substantially modified newer version of the code or vice-versa. Input Parameters Additional configuration for the solver should be denoted in the input_params section of the request. While some parameters are shared between all solvers (e.g., time_limit), others are specific to the solver. For instance, the Parallel Tempering solver can be configured with: ... \"input_params\": { \"max_steps\": 10, \"seed\": 42, \"temperature_set\": { \"type\": \"geometric\", \"low\": 0.5, \"high\": 3, \"steps\": 32 } }, ... Note For the exact set of parameters understood by a solver, refer to the solver specifications. Input Data The model is configured from additional parameters in the input_data section. These input parameters determine which cost function will be optimized. For instance, the Ising model expects a term entry describing the terms of the polynomial in the Ising cost function: ... \"input_data\": { \"type\": \"ising\", \"version\": \"1.0\", \"terms\": [ {\"c\": -1, \"ids\": [0,1]}, {\"c\": 2, \"ids\": [0,2]}, {\"c\": 1, \"ids\": [1,2]}, ... ] }, ... Which, in this case, describes a cost function of the form \\mathcal{H} = -s_0s_1 + 2s_0s_2 + s_1s_2 + \\cdots Note For the exact set of parameters required by each model, refer to the model specifications."
  },
  "tutorial/usage/response.html": {
    "href": "tutorial/usage/response.html",
    "title": "Anatomy of a Response | qiotoolkit",
    "keywords": "Anatomy of a Response A qiotoolkit response is in JSON format. It contains: The configuration (both explicit and inferred parameters) Telemetry including basic benchmarking attributes and qiotoolkit/solver-specific extensions The best solution(s) found { \"target\": \"parallel-tempering.qiotoolkit\", \"input_params\": {...} \"input_data\": {...} \"benchmark\": { \"execution_time_ms\": 19.98242, ... \"extensions\": [...] }, \"solutions\": [ ] } Configuration Telemetry The benchmark field contains basic benchmarking measures as defined in conjunction with the QIO service solvers. This denotes the resources consumed by the simulation (wall time, threads, memory usage, cost function evaluations). Benchmark Extensions Additionally, qiotoolkit will populate a number of extensions to this shared format to indicate provenance metadata and solver properties. Build information The build information extension ensures data provenance w.r.t. the version of the code which was used to generate a specific response. It contains the build type, branch and git hash of the code that was compiled. \"extensions\": [ ... { \"name\": \"build\", \"value\" { \"build_type\": \"RELEASE\", \"branch\": \"master\", \"hash\": \"a892de1\" } }, ... ] Note Ensure you always manually enable the RELEASE build type when running large scale simulations. There is a notable overhead for running in DEBUG mode (default). Solver properties The AbstractSolver base class inherits from utils::Observer, allowing it to take measurements during the course of the simulation. These measurements are summarized in the solver extension. Solutions Solutions will always be an array containing one or multiple solutions found by the solver. As these are stochastic methods, there is no guarantee that they are optimal."
  },
  "tutorial/usage/solvers.html": {
    "href": "tutorial/usage/solvers.html",
    "title": "Solvers | qiotoolkit",
    "keywords": "Built-in Solvers Solver Description Simulated Annealing Simulates independent replicas with metropolis dynamics while gradually cooling the system. Parallel Tempering A chain of metropolis replicas from a low to high temperature regime with exchange dynamics. Population Annealing A population of metropolis walkers with resampling. Substochastic Monte Carlo A population of random walkers with resampling Multi Objective Replica Exchange (MUREX) A network of metropolis nodes in temperature + parameter space with exchange dynamics"
  }
}